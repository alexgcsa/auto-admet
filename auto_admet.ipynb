{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d2cfc3c-a8f6-446a-aa89-b7073651d567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION: 0\n",
      "(596, 266)\n",
      "(596, 79)\n",
      "(596, 79)\n",
      "(596, 102)(596, 167)(596, 266)\n",
      "\n",
      "\n",
      "(596, 167)\n",
      "(596, 102)\n",
      "(596, 102)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/tmp/ipykernel_3292581/87923448.py\", line 704, in evaluate_fitness\n    print(sel_dataset_df.shape)\nAttributeError: 'NoneType' object has no attribute 'shape'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 988\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# Run GGP\u001b[39;00m\n\u001b[1;32m    987\u001b[0m ggp \u001b[38;5;241m=\u001b[39m GrammarBasedGP(grammar, training_dir, testing_dir, fitness_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauprc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 988\u001b[0m best_program \u001b[38;5;241m=\u001b[39m \u001b[43mggp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# Print the best program\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m#print(\"Best Program Found:\", )\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 859\u001b[0m, in \u001b[0;36mGrammarBasedGP.evolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    855\u001b[0m     condition \u001b[38;5;241m=\u001b[39m time_diff_minutes \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_time            \n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m#condition = generation < self.max_generations\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# Evaluate fitness\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m pop_fitness_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m evaluated_population \u001b[38;5;241m=\u001b[39m pop_fitness_scores[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation \u001b[38;5;241m=\u001b[39m deepcopy(evaluated_population)\n",
      "Cell \u001b[0;32mIn[6], line 769\u001b[0m, in \u001b[0;36mGrammarBasedGP.fitness\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pipeline, async_result \u001b[38;5;129;01min\u001b[39;00m async_results:\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m         fitness_value \u001b[38;5;241m=\u001b[39m \u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_budget_minutes_alg_eval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mTimeoutError:\n\u001b[1;32m    771\u001b[0m         fitness_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m  \u001b[38;5;66;03m# Timeout case\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/autoadmet/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SelectFpr, SelectFwe, SelectFdr, chi2, f_classif\n",
    "import warnings\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef, roc_auc_score, recall_score, average_precision_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from datetime import datetime\n",
    "\n",
    "class BNFGrammar:\n",
    "    def __init__(self):\n",
    "        self.grammar = defaultdict(list)\n",
    "        self.non_terminals = set()\n",
    "        self.terminals = set()\n",
    "        \n",
    "\n",
    "    def load_grammar(self, bnf_text: str):\n",
    "        \"\"\"\n",
    "        Parses the BNF grammar from a string.\n",
    "        \"\"\"\n",
    "        for line in bnf_text.strip().splitlines():\n",
    "            if \"::=\" in line:\n",
    "                lhs, rhs = line.split(\"::=\", 1)\n",
    "                lhs = lhs.strip()\n",
    "                self.non_terminals.add(lhs)\n",
    "                rhs_options = [option.strip() for option in rhs.split(\"|\")]\n",
    "                for option in rhs_options:\n",
    "                    self.grammar[lhs].append(option.split())\n",
    "                    for token in option.split():\n",
    "                        if token not in self.non_terminals:\n",
    "                            self.terminals.add(token)\n",
    "                            \n",
    "\n",
    "    def generate_parse_tree(self, symbol: str = \"<start>\", max_depth: int = 10) -> dict:\n",
    "        \"\"\"\n",
    "        Generates a parse tree starting from the given symbol, ensuring mandatory grammar components are included.\n",
    "        \"\"\"\n",
    "        if max_depth <= 0 or symbol not in self.grammar:\n",
    "            return symbol  # Return the symbol as a terminal if max depth is reached\n",
    "    \n",
    "        # Strictly enforce the `<start>` rule\n",
    "        if symbol == \"<start>\":\n",
    "            # Generate each mandatory component\n",
    "            feature_def = self.generate_parse_tree(\"<feature_definition>\", max_depth - 1)\n",
    "            scaling = self.generate_parse_tree(\"<feature_scaling>\", max_depth - 1)\n",
    "            selection = self.generate_parse_tree(\"<feature_selection>\", max_depth - 1)\n",
    "            ml_algo = self.generate_parse_tree(\"<ml_algorithms>\", max_depth - 1)\n",
    "    \n",
    "            return {symbol: [feature_def, \"#\", scaling, \"#\", selection, \"#\", ml_algo]}\n",
    "    \n",
    "        # Select a random production for other non-terminals\n",
    "        production = random.choice(self.grammar[symbol])\n",
    "        return {symbol: [self.generate_parse_tree(token, max_depth - 1) for token in production]}\n",
    "\n",
    "    \n",
    "    def parse_tree_to_string(self, tree) -> str:\n",
    "        \"\"\"\n",
    "        Reconstructs a string from the parse tree.\n",
    "        \"\"\"\n",
    "        if isinstance(tree, str):\n",
    "            # Leaf node (terminal)\n",
    "            return tree\n",
    "        # Non-terminal with its production rules as children\n",
    "        root, children = list(tree.items())[0]\n",
    "        return \" \".join(self.parse_tree_to_string(child) for child in children)\n",
    "\n",
    "    \n",
    "    def validate_parse_tree(self, tree, symbol=\"<start>\") -> bool:\n",
    "        \"\"\"\n",
    "        Validates if the parse tree conforms to the grammar and respects the `<start>` structure.\n",
    "        \"\"\"\n",
    "        if isinstance(tree, str):\n",
    "            return tree in self.terminals  # Check terminal validity\n",
    "    \n",
    "        if not isinstance(tree, dict) or len(tree) != 1:\n",
    "            return False\n",
    "    \n",
    "        root, children = list(tree.items())[0]\n",
    "        if root != symbol:\n",
    "            return False\n",
    "    \n",
    "        if symbol == \"<start>\":\n",
    "            # Check `<start>` structure\n",
    "            if len(children) != 7:\n",
    "                return False\n",
    "            expected_symbols = [\"<feature_definition>\", \"#\", \"<feature_scaling>\", \"#\", \"<feature_selection>\", \"#\", \"<ml_algorithms>\"]\n",
    "            for i, child_symbol in enumerate(expected_symbols):\n",
    "                if i % 2 == 0 and not self.validate_parse_tree(children[i], child_symbol):  # Validate non-terminals\n",
    "                    return False\n",
    "                if i % 2 == 1 and children[i] != \"#\":  # Ensure separator\n",
    "                    return False\n",
    "    \n",
    "        # Validate other non-terminals\n",
    "        for production in self.grammar[symbol]:\n",
    "            if len(production) == len(children) and all(\n",
    "                self.validate_parse_tree(child, production[i])\n",
    "                for i, child in enumerate(children)\n",
    "            ):\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "        \n",
    "\n",
    "class MLAlgorithmTransformer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def XGBoost(self, n_estimators_str, max_depth_str, max_leaves_str, learning_rate_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        clf = XGBClassifier(n_estimators=int(n_estimators_str), max_depth=max_depth_actual, random_state=42, \n",
    "                            max_leaves=int(max_leaves_str), learning_rate=float(learning_rate_str), n_jobs=1)        \n",
    "    \n",
    "        return clf \n",
    "    \n",
    "    \n",
    "    def GradientBoosting(self, n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, loss_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, random_state=42, \n",
    "                                         min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str), \n",
    "                                         max_features=max_features_actual, loss=loss_str)        \n",
    "    \n",
    "        return clf      \n",
    " \n",
    "    \n",
    "    def ExtraTrees(self, n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "     \n",
    "            \n",
    "        clf = ExtraTreesClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, n_jobs=1, random_state=42, \n",
    "                                   class_weight=class_weight_actual,  min_samples_split=int(min_samples_split_str), \n",
    "                                   min_samples_leaf=int(min_samples_split_str), max_features=max_features_actual)        \n",
    "    \n",
    "        return clf  \n",
    "    \n",
    "    \n",
    "    def RandomForest(self, n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "     \n",
    "        clf = RandomForestClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, n_jobs=1, random_state=42, \n",
    "                                     class_weight=class_weight_actual,  min_samples_split=int(min_samples_split_str), \n",
    "                                     min_samples_leaf=int(min_samples_split_str), max_features=max_features_actual)        \n",
    " \n",
    "        return clf   \n",
    "        \n",
    "    \n",
    "    def ExtraTree(self, criterion_str, splitter_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "            \n",
    "        clf = ExtraTreeClassifier(criterion=criterion_str, splitter='best', max_depth=max_depth_actual, \n",
    "                                  min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str),                                      \n",
    "                                  max_features=max_features_actual, random_state=0)      \n",
    "    \n",
    "        return clf  \n",
    "            \n",
    "    \n",
    "    def DecisionTree(self, criterion_str, splitter_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "\n",
    "\n",
    "        clf = DecisionTreeClassifier(criterion=criterion_str, splitter=splitter_str, max_depth=max_depth_actual, \n",
    "                                     min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str), \n",
    "                                     max_features=max_features_actual, random_state=0,\n",
    "                                     class_weight=class_weight_actual)      \n",
    "    \n",
    "        return clf\n",
    "    \n",
    "    \n",
    "    def AdaBoost(self, alg, n_est, lr):\n",
    "        clf = AdaBoostClassifier(n_estimators=n_est, learning_rate=lr, algorithm=alg, random_state=0)\n",
    "        return clf\n",
    "\n",
    "\n",
    "\n",
    "class FeatureSelectionTransformer:\n",
    "    def __init__(self, training_df, testing_df, training_label_col):\n",
    "        self.training_df = training_df\n",
    "        self.testing_df = testing_df\n",
    "        self.model = None\n",
    "        self.training_label_col = training_label_col\n",
    "\n",
    "    def select_fwe(self, alpha_str, score_function_str):\n",
    "    \n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            self.model = SelectFwe(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.training_df, self.training_label_col)\n",
    "            #df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.training_df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return None \n",
    "            \n",
    "    \n",
    "    def select_fdr(self, alpha_str, score_function_str):\n",
    "    \n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            self.model = SelectFdr(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.training_df, self.training_label_col)\n",
    "            #df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.training_df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return None\n",
    "            \n",
    "    \n",
    "    def select_fpr(self, alpha_str, score_function_str):\n",
    "    \n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            self.model = SelectFpr(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.training_df, self.training_label_col)\n",
    "            #df_np = self.model.transform(self.training_df)\n",
    "        \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.training_df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return None\n",
    "            \n",
    "    \n",
    "    def select_percentile(self, percentile_str, score_function_str):\n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            self.model = SelectPercentile(score_func=score_function_actual, percentile = int(percentile_str)).fit(self.training_df, self.training_label_col)\n",
    "            #df_np = self.model.transform(self.training_df)\n",
    "        \n",
    "            cols_idxs = self.model.get_support(indices=True)\n",
    "            features_df_new = self.training_df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return None\n",
    "            \n",
    "    \n",
    "    def variance_threshold(self,thrsh):\n",
    "        try:\n",
    "            self.model =VarianceThreshold(threshold=thrsh).fit(self.training_df, training_label_col)\n",
    "            #df_np = model.transform(self.training_df)\n",
    "        \n",
    "            cols_idxs = self.model.get_support(indices=True)\n",
    "            features_df_new = self.training_df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def apply_model():\n",
    "        try:\n",
    "            cols_idxs = self.model.get_support(indices=True)\n",
    "            features_df_new_testing = self.testing.iloc[:,cols_idxs]       \n",
    "            return features_df_new_testing\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "        \n",
    "\n",
    "\n",
    "class ScalingTransformer:\n",
    "    def __init__(self, training_df, testing_df):\n",
    "        self.training_df = training_df\n",
    "        self.testing_df = testing_df\n",
    "        self.model = None\n",
    "\n",
    "    def normalizer(self, norm_hp):\n",
    "        try:\n",
    "            self.model = Normalizer(norm=norm_hp).fit(self.training_df)\n",
    "            df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.training_df.columns)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def max_abs_scaler(self):\n",
    "        try:\n",
    "            self.model = MaxAbsScaler().fit(self.training_df)\n",
    "            df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.training_df.columns)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def min_max_scaler(self):\n",
    "        try:\n",
    "            self.model = MinMaxScaler().fit(self.training_df)\n",
    "            df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.training_df.columns)\n",
    "        except Exception as e:\n",
    "            return None \n",
    "\n",
    "    \n",
    "    def standard_scaler(self, with_mean_str, with_std_str):\n",
    "        with_mean_actual = True\n",
    "        with_std_actual = True\n",
    "    \n",
    "        if with_mean_str == \"False\":\n",
    "            with_mean_actual = False\n",
    "        if with_std_str == \"False\":\n",
    "            with_std_actual = False        \n",
    "        try:\n",
    "            self.model = StandardScaler(with_mean=with_mean_actual, with_std=with_std_actual).fit(self.training_df)\n",
    "            df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.training_df.columns)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def robust_scaler(self, with_centering_str, with_scaling_str):\n",
    "        with_centering_actual = True\n",
    "        with_scaling_actual = True\n",
    "    \n",
    "        if with_centering_str == \"False\":\n",
    "            with_centering_actual = False\n",
    "        if with_scaling_str == \"False\":\n",
    "            with_scaling_actual = False        \n",
    "        try:\n",
    "            self.model = RobustScaler(with_centering=with_centering_actual, with_scaling=with_scaling_actual).fit(self.training_df)\n",
    "            df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.training_df.columns)\n",
    "        except Exception as e:            \n",
    "            return None\n",
    "\n",
    "    def apply_model():\n",
    "        try:\n",
    "            df_np_testing = self.model.transform(self.testing_df)        \n",
    "            return df_np_testing\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "        \n",
    "\n",
    "\n",
    "class GrammarBasedGP:\n",
    "    def __init__(self, grammar, training_dir, testing_dir, fitness_cache={}, num_cores=20, time_budget_minutes_alg_eval = 3, population_size=5, \n",
    "                 max_generations=3, max_time=3, mutation_rate=0.1, crossover_rate=0.7, crossover_mutation_rate=0.05, elitism_size=2, fitness_metric=\"auc\", \n",
    "                 experiment_name = \"expABC\", stopping_criterion = \"generations\"):\n",
    "        self.grammar = grammar\n",
    "        self.training_dir = training_dir\n",
    "        self.testing_dir = testing_dir\n",
    "        self.fitness_cache = fitness_cache\n",
    "        self.num_cores = num_cores\n",
    "        self.time_budget_minutes_alg_eval = time_budget_minutes_alg_eval\n",
    "        self.population_size = population_size\n",
    "        self.max_generations = max_generations\n",
    "        self.max_time = max_time\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.crossover_mutation_rate = crossover_mutation_rate\n",
    "        self.elitism_size = elitism_size\n",
    "        self.fitness_metric = fitness_metric\n",
    "        self.experiment_name = experiment_name\n",
    "        self.stopping_criterion = stopping_criterion\n",
    "        self.population = []\n",
    "\n",
    "    def select_ml_algorithms(self, ml_algorithm):\n",
    "        ml_alg_selection = MLAlgorithmTransformer()\n",
    "        if ml_algorithm[0] == \"AdaBoostClassifier\":\n",
    "            return ml_alg_selection.AdaBoost(str(ml_algorithm[1]), int(ml_algorithm[2]), float(ml_algorithm[3]))\n",
    "        elif ml_algorithm[0] == \"DecisionTreeClassifier\":\n",
    "            return ml_alg_selection.DecisionTree(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])    \n",
    "        elif ml_algorithm[0] == \"ExtraTreeClassifier\":\n",
    "            return ml_alg_selection.ExtraTree(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"RandomForestClassifier\":\n",
    "            return ml_alg_selection.RandomForest(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"ExtraTreesClassifier\":\n",
    "            return ml_alg_selection.ExtraTrees(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"GradientBoostingClassifier\":\n",
    "            return ml_alg_selection.GradientBoosting(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7]) \n",
    "        elif ml_algorithm[0] == \"XGBClassifier\":\n",
    "            return ml_alg_selection.XGBoost(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4])         \n",
    "                            \n",
    "        else:\n",
    "            return None    \n",
    "\n",
    "    \n",
    "    def select_features(self, feature_selection, training_dataset_df, training_label_col, testing_dataset_df=None, testing=False):\n",
    "\n",
    "        cp_training_dataset_df = training_dataset_df.copy(deep=True)\n",
    "        cp_testing_datset_df = None        \n",
    "        if(testing):\n",
    "            cp_testing_datset_df = testing_dataset_df.copy(deep=True)\n",
    "\n",
    "        cp_training_label_col = training_label_col.copy(deep=True)\n",
    "        feature_selection_transformer = FeatureSelectionTransformer(cp_training_dataset_df, cp_testing_datset_df, cp_training_label_col)\n",
    "        mod_training_dataset_df = None\n",
    "        mod_testing_dataset_df = None        \n",
    "        if feature_selection[0] == \"NoFeatureSelection\":\n",
    "            if(testing):\n",
    "                return training_dataset_df, testing_dataset_df\n",
    "            else:\n",
    "                return training_dataset_df\n",
    "        elif feature_selection[0] == \"VarianceThreshold\":\n",
    "            mod_training_dataset_df = feature_selection_transformer.variance_threshold(float(feature_selection[1]))\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = feature_selection_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df\n",
    "\n",
    "        elif feature_selection[0] == \"SelectPercentile\":        \n",
    "            mod_training_dataset_df = feature_selection_transformer.select_percentile(feature_selection[1], feature_selection[2])    \n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = feature_selection_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df   \n",
    "        elif feature_selection[0] == \"SelectFpr\":        \n",
    "            mod_training_dataset_df = feature_selection_transformer.select_fpr(feature_selection[1], feature_selection[2])    \n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = feature_selection_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df  \n",
    "        elif feature_selection[0] == \"SelectFdr\":        \n",
    "            mod_training_dataset_df = feature_selection_transformer.select_fdr(feature_selection[1], feature_selection[2])    \n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = feature_selection_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df  \n",
    "        elif feature_selection[0] == \"SelectFwe\":        \n",
    "            mod_training_dataset_df = feature_selection_transformer.select_fwe(feature_selection[1], feature_selection[2])    \n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = feature_selection_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df           \n",
    "        else:\n",
    "            return None       \n",
    "\n",
    "    \n",
    "    def scale_features(self, feature_scaling, training_dataset_df, testing_dataset_df=None, testing=False):\n",
    "\n",
    "        cp_training_dataset_df = training_dataset_df.copy(deep=True)\n",
    "        cp_testing_datset_df = None\n",
    "        if(testing):\n",
    "            cp_testing_datset_df = testing_dataset_df.copy(deep=True)\n",
    "        \n",
    "        scaling_transformer = ScalingTransformer(cp_training_dataset_df, cp_testing_datset_df)\n",
    "        mod_training_dataset_df = None\n",
    "        mod_testing_dataset_df = None\n",
    "        if feature_scaling[0] == \"NoScaling\":\n",
    "            if(testing):\n",
    "                return training_dataset_df, testing_dataset_df\n",
    "            else:\n",
    "                return training_dataset_df\n",
    "            \n",
    "        elif feature_scaling[0] == \"Normalizer\":\n",
    "            mod_training_dataset_df = scaling_transformer.normalizer(str(feature_scaling[1]))\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = scaling_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df\n",
    "        elif feature_scaling[0] == \"MinMaxScaler\":\n",
    "            mod_training_dataset_df = scaling_transformer.min_max_scaler()\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = scaling_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df\n",
    "        elif feature_scaling[0] == \"MaxAbsScaler\":\n",
    "            mod_training_dataset_df = scaling_transformer.max_abs_scaler()\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = scaling_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df  \n",
    "        elif feature_scaling[0] == \"StandardScaler\":\n",
    "            mod_training_dataset_df  = scaling_transformer.standard_scaler(feature_scaling[1], feature_scaling[2])\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = scaling_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df\n",
    "        elif feature_scaling[0] == \"RobustScaler\":\n",
    "            mod_training_dataset_df = scaling_transformer.robust_scaler(feature_scaling[1], feature_scaling[2])\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = scaling_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df      \n",
    "        else:            \n",
    "            return None    \n",
    "    \n",
    "\n",
    "\n",
    "    def represent_molecules(self, list_of_feature_types, training_dataset_df, testing_dataset_df=None, testing=False):\n",
    "        \"\"\"\n",
    "        represents a chemical dataset with descriptors.\n",
    "        \"\"\"          \n",
    "    \n",
    "        columns = []\n",
    "        for lft in list_of_feature_types:\n",
    "            if lft == \"General_Descriptors\":\n",
    "                columns += [\"HeavyAtomCount\",\"MolLogP\",\"NumHeteroatoms\",\"NumRotatableBonds\",\"RingCount\",\"TPSA\",\"LabuteASA\",\"MolWt\",\"FCount\",\"FCount2\",\"Acceptor_Count\",\"Aromatic_Count\",\"Donor_Count\",\"Hydrophobe_Count\",\"NegIonizable_Count\",\"PosIonizable_Count\",]\n",
    "            elif lft == \"Advanced_Descriptors\":\n",
    "                columns += [\"BalabanJ\",\"BertzCT\",\"Chi0\",\"Chi0n\",\"Chi0v\",\"Chi1\",\"Chi1n\",\"Chi1v\",\"Chi2n\",\"Chi2v\",\"Chi3n\",\"Chi3v\",\"Chi4n\",\"Chi4v\",\"HallKierAlpha\",\"Kappa1\",\"Kappa2\",\"Kappa3\",\"NHOHCount\",\"NOCount\",\"PEOE_VSA1\",\"PEOE_VSA10\",\"PEOE_VSA11\",\"PEOE_VSA12\",\"PEOE_VSA13\",\"PEOE_VSA14\",\"PEOE_VSA2\",\"PEOE_VSA3\",\"PEOE_VSA4\",\"PEOE_VSA5\",\"PEOE_VSA6\",\"PEOE_VSA7\",\"PEOE_VSA8\",\"PEOE_VSA9\",\"SMR_VSA1\",\"SMR_VSA10\",\"SMR_VSA2\",\"SMR_VSA3\",\"SMR_VSA4\",\"SMR_VSA5\",\"SMR_VSA6\",\"SMR_VSA7\",\"SMR_VSA8\",\"SMR_VSA9\",\"SlogP_VSA1\",\"SlogP_VSA10\",\"SlogP_VSA11\",\"SlogP_VSA12\",\"SlogP_VSA2\",\"SlogP_VSA3\",\"SlogP_VSA4\",\"SlogP_VSA5\",\"SlogP_VSA6\",\"SlogP_VSA7\",\"SlogP_VSA8\",\"SlogP_VSA9\",\"VSA_EState1\",\"VSA_EState10\",\"VSA_EState2\",\"VSA_EState3\",\"VSA_EState4\",\"VSA_EState5\",\"VSA_EState6\",\"VSA_EState7\",\"VSA_EState8\",\"VSA_EState9\"]\n",
    "            elif lft == \"Toxicophores\":\n",
    "                columns += [\"Tox_1\",\"Tox_2\",\"Tox_3\",\"Tox_4\",\"Tox_5\",\"Tox_6\",\"Tox_7\",\"Tox_8\",\"Tox_9\",\"Tox_10\",\"Tox_11\",\"Tox_12\",\"Tox_13\",\"Tox_14\",\"Tox_15\",\"Tox_16\",\"Tox_17\",\"Tox_18\",\"Tox_19\",\"Tox_20\",\"Tox_21\",\"Tox_22\",\"Tox_23\",\"Tox_24\",\"Tox_25\",\"Tox_26\",\"Tox_27\",\"Tox_28\",\"Tox_29\",\"Tox_30\",\"Tox_31\",\"Tox_32\",\"Tox_33\",\"Tox_34\",\"Tox_35\",\"Tox_36\"]\n",
    "            elif lft == \"Fragments\":\n",
    "                columns += [\"fr_Al_COO\",\"fr_Al_OH\",\"fr_Al_OH_noTert\",\"fr_ArN\",\"fr_Ar_COO\",\"fr_Ar_N\",\"fr_Ar_NH\",\"fr_Ar_OH\",\"fr_COO\",\"fr_COO2\",\"fr_C_O\",\"fr_C_O_noCOO\",\"fr_C_S\",\"fr_HOCCN\",\"fr_Imine\",\"fr_NH0\",\"fr_NH1\",\"fr_NH2\",\"fr_N_O\",\"fr_Ndealkylation1\",\"fr_Ndealkylation2\",\"fr_Nhpyrrole\",\"fr_SH\",\"fr_aldehyde\",\"fr_alkyl_carbamate\",\"fr_alkyl_halide\",\"fr_allylic_oxid\",\"fr_amide\",\"fr_amidine\",\"fr_aniline\",\"fr_aryl_methyl\",\"fr_azide\",\"fr_azo\",\"fr_barbitur\",\"fr_benzene\",\"fr_benzodiazepine\",\"fr_bicyclic\",\"fr_diazo\",\"fr_dihydropyridine\",\"fr_epoxide\",\"fr_ester\",\"fr_ether\",\"fr_furan\",\"fr_guanido\",\"fr_halogen\",\"fr_hdrzine\",\"fr_hdrzone\",\"fr_imidazole\",\"fr_imide\",\"fr_isocyan\",\"fr_isothiocyan\",\"fr_ketone\",\"fr_ketone_Topliss\",\"fr_lactam\",\"fr_lactone\",\"fr_methoxy\",\"fr_morpholine\",\"fr_nitrile\",\"fr_nitro\",\"fr_nitro_arom\",\"fr_nitro_arom_nonortho\",\"fr_nitroso\",\"fr_oxazole\",\"fr_oxime\",\"fr_para_hydroxylation\",\"fr_phenol\",\"fr_phenol_noOrthoHbond\",\"fr_phos_acid\",\"fr_phos_ester\",\"fr_piperdine\",\"fr_piperzine\",\"fr_priamide\",\"fr_prisulfonamd\",\"fr_pyridine\",\"fr_quatN\",\"fr_sulfide\",\"fr_sulfonamd\",\"fr_sulfone\",\"fr_term_acetylene\",\"fr_tetrazole\",\"fr_thiazole\",\"fr_thiocyan\",\"fr_thiophene\",\"fr_unbrch_alkane\",\"fr_urea\"]\n",
    "            elif lft == \"Graph_based_Signatures\":\n",
    "                columns += [\"Acceptor:Acceptor-6.00\",\"Acceptor:Aromatic-6.00\",\"Acceptor:Donor-6.00\",\"Acceptor:Hydrophobe-6.00\",\"Acceptor:NegIonizable-6.00\",\"Acceptor:PosIonizable-6.00\",\"Aromatic:Aromatic-6.00\",\"Aromatic:Donor-6.00\",\"Aromatic:Hydrophobe-6.00\",\"Aromatic:NegIonizable-6.00\",\"Aromatic:PosIonizable-6.00\",\"Donor:Donor-6.00\",\"Donor:Hydrophobe-6.00\",\"Donor:NegIonizable-6.00\",\"Donor:PosIonizable-6.00\",\"Hydrophobe:Hydrophobe-6.00\",\"Hydrophobe:NegIonizable-6.00\",\"Hydrophobe:PosIonizable-6.00\",\"NegIonizable:NegIonizable-6.00\",\"NegIonizable:PosIonizable-6.00\",\"PosIonizable:PosIonizable-6.00\",\"Acceptor:Acceptor-4.00\",\"Acceptor:Aromatic-4.00\",\"Acceptor:Donor-4.00\",\"Acceptor:Hydrophobe-4.00\",\"Acceptor:NegIonizable-4.00\",\"Acceptor:PosIonizable-4.00\",\"Aromatic:Aromatic-4.00\",\"Aromatic:Donor-4.00\",\"Aromatic:Hydrophobe-4.00\",\"Aromatic:NegIonizable-4.00\",\"Aromatic:PosIonizable-4.00\",\"Donor:Donor-4.00\",\"Donor:Hydrophobe-4.00\",\"Donor:NegIonizable-4.00\",\"Donor:PosIonizable-4.00\",\"Hydrophobe:Hydrophobe-4.00\",\"Hydrophobe:NegIonizable-4.00\",\"Hydrophobe:PosIonizable-4.00\",\"NegIonizable:NegIonizable-4.00\",\"NegIonizable:PosIonizable-4.00\",\"PosIonizable:PosIonizable-4.00\",\"Acceptor:Acceptor-2.00\",\"Acceptor:Aromatic-2.00\",\"Acceptor:Donor-2.00\",\"Acceptor:Hydrophobe-2.00\",\"Acceptor:NegIonizable-2.00\",\"Acceptor:PosIonizable-2.00\",\"Aromatic:Aromatic-2.00\",\"Aromatic:Donor-2.00\",\"Aromatic:Hydrophobe-2.00\",\"Aromatic:NegIonizable-2.00\",\"Aromatic:PosIonizable-2.00\",\"Donor:Donor-2.00\",\"Donor:Hydrophobe-2.00\",\"Donor:NegIonizable-2.00\",\"Donor:PosIonizable-2.00\",\"Hydrophobe:Hydrophobe-2.00\",\"Hydrophobe:NegIonizable-2.00\",\"Hydrophobe:PosIonizable-2.00\",\"NegIonizable:NegIonizable-2.00\",\"NegIonizable:PosIonizable-2.00\",\"PosIonizable:PosIonizable-2.00\"]\n",
    "            \n",
    "        mod_training_dataset_df = None\n",
    "        mod_testing_dataset_df = None\n",
    "        try:\n",
    "            cp_training_dataset_df = training_dataset_df.copy(deep=True)\n",
    "            mod_training_dataset_df = cp_training_dataset_df[columns]\n",
    "\n",
    "            if(testing):\n",
    "                cp_testing_dataset_df = testing_dataset_df.copy(deep=True)\n",
    "                mod_testing_dataset_df = cp_testing_dataset_df[columns]                \n",
    "                \n",
    "        except:\n",
    "            print(\"Error representation. \")\n",
    "    \n",
    "        if(testing):\n",
    "            return mod_training_dataset_df, mod_testing_dataset_df\n",
    "        else:\n",
    "            return mod_training_dataset_df   \n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_train_test(self, pipeline):\n",
    "        \"\"\"\n",
    "        Evaluates the pipeline on training and testing, performing each step of the ML pipeline.\n",
    "        \"\"\"  \n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        #all the steps in Auto-ADMET pipeline:\n",
    "        pipeline_string = self.grammar.parse_tree_to_string(pipeline)\n",
    "        pipeline_list = pipeline_string.split(\" # \")\n",
    "        representation = pipeline_list[0].split(\" \")\n",
    "        feature_scaling = pipeline_list[1].split(\" \")\n",
    "        feature_selection = pipeline_list[2].split(\" \")\n",
    "        ml_algorithm = pipeline_list[3].split(\" \")\n",
    "\n",
    "        #applying the steps to an actual dataset:\n",
    "        training_dataset_df = pd.read_csv(self.training_dir, header=0, sep=\",\")\n",
    "        training_label_col = training_dataset_df[\"CLASS\"]\n",
    "        training_dataset_df = training_dataset_df.drop(\"CLASS\", axis=1)\n",
    "\n",
    "        testing_dataset_df = pd.read_csv(self.testing_dir, header=0, sep=\",\")\n",
    "        testing_label_col = dataset_df[\"CLASS\"]\n",
    "        testing_dataset_df = dataset_df.drop(\"CLASS\", axis=1)        \n",
    "\n",
    "        rep_dataset_df = self.represent_molecules(representation, dataset_df) \n",
    "        prep_dataset_df = self.scale_features(feature_scaling, rep_dataset_df)\n",
    "        sel_dataset_df = self.select_features(feature_selection, prep_dataset_df, label_col)\n",
    "        #ml_algorithm  = self.select_ml_algorithms(ml_algorithm)\n",
    "        #sel_dataset_df[\"CLASS\"] = pd.Series(label_col)        \n",
    "        \n",
    "        try:\n",
    "            y = sel_dataset_df.iloc[:,-1:]\n",
    "            X = sel_dataset_df[sel_dataset_df.columns[:-1]]\n",
    "            scores = None\n",
    "            \n",
    "            if(self.fitness_metric == \"auc\"):                \n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(roc_auc_score))\n",
    "            elif(self.fitness_metric == \"mcc\"):            \n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(matthews_corrcoef))\n",
    "            elif(self.fitness_metric == \"recall\"):\n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(recall_score))\n",
    "            elif(self.fitness_metric == \"precision\"):\n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(precision_score))\n",
    "            elif(self.fitness_metric == \"auprc\"):\n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(average_precision_score))\n",
    "            elif(self.fitness_metric == \"accuracy\"):\n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(accuracy_score))                \n",
    "                \n",
    "            fitness_value = scores.mean()\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            fitness_value = -1.00            \n",
    "\n",
    "        # This function should evaluate the fitness of the individual within the time budget        \n",
    "        elapsed_time = time.time() - start_time    \n",
    "        if elapsed_time > (time_budget_minutes_alg_eval * 60):  # Check if elapsed time exceeds time budget\n",
    "            fitness_value = 0.0  # Set fitness value to zero if time budget exceeded\n",
    "\n",
    "        \n",
    "            \n",
    "        return fitness_value\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def evaluate_fitness(self, pipeline, dataset_path, time_budget_minutes_alg_eval):\n",
    "        \"\"\"\n",
    "        evaluates pipeline with the fitness, performing each step of the ML pipeline.\n",
    "        \"\"\"  \n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        #all the steps in Auto-ADMET pipeline:\n",
    "        pipeline_string = self.grammar.parse_tree_to_string(pipeline)\n",
    "        pipeline_list = pipeline_string.split(\" # \")\n",
    "        representation = pipeline_list[0].split(\" \")\n",
    "        feature_scaling = pipeline_list[1].split(\" \")\n",
    "        feature_selection = pipeline_list[2].split(\" \")\n",
    "        ml_algorithm = pipeline_list[3].split(\" \")\n",
    "\n",
    "        #applying the steps to an actual dataset:\n",
    "        dataset_df = pd.read_csv(self.training_dir, header=0, sep=\",\")\n",
    "        label_col = dataset_df[\"CLASS\"]\n",
    "        dataset_df = dataset_df.drop(\"CLASS\", axis=1)\n",
    "\n",
    "        rep_dataset_df = self.represent_molecules(representation, dataset_df)\n",
    "        print(rep_dataset_df.shape)\n",
    "        if(rep_dataset_df is None):\n",
    "            return 0.0\n",
    "        prep_dataset_df = self.scale_features(feature_scaling, rep_dataset_df)\n",
    "        print(prep_dataset_df.shape)\n",
    "        if(prep_dataset_df is None):\n",
    "            return 0.0\n",
    "        sel_dataset_df = self.select_features(feature_selection, prep_dataset_df, label_col)\n",
    "        print(sel_dataset_df.shape)\n",
    "        if(sel_dataset_df is None):\n",
    "            return 0.0        \n",
    "        #ml_algorithm  = self.select_ml_algorithms(ml_algorithm)\n",
    "        #sel_dataset_df[\"CLASS\"] = pd.Series(label_col)        \n",
    "        \n",
    "        try:\n",
    "            y = sel_dataset_df.iloc[:,-1:]\n",
    "            X = sel_dataset_df[sel_dataset_df.columns[:-1]]\n",
    "            scores = None\n",
    "            \n",
    "            if(self.fitness_metric == \"auc\"):                \n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(roc_auc_score))\n",
    "            elif(self.fitness_metric == \"mcc\"):            \n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(matthews_corrcoef))\n",
    "            elif(self.fitness_metric == \"recall\"):\n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(recall_score))\n",
    "            elif(self.fitness_metric == \"precision\"):\n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(precision_score))\n",
    "            elif(self.fitness_metric == \"auprc\"):\n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(average_precision_score))\n",
    "            elif(self.fitness_metric == \"accuracy\"):\n",
    "                scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(accuracy_score))                \n",
    "                \n",
    "            fitness_value = scores.mean()\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            fitness_value = 0.00            \n",
    "\n",
    "        # This function should evaluate the fitness of the individual within the time budget        \n",
    "        elapsed_time = time.time() - start_time    \n",
    "        if elapsed_time > (time_budget_minutes_alg_eval * 60):  # Check if elapsed time exceeds time budget\n",
    "            fitness_value = 0.0  # Set fitness value to zero if time budget exceeded\n",
    "\n",
    "        \n",
    "            \n",
    "        return fitness_value\n",
    "        \n",
    "    def fitness(self):\n",
    "        \"\"\"\n",
    "        Calculates the fitness function in parallel using multiprocessing,\n",
    "        while caching results to avoid redundant evaluations.\n",
    "        \"\"\"\n",
    "        with multiprocessing.Pool(processes=self.num_cores) as pool:\n",
    "            results = []\n",
    "            async_results = []\n",
    "            \n",
    "            # Submit all tasks asynchronously, checking cache first\n",
    "            for pipeline in self.population:\n",
    "                pipeline_str =self.grammar.parse_tree_to_string(pipeline)  # Convert individual to a string representation\n",
    "                \n",
    "                if pipeline_str in self.fitness_cache:\n",
    "                    # Use cached value if available\n",
    "                    results.append((pipeline, self.fitness_cache[pipeline_str]))\n",
    "                else:\n",
    "                    # Otherwise, evaluate it asynchronously\n",
    "                    async_result = pool.apply_async(\n",
    "                        self.evaluate_fitness, \n",
    "                        (pipeline, self.training_dir, self.time_budget_minutes_alg_eval)\n",
    "                    )\n",
    "                    async_results.append((pipeline, async_result))\n",
    "    \n",
    "            # Collect results in a non-blocking way\n",
    "            for pipeline, async_result in async_results:\n",
    "                try:\n",
    "                    fitness_value = async_result.get(timeout=self.time_budget_minutes_alg_eval * 60)\n",
    "                except multiprocessing.TimeoutError:\n",
    "                    fitness_value = 0.0  # Timeout case\n",
    "                \n",
    "                # Cache the computed fitness value\n",
    "                pipeline_str =self.grammar.parse_tree_to_string(pipeline)\n",
    "               \n",
    "                self.fitness_cache[pipeline_str] = fitness_value  # Store in dictionary\n",
    "                results.append((pipeline, fitness_value))\n",
    "        \n",
    "        # Separate pipelines and fitness values\n",
    "        pipelines, fitness_results = zip(*results) if results else ([], [])\n",
    "    \n",
    "        return list(pipelines), list(fitness_results)\n",
    "\n",
    "\n",
    "    \n",
    "    def crossover(self, parent1, parent2):\n",
    "        \"\"\"\n",
    "        Performs crossover by swapping compatible components between parents.\n",
    "        \"\"\"\n",
    "        if isinstance(parent1, str) or isinstance(parent2, str):  # No crossover if terminal\n",
    "            return parent1, parent2\n",
    "    \n",
    "        root1, children1 = list(parent1.items())[0]\n",
    "        root2, children2 = list(parent2.items())[0]\n",
    "    \n",
    "        if root1 == \"<start>\" and root2 == \"<start>\":\n",
    "            # Swap one of the four main components of `<start>`\n",
    "            idx = random.choice([0, 2, 4, 6])  # Indices of the main components\n",
    "            children1[idx], children2[idx] = children2[idx], children1[idx]\n",
    "        elif root1 == root2:\n",
    "            # Swap subtrees for other non-terminals\n",
    "            idx1 = random.randint(0, len(children1) - 1)\n",
    "            idx2 = random.randint(0, len(children2) - 1)\n",
    "            children1[idx1], children2[idx2] = children2[idx2], children1[idx1]\n",
    "    \n",
    "        return parent1, parent2\n",
    "\n",
    "    \n",
    "    def mutate(self, individual, max_mutation_depth=4):\n",
    "        \"\"\"\n",
    "        Mutates an individual by replacing a specific component with a new valid subtree.\n",
    "        \"\"\"\n",
    "        if isinstance(individual, str):  # Terminal, no mutation possible\n",
    "            return individual\n",
    "    \n",
    "        root, children = list(individual.items())[0]\n",
    "    \n",
    "        if root == \"<start>\":\n",
    "            # Mutate one of the four main components of `<start>`\n",
    "            idx = random.choice([0, 2, 4, 6])  # Indices of the main components\n",
    "            components = [\"<feature_definition>\", \"<feature_scaling>\", \"<feature_selection>\", \"<ml_algorithms>\"]\n",
    "            replacement = self.grammar.generate_parse_tree(components[idx // 2], max_depth=max_mutation_depth)\n",
    "            children[idx] = replacement\n",
    "        else:\n",
    "            # Mutate other non-terminals\n",
    "            idx = random.randint(0, len(children) - 1)\n",
    "            children[idx] = self.grammar.generate_parse_tree(root, max_depth=max_mutation_depth)\n",
    "    \n",
    "        return individual\n",
    "    \n",
    "    \n",
    "    def evolve(self):\n",
    "        \"\"\"\n",
    "        Runs the genetic programming algorithm.\n",
    "        \"\"\"\n",
    "        # Initialize population\n",
    "        self.population = [self.grammar.generate_parse_tree() for _ in range(self.population_size)]\n",
    "           \n",
    "        \n",
    "        generation = 0\n",
    "        start = datetime.now()\n",
    "        end = start\n",
    "        time_diff_minutes = (end - start).total_seconds() / 60\n",
    "        condition = \"\"\n",
    "        if(self.stopping_criterion == \"generations\"):\n",
    "            condition = generation < self.max_generations\n",
    "        elif(self.stopping_criterion == \"time\"):\n",
    "            condition = time_diff_minutes < self.max_time\n",
    "        \n",
    "        while condition:   \n",
    "            print(\"GENERATION: \" + str(generation))\n",
    "            if(self.stopping_criterion == \"generations\"):\n",
    "                condition = generation < self.max_generations\n",
    "            elif(self.stopping_criterion == \"time\"):\n",
    "                condition = time_diff_minutes < self.max_time            \n",
    "            \n",
    "            #condition = generation < self.max_generations\n",
    "            # Evaluate fitness\n",
    "            pop_fitness_scores = self.fitness()\n",
    "            evaluated_population = pop_fitness_scores[0]\n",
    "            self.population = deepcopy(evaluated_population)\n",
    "            fitness_scores = pop_fitness_scores[1]\n",
    "\n",
    "            elites_indices = sorted(range(len(self.population)), key=lambda i: fitness_scores[i], reverse=True)\n",
    "            elites = [self.population[i] for i in elites_indices[:self.elitism_size]] \n",
    "            \n",
    "\n",
    "            for i in elites_indices:\n",
    "                print(self.grammar.parse_tree_to_string(self.population[i]) + \"--->\" + str(fitness_scores[i]))\n",
    "       \n",
    "\n",
    "            # Elitism: retain the best individuals\n",
    "            new_population = []\n",
    "            new_population.extend(elites)\n",
    "\n",
    "            # Selection probabilities\n",
    "            fitness_values = [1.0 / (f + 1e-6) for f in fitness_scores]\n",
    "            total_fitness = sum(fitness_values)\n",
    "            probabilities = [f / total_fitness for f in fitness_values]\n",
    "\n",
    "            while len(new_population) < self.population_size:\n",
    "                random_num = random.random()                \n",
    "                parent1, parent2 = random.choices(self.population, probabilities, k=2)\n",
    "                \n",
    "                if (random_num < self.crossover_mutation_rate):                    \n",
    "                    #perform crossover                    \n",
    "                    child1, child2 = self.crossover(deepcopy(parent1), deepcopy(parent2))                    \n",
    "                    # and mutation                    \n",
    "                    child1_1 = self.mutate(deepcopy(child1))\n",
    "                    child2_1 = self.mutate(deepcopy(child2))\n",
    "                    new_population.extend([child1_1, child2_1]) \n",
    "                elif (random_num < (self.crossover_mutation_rate + self.mutation_rate)):\n",
    "                    #only mutation\n",
    "                    child = self.mutate(deepcopy(parent1))\n",
    "                    new_population.append(child)\n",
    "                elif (random_num < (self.crossover_mutation_rate + self.mutation_rate + self.crossover_rate)):\n",
    "                    #only crossover\n",
    "                    child1, child2 = self.crossover(deepcopy(parent1), deepcopy(parent2))\n",
    "                    new_population.extend([child1, child2])     \n",
    "                else:\n",
    "                    #no operation\n",
    "                    new_population.extend([deepcopy(parent1), deepcopy(parent2)])\n",
    "                    \n",
    "            # Trim excess individuals\n",
    "            self.population = new_population[:self.population_size]\n",
    "            end =  datetime.now()\n",
    "            time_diff_minutes = (end - start).total_seconds() / 60\n",
    "            generation += 1\n",
    "            print(\"-----------------------------------------------\")            \n",
    "\n",
    "        print(time_diff_minutes)\n",
    "        print(generation)\n",
    "        best_indices = sorted(range(len(self.population)), key=lambda i: fitness_scores[i], reverse=True)[:1]\n",
    "        best_fitness = [fitness_scores[i] for i in elites_indices][0]  \n",
    "        best_individual = self.population[best_indices[0]]\n",
    "\n",
    "        \n",
    "        \n",
    "        #print(f\"Generation {generation}: Best Fitness = {best_fitness}\")\n",
    "        #print(f\"Best Individual: {self.grammar.parse_tree_to_string(best_individual)}\")\n",
    "\n",
    "        #final_file_name = self.experiment_name + \".txt\"\n",
    "        #with open(final_file_name, \"w\") as file:\n",
    "        #    \n",
    "        #    file.write()\n",
    "            \n",
    "\n",
    "        #return None  # Return the best individual\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(7)  # For reproducibility\n",
    "\n",
    "    # Define grammar\n",
    "    grammar_text = \"\"\"\n",
    "    <start> ::= <feature_definition> # <feature_scaling> # <feature_selection> # <ml_algorithms>\n",
    "    <feature_definition> ::=  General_Descriptors | Advanced_Descriptors | Graph_based_Signatures | Toxicophores | Fragments | General_Descriptors Advanced_Descriptors | General_Descriptors Graph_based_Signatures | General_Descriptors Toxicophores | General_Descriptors Fragments | Advanced_Descriptors Graph_based_Signatures | Advanced_Descriptors Toxicophores | Advanced_Descriptors Fragments | Graph_based_Signatures Toxicophores | Graph_based_Signatures Fragments | Toxicophores Fragments | General_Descriptors Advanced_Descriptors Graph_based_Signatures | General_Descriptors Advanced_Descriptors Toxicophores | General_Descriptors Advanced_Descriptors Fragments | General_Descriptors Graph_based_Signatures Toxicophores | General_Descriptors Graph_based_Signatures Fragments | General_Descriptors Toxicophores Fragments | Advanced_Descriptors Graph_based_Signatures Toxicophores | Advanced_Descriptors Graph_based_Signatures Fragments | Advanced_Descriptors Toxicophores Fragments | Graph_based_Signatures Toxicophores Fragments | General_Descriptors Advanced_Descriptors Graph_based_Signatures Toxicophores | General_Descriptors Advanced_Descriptors Graph_based_Signatures Fragments | General_Descriptors Advanced_Descriptors Toxicophores Fragments | General_Descriptors Graph_based_Signatures Toxicophores Fragments | Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments | General_Descriptors Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments\n",
    "    <feature_scaling> ::= <no_scaling> | <normalizer> | MinMaxScaler | MaxAbsScaler | <robust_scaler> | <standard_scaler>\n",
    "    <normalizer> ::= Normalizer <norm>\n",
    "    <robust_scaler> ::= RobustScaler <boolean> <boolean>\n",
    "    <standard_scaler> ::= StandardScaler <boolean> <boolean>\n",
    "    <feature_selection> ::= <no_feature_selection> | <variance_threshold> | <select_percentile> | <selectfpr> | <selectfwe> | <selectfdr>\n",
    "    <variance_threshold> ::= VarianceThreshold <threshold>\n",
    "    <select_percentile> ::= SelectPercentile <percentile> <score_function>\n",
    "    <selectfpr> ::= SelectFpr <value_rand_1> <score_function>\n",
    "    <selectfwe> ::= SelectFwe <value_rand_1> <score_function>\n",
    "    <selectfdr> ::= SelectFdr <value_rand_1> <score_function>\n",
    "    <ml_algorithms> ::= <adaboost> | <decision_tree> | <extra_tree> | <random_rorest> | <extra_trees> | <gradient_boosting> |  <xgboost>\n",
    "    <adaboost> ::= AdaBoostClassifier <algorithm_ada> <n_estimators> <learning_rate_ada>\n",
    "    <decision_tree> ::= DecisionTreeClassifier <criterion> <splitter> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight>\n",
    "    <extra_tree> ::= ExtraTreeClassifier <criterion> <splitter> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight> \n",
    "    <random_rorest> ::= RandomForestClassifier <n_estimators> <criterion> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight_rf>\n",
    "    <extra_trees> ::= ExtraTreesClassifier <n_estimators> <criterion> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight_rf>\n",
    "    <gradient_boosting> ::= GradientBoostingClassifier <n_estimators> <criterion_gb> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <loss>\n",
    "    <xgboost> ::= XGBClassifier <n_estimators> <max_depth> <max_leaves> <learning_rate_ada>\n",
    "    <no_scaling> ::= NoScaling\n",
    "    <no_feature_selection> ::= NoFeatureSelection\n",
    "    <norm> ::= l1 | l2 | max\n",
    "    <threshold> ::= 0.0 | 0.05 | 0.10 | 0.15 | 0.20 | 0.25 | 0.30 | 0.35 | 0.40 | 0.45 | 0.50 | 0.55 | 0.60 | 0.65 | 0.70 | 0.75 | 0.80 | 0.85 | 0.90 | 0.95 | 1.0\n",
    "    <algorithm_ada> ::= SAMME.R | SAMME\n",
    "    <n_estimators> ::= 2 | 5 | 10 | 15 | 20 | 25 | 30 | 35 | 45 | 50 | 55 | 60 | 65 | 70 | 75 | 80 | 90 | 95 | 100 | 150 | 200 | 250 | 300 | 350 | 400 | 450 | 500 | 600 | 700 | 900 | 1000 | 1500 | 2000 | 2500 | 3000 | 4000 | 5000 | 7500 | 10000\n",
    "    <learning_rate_ada> ::= 0.01 | 0.05 | 0.10 | 0.15 | 0.20 | 0.25 | 0.30 | 0.35 | 0.40 | 0.45 | 0.50 | 0.55 | 0.60 | 0.65 | 0.70 | 0.75 | 0.80 | 0.85 | 0.90 | 0.95 | 1.0 | 1.05 | 1.10 | 1.15 | 1.20 | 1.25 | 1.30 | 1.35 | 1.40 | 1.45 | 1.50 | 1.55 | 1.60 | 1.65 | 1.70 | 1.75 | 1.80 | 1.85 | 1.90 | 1.95 | 2.0\n",
    "    <boolean> ::= True | False\n",
    "    <percentile> ::= 5 | 10 | 15 | 20 | 25 | 30 | 35 | 45 | 50 | 55 | 60 | 65 | 70 | 75 | 80 | 90 | 95\n",
    "    <score_function> ::= f_classif | chi2\n",
    "    <value_rand_1> ::= 0.0 | 0.05 | 0.10 | 0.15 | 0.20 | 0.25 | 0.30 | 0.35 | 0.40 | 0.45 | 0.50 | 0.55 | 0.60 | 0.65 | 0.70 | 0.75 | 0.80 | 0.85 | 0.90 | 0.95 | 1.0\n",
    "    <criterion> ::= gini | entropy | log_loss\n",
    "    <splitter> ::= best | random\n",
    "    <max_depth> ::= 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | None\n",
    "    <min_samples_split> ::= 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20\n",
    "    <min_samples_leaf> ::= 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20\n",
    "    <max_features> ::= None | log2 | sqrt\n",
    "    <class_weight> ::= balanced | None\n",
    "    <class_weight_rf> ::= balanced | balanced_subsample | None\n",
    "    <criterion_gb> ::= friedman_mse | squared_error\n",
    "    <loss> ::= log_loss | exponential\n",
    "    <max_leaves> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10\n",
    "    \"\"\"\n",
    "    training_dir = \"/home/alexgcsa/auto-admet/datasets/01_caco2_train.csv\"\n",
    "    testing_dir = \"/home/alexgcsa/auto-admet/datasets/01_caco2_blindtest.csv\"\n",
    "    # Load grammar\n",
    "    grammar = BNFGrammar()\n",
    "    grammar.load_grammar(grammar_text)\n",
    "\n",
    "    # Run GGP\n",
    "    ggp = GrammarBasedGP(grammar, training_dir, testing_dir, fitness_metric=\"auprc\")\n",
    "    best_program = ggp.evolve()\n",
    "\n",
    "    # Print the best program\n",
    "    #print(\"Best Program Found:\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2932db1-9dbb-4d11-a0d2-1e96efb9e062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
