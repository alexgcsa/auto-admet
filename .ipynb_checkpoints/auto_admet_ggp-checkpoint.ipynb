{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d2cfc3c-a8f6-446a-aa89-b7073651d567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION: 0\n",
      "NuSVM\n",
      "SVM\n",
      "NuSVM\n",
      "Toxicophores Fragments # StandardScaler False False # SelectPercentile 45 f_classif # NuSVM linear 5 0.08 1000 balanced--->0.553759815925295\n",
      "Advanced_Descriptors Graph_based_Signatures Fragments # StandardScaler True True # NoFeatureSelection # NuSVM linear 1 0.09 3500 None--->0.5373566762862688\n",
      "General_Descriptors Advanced_Descriptors # MaxAbsScaler # SelectFwe 0.75 f_classif # SVM linear 10 0.07 250 None--->0.5358872372905584\n",
      "General_Descriptors Graph_based_Signatures Toxicophores # MaxAbsScaler # SelectRFE 0.90 0.050 # NuSVM linear 10 0.05 3500 None--->0.5346916145205409\n",
      "Advanced_Descriptors Toxicophores Fragments # RobustScaler True False # SelectRFE 0.20 0.100 # NuSVM linear 10 0.08 2000 None--->0.5098210496701321\n",
      "General_Descriptors Toxicophores # MaxAbsScaler # SelectFwe 0.50 f_classif # NuSVM linear 1 0.09 750 balanced--->0.49845221681096175\n",
      "Graph_based_Signatures Fragments # NoScaling # SelectPercentile 45 f_classif # NuSVM linear 3 0.0000000001 3000 balanced--->0.4685219137350391\n",
      "General_Descriptors Graph_based_Signatures Toxicophores Fragments # NoScaling # NoFeatureSelection # NuSVM linear 9 0.03 100 None--->0.435291333233473\n",
      "General_Descriptors # MinMaxScaler # SelectPercentile 50 chi2 # SVM linear 10 0.03 50 balanced--->0.17284706602628705\n",
      "Graph_based_Signatures Toxicophores # NoScaling # SelectRFE 0.35 0.100 # SVM linear 9 0.07 100 balanced--->0.1680927659824469\n",
      "General_Descriptors Advanced_Descriptors # StandardScaler False False # SelectPercentile 95 f_classif # SVM linear 9 0.08 1000 balanced--->0.15792371869433733\n",
      "General_Descriptors Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments # Normalizer max # SelectFdr 0.25 f_classif # SVM linear 6 0.03 50 None--->0.14890470709455667\n",
      "Toxicophores # StandardScaler False False # NoFeatureSelection # SVM linear 1 0.07 10 None--->0.021285991521685\n",
      "Graph_based_Signatures # Normalizer l2 # VarianceThreshold 0.60 # NuSVM linear 2 0.04 1500 balanced--->0.0\n",
      "General_Descriptors Graph_based_Signatures Fragments # MaxAbsScaler # SelectPercentile 75 chi2 # SVM linear 9 0.0000000001 100 None--->0.0\n",
      "Advanced_Descriptors Toxicophores # NoScaling # SelectFdr 0.90 chi2 # NuSVM linear 2 0.05 750 None--->0.0\n",
      "Advanced_Descriptors Fragments # MaxAbsScaler # VarianceThreshold 0.80 # NuSVM linear 8 0.09 2500 None--->0.0\n",
      "General_Descriptors Toxicophores Fragments # MinMaxScaler # VarianceThreshold 0.70 # SVM linear 3 0.09 10 None--->0.0\n",
      "Graph_based_Signatures Fragments # Normalizer max # VarianceThreshold 0.90 # SVM linear 8 0.07 1500 balanced--->0.0\n",
      "General_Descriptors Graph_based_Signatures Fragments # StandardScaler False True # SelectPercentile 80 chi2 # NuSVM linear 7 0.1 2000 balanced--->0.0\n",
      "-----------------------------------------------\n",
      "GENERATION: 1\n",
      "SVMSVM\n",
      "\n",
      "NuSVM\n",
      "Toxicophores Fragments # StandardScaler False False # SelectPercentile 45 f_classif # NuSVM linear 5 0.08 1000 balanced--->0.553759815925295\n",
      "Advanced_Descriptors Graph_based_Signatures Fragments # StandardScaler True True # VarianceThreshold 0.60 # NuSVM linear 1 0.09 3500 None--->0.5373566762862688\n",
      "Advanced_Descriptors Graph_based_Signatures Fragments # StandardScaler True True # VarianceThreshold 0.60 # NuSVM linear 1 0.09 3500 None--->0.5373566762862688\n",
      "General_Descriptors Advanced_Descriptors # MaxAbsScaler # SelectFwe 0.50 f_classif # SVM linear 10 0.07 250 None--->0.5352818677179165\n",
      "General_Descriptors Advanced_Descriptors # MaxAbsScaler # SelectFwe 0.75 f_classif # SVM linear 9 0.08 1000 balanced--->0.5280112976206203\n",
      "General_Descriptors Toxicophores # MaxAbsScaler # SelectFwe 0.50 f_classif # NuSVM linear 10 0.08 2000 None--->0.5187846810187102\n",
      "General_Descriptors Graph_based_Signatures Toxicophores Fragments # MinMaxScaler # NoFeatureSelection # NuSVM linear 9 0.03 100 None--->0.5146775132645791\n",
      "Advanced_Descriptors Fragments # NoScaling # VarianceThreshold 0.80 # NuSVM linear 8 0.09 2500 None--->0.511155741669895\n",
      "Advanced_Descriptors Toxicophores Fragments # RobustScaler True False # SelectRFE 0.20 0.100 # NuSVM linear 1 0.09 750 balanced--->0.49207411679178836\n",
      "Graph_based_Signatures # Normalizer l2 # NoFeatureSelection # NuSVM linear 2 0.04 1500 balanced--->0.2521476194324846\n",
      "Graph_based_Signatures # Normalizer l2 # NoFeatureSelection # NuSVM linear 2 0.04 1500 balanced--->0.2521476194324846\n",
      "Graph_based_Signatures Toxicophores # MaxAbsScaler # SelectRFE 0.35 0.100 # SVM linear 9 0.07 100 balanced--->0.22919680611080276\n",
      "General_Descriptors Toxicophores Fragments # NoScaling # NoFeatureSelection # NuSVM linear 9 0.03 100 None--->0.2206421789491857\n",
      "General_Descriptors Advanced_Descriptors # MaxAbsScaler # SelectFwe 0.75 f_classif # SVM linear 1 0.07 10 None--->0.18819518149744013\n",
      "Advanced_Descriptors Toxicophores Fragments # RobustScaler True False # SelectRFE 0.20 0.100 # SVM linear 3 0.09 10 None--->0.14371627127594\n",
      "General_Descriptors Advanced_Descriptors # StandardScaler False False # SelectPercentile 95 f_classif # SVM linear 10 0.07 250 None--->0.08859354170664686\n",
      "General_Descriptors # NoScaling # SelectPercentile 50 chi2 # SVM linear 10 0.03 50 balanced--->0.0\n",
      "General_Descriptors Graph_based_Signatures Toxicophores Fragments # MinMaxScaler # VarianceThreshold 0.70 # SVM linear 3 0.09 10 None--->0.0\n",
      "General_Descriptors Toxicophores Fragments # MinMaxScaler # VarianceThreshold 0.70 # NuSVM linear 10 0.08 2000 None--->0.0\n",
      "Toxicophores # StandardScaler False False # NoFeatureSelection # SVM linear 10 0.07 250 None--->-0.013525508963248282\n",
      "-----------------------------------------------\n",
      "GENERATION: 2\n",
      "NuSVM\n",
      "Toxicophores Fragments # MaxAbsScaler # SelectPercentile 45 f_classif # NuSVM linear 5 0.08 1000 balanced--->0.5719300790775494\n",
      "Toxicophores Fragments # StandardScaler False False # SelectPercentile 45 f_classif # NuSVM linear 5 0.08 1000 balanced--->0.553759815925295\n",
      "General_Descriptors Advanced_Descriptors # RobustScaler True True # SelectFwe 0.75 f_classif # SVM linear 9 0.08 1000 balanced--->0.524194467086989\n",
      "General_Descriptors Toxicophores # MaxAbsScaler # SelectFwe 0.50 f_classif # NuSVM linear 10 0.08 2000 None--->0.5187846810187102\n",
      "Graph_based_Signatures # StandardScaler True True # NoFeatureSelection # NuSVM linear 2 0.04 1500 balanced--->0.41906746506089293\n",
      "Graph_based_Signatures # MaxAbsScaler # NoFeatureSelection # NuSVM linear 2 0.04 1500 balanced--->0.41592834280191027\n",
      "Advanced_Descriptors Toxicophores Fragments # RobustScaler True False # NoFeatureSelection # NuSVM linear 1 0.09 750 balanced--->0.4131985012726288\n",
      "General_Descriptors Advanced_Descriptors # Normalizer l2 # SelectFwe 0.75 f_classif # SVM linear 9 0.08 1000 balanced--->0.36752613346198154\n",
      "Graph_based_Signatures # Normalizer l2 # SelectRFE 0.20 0.100 # NuSVM linear 2 0.04 1500 balanced--->0.2990070420326883\n",
      "General_Descriptors Graph_based_Signatures Toxicophores Fragments # MinMaxScaler # SelectPercentile 50 chi2 # SVM linear 3 0.09 10 None--->0.24671244491766872\n",
      "General_Descriptors Graph_based_Signatures Toxicophores Fragments # MinMaxScaler # SelectPercentile 50 chi2 # SVM linear 3 0.09 10 None--->0.24671244491766872\n",
      "General_Descriptors Advanced_Descriptors # StandardScaler False False # SelectFwe 0.75 f_classif # SVM linear 9 0.08 1000 balanced--->0.2329071287113864\n",
      "General_Descriptors # NoScaling # VarianceThreshold 0.70 # SVM linear 10 0.03 50 balanced--->0.2190335819476116\n",
      "General_Descriptors # NoScaling # VarianceThreshold 0.70 # SVM linear 10 0.03 50 balanced--->0.2190335819476116\n",
      "General_Descriptors Advanced_Descriptors # MaxAbsScaler # SelectFwe 0.75 f_classif # SVM linear 1 0.07 10 None--->0.18819518149744013\n",
      "Advanced_Descriptors # MaxAbsScaler # SelectFwe 0.75 f_classif # SVM linear 1 0.07 10 None--->0.12942796588337527\n",
      "General_Descriptors Toxicophores Fragments # NoScaling # NoFeatureSelection # SVM linear 10 0.03 50 balanced--->0.041695153076934506\n",
      "Advanced_Descriptors Graph_based_Signatures Fragments # Normalizer l2 # VarianceThreshold 0.60 # NuSVM linear 1 0.09 3500 None--->0.0\n",
      "Advanced_Descriptors Toxicophores Fragments # RobustScaler True False # SelectPercentile 45 chi2 # SVM linear 3 0.09 10 None--->0.0\n",
      "General_Descriptors # NoScaling # SelectPercentile 50 chi2 # NuSVM linear 9 0.03 100 None--->0.0\n",
      "-----------------------------------------------\n",
      "GENERATION: 3\n",
      "NuSVMNuSVM\n",
      "\n",
      "Toxicophores Fragments # MaxAbsScaler # SelectPercentile 45 f_classif # NuSVM linear 5 0.08 1000 balanced--->0.5719300790775494\n",
      "Advanced_Descriptors # MaxAbsScaler # SelectFwe 0.75 f_classif # NuSVM linear 1 0.09 750 balanced--->0.538570302663221\n",
      "General_Descriptors Toxicophores # MaxAbsScaler # SelectFwe 0.50 f_classif # SVM linear 9 0.08 1000 balanced--->0.5060873264642267\n",
      "General_Descriptors # MaxAbsScaler # NoFeatureSelection # NuSVM linear 2 0.04 1500 balanced--->0.5034107123575486\n",
      "General_Descriptors Toxicophores Fragments # NoScaling # VarianceThreshold 0.70 # NuSVM linear 3 0.08 2500 None--->0.45530591247457824\n",
      "General_Descriptors Advanced_Descriptors # Normalizer l2 # SelectFwe 0.75 f_classif # SVM linear 9 0.08 1000 balanced--->0.36752613346198154\n",
      "Graph_based_Signatures # Normalizer l2 # SelectRFE 0.25 0.025 # NuSVM linear 2 0.04 1500 balanced--->0.25731141455008033\n",
      "General_Descriptors Toxicophores Fragments # Normalizer l2 # SelectRFE 0.20 0.100 # NuSVM linear 2 0.04 1500 balanced--->0.25492611931625825\n",
      "General_Descriptors Advanced_Descriptors # StandardScaler False False # SelectFwe 0.75 f_classif # SVM linear 9 0.08 1000 balanced--->0.2329071287113864\n",
      "General_Descriptors # StandardScaler False False # NoFeatureSelection # SVM linear 10 0.03 50 balanced--->0.1920026653244015\n",
      "Graph_based_Signatures # MaxAbsScaler # NoFeatureSelection # SVM linear 3 0.09 10 None--->0.17115632227858482\n",
      "Graph_based_Signatures # NoScaling # NoFeatureSelection # SVM linear 10 0.03 50 balanced--->0.15896060388377825\n",
      "Advanced_Descriptors Toxicophores Fragments # RobustScaler True False # SelectFwe 0.75 f_classif # SVM linear 3 0.09 10 None--->0.10503103719618512\n",
      "Advanced_Descriptors Toxicophores Fragments # RobustScaler True False # NoFeatureSelection # SVM linear 1 0.07 10 None--->0.10472831623382348\n",
      "General_Descriptors Advanced_Descriptors # Normalizer l2 # SelectFwe 0.75 f_classif # NuSVM linear 10 0.08 2000 None--->0.09412986736054675\n",
      "Graph_based_Signatures # NoScaling # VarianceThreshold 0.70 # SVM linear 10 0.03 50 balanced--->0.08944787307662065\n",
      "General_Descriptors # MaxAbsScaler # SelectPercentile 50 chi2 # NuSVM linear 9 0.03 100 None--->0.0\n",
      "Advanced_Descriptors Toxicophores Fragments # RobustScaler True False # SelectPercentile 45 chi2 # NuSVM linear 2 0.04 1500 balanced--->0.0\n",
      "General_Descriptors Advanced_Descriptors # MaxAbsScaler # SelectPercentile 45 chi2 # SVM linear 1 0.07 10 None--->0.0\n",
      "Graph_based_Signatures # Normalizer l2 # VarianceThreshold 0.60 # NuSVM linear 1 0.09 3500 None--->0.0\n",
      "-----------------------------------------------\n",
      "expABC;0;4;0.1599;0.3483;0.8189;0.2;0.5582;1.0;0.6418;;Toxicophores Fragments # MaxAbsScaler # SelectPercentile 45 f_classif # NuSVM linear 5 0.08 1000 balanced\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SelectFpr, SelectFwe, SelectFdr, chi2, f_classif, RFE\n",
    "import warnings\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef, roc_auc_score, recall_score, average_precision_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import fcntl\n",
    "\n",
    "class BNFGrammar:\n",
    "    def __init__(self):\n",
    "        self.grammar = defaultdict(list)\n",
    "        self.non_terminals = set()\n",
    "        self.terminals = set()\n",
    "        \n",
    "\n",
    "    def load_grammar(self, bnf_text: str):\n",
    "        \"\"\"\n",
    "        Parses the BNF grammar from a string.\n",
    "        \"\"\"\n",
    "        for line in bnf_text.strip().splitlines():\n",
    "            if \"::=\" in line:\n",
    "                lhs, rhs = line.split(\"::=\", 1)\n",
    "                lhs = lhs.strip()\n",
    "                self.non_terminals.add(lhs)\n",
    "                rhs_options = [option.strip() for option in rhs.split(\"|\")]\n",
    "                for option in rhs_options:\n",
    "                    self.grammar[lhs].append(option.split())\n",
    "                    for token in option.split():\n",
    "                        if token not in self.non_terminals:\n",
    "                            self.terminals.add(token)\n",
    "                            \n",
    "\n",
    "    def generate_parse_tree(self, symbol: str = \"<start>\", max_depth: int = 10) -> dict:\n",
    "        \"\"\"\n",
    "        Generates a parse tree starting from the given symbol, ensuring mandatory grammar components are included.\n",
    "        \"\"\"\n",
    "        if max_depth <= 0 or symbol not in self.grammar:\n",
    "            return symbol  # Return the symbol as a terminal if max depth is reached\n",
    "    \n",
    "        # Strictly enforce the `<start>` rule\n",
    "        if symbol == \"<start>\":\n",
    "            # Generate each mandatory component\n",
    "            feature_def = self.generate_parse_tree(\"<feature_definition>\", max_depth - 1)\n",
    "            scaling = self.generate_parse_tree(\"<feature_scaling>\", max_depth - 1)\n",
    "            selection = self.generate_parse_tree(\"<feature_selection>\", max_depth - 1)\n",
    "            ml_algo = self.generate_parse_tree(\"<ml_algorithms>\", max_depth - 1)\n",
    "    \n",
    "            return {symbol: [feature_def, \"#\", scaling, \"#\", selection, \"#\", ml_algo]}\n",
    "    \n",
    "        # Select a random production for other non-terminals\n",
    "        production = random.choice(self.grammar[symbol])\n",
    "        return {symbol: [self.generate_parse_tree(token, max_depth - 1) for token in production]}\n",
    "\n",
    "    \n",
    "    def parse_tree_to_string(self, tree) -> str:\n",
    "        \"\"\"\n",
    "        Reconstructs a string from the parse tree.\n",
    "        \"\"\"\n",
    "        if isinstance(tree, str):\n",
    "            # Leaf node (terminal)\n",
    "            return tree\n",
    "        # Non-terminal with its production rules as children\n",
    "        root, children = list(tree.items())[0]\n",
    "        return \" \".join(self.parse_tree_to_string(child) for child in children)\n",
    "\n",
    "    \n",
    "    def validate_parse_tree(self, tree, symbol=\"<start>\") -> bool:\n",
    "        \"\"\"\n",
    "        Validates if the parse tree conforms to the grammar and respects the `<start>` structure.\n",
    "        \"\"\"\n",
    "        if isinstance(tree, str):\n",
    "            return tree in self.terminals  # Check terminal validity\n",
    "    \n",
    "        if not isinstance(tree, dict) or len(tree) != 1:\n",
    "            return False\n",
    "    \n",
    "        root, children = list(tree.items())[0]\n",
    "        if root != symbol:\n",
    "            return False\n",
    "    \n",
    "        if symbol == \"<start>\":\n",
    "            # Check `<start>` structure\n",
    "            if len(children) != 7:\n",
    "                return False\n",
    "            expected_symbols = [\"<feature_definition>\", \"#\", \"<feature_scaling>\", \"#\", \"<feature_selection>\", \"#\", \"<ml_algorithms>\"]\n",
    "            for i, child_symbol in enumerate(expected_symbols):\n",
    "                if i % 2 == 0 and not self.validate_parse_tree(children[i], child_symbol):  # Validate non-terminals\n",
    "                    return False\n",
    "                if i % 2 == 1 and children[i] != \"#\":  # Ensure separator\n",
    "                    return False\n",
    "    \n",
    "        # Validate other non-terminals\n",
    "        for production in self.grammar[symbol]:\n",
    "            if len(production) == len(children) and all(\n",
    "                self.validate_parse_tree(child, production[i])\n",
    "                for i, child in enumerate(children)\n",
    "            ):\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "        \n",
    "\n",
    "class MLAlgorithmTransformer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def XGBoost(self, n_estimators_str, max_depth_str, max_leaves_str, learning_rate_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        clf = XGBClassifier(n_estimators=int(n_estimators_str), max_depth=max_depth_actual, random_state=42, \n",
    "                            max_leaves=int(max_leaves_str), learning_rate=float(learning_rate_str), n_jobs=1)        \n",
    "    \n",
    "        return clf \n",
    "    \n",
    "    \n",
    "    def GradientBoosting(self, n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, loss_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, random_state=42, \n",
    "                                         min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str), \n",
    "                                         max_features=max_features_actual, loss=loss_str)        \n",
    "    \n",
    "        return clf      \n",
    " \n",
    "    \n",
    "    def ExtraTrees(self, n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "     \n",
    "            \n",
    "        clf = ExtraTreesClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, n_jobs=1, random_state=42, \n",
    "                                   class_weight=class_weight_actual,  min_samples_split=int(min_samples_split_str), \n",
    "                                   min_samples_leaf=int(min_samples_split_str), max_features=max_features_actual)        \n",
    "    \n",
    "        return clf  \n",
    "    \n",
    "    \n",
    "    def RandomForest(self, n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "     \n",
    "        clf = RandomForestClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, n_jobs=1, random_state=42, \n",
    "                                     class_weight=class_weight_actual,  min_samples_split=int(min_samples_split_str), \n",
    "                                     min_samples_leaf=int(min_samples_split_str), max_features=max_features_actual)        \n",
    " \n",
    "        return clf   \n",
    "        \n",
    "    \n",
    "    def ExtraTree(self, criterion_str, splitter_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "            \n",
    "        clf = ExtraTreeClassifier(criterion=criterion_str, splitter='best', max_depth=max_depth_actual, \n",
    "                                  min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str),                                      \n",
    "                                  max_features=max_features_actual, random_state=0)      \n",
    "    \n",
    "        return clf  \n",
    "            \n",
    "    \n",
    "    def DecisionTree(self, criterion_str, splitter_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "\n",
    "\n",
    "        clf = DecisionTreeClassifier(criterion=criterion_str, splitter=splitter_str, max_depth=max_depth_actual, \n",
    "                                     min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str), \n",
    "                                     max_features=max_features_actual, random_state=0,\n",
    "                                     class_weight=class_weight_actual)      \n",
    "    \n",
    "        return clf\n",
    "    \n",
    "    \n",
    "    def AdaBoost(self, alg, n_est, lr):\n",
    "        clf = AdaBoostClassifier(n_estimators=n_est, learning_rate=lr, algorithm=alg, random_state=0)\n",
    "        return clf\n",
    "\n",
    "    def SVM(self, kernel, degree, tol, max_iter, class_weight):\n",
    "  \n",
    "        #<class_weight> ::= balanced | None \n",
    "        actual_class_weight = None\n",
    "        if(class_weight == \"balanced\"):\n",
    "            actual_class_weight = \"balanced\"\n",
    "            \n",
    "        clf = SVC(kernel=str(kernel), degree=int(degree), probability=True, tol=float(tol), class_weight=actual_class_weight, max_iter=int(max_iter), random_state=0)\n",
    "       \n",
    "        return clf\n",
    "\n",
    "    def NuSVM(self, kernel, degree, tol, max_iter, class_weight):\n",
    "  \n",
    "        #<class_weight> ::= balanced | None \n",
    "        actual_class_weight = None\n",
    "        if(class_weight == \"balanced\"):\n",
    "            actual_class_weight = \"balanced\"\n",
    "            \n",
    "        clf = NuSVC(kernel=str(kernel), degree=int(degree), probability=True, tol=float(tol), class_weight=actual_class_weight, max_iter=int(max_iter), random_state=0)\n",
    "       \n",
    "        return clf\n",
    "\n",
    "\n",
    "    def NeuroNets(self, ml_algorithm_options):\n",
    "        if(len(ml_algorithm_options)==6):\n",
    "            hls = (int(ml_algorithm_options[0]),)\n",
    "            af = ml_algorithm_options[1]\n",
    "            sol = ml_algorithm_options[2]\n",
    "            lr = ml_algorithm_options[3]\n",
    "            mi = int(ml_algorithm_options[4])\n",
    "            t = float(ml_algorithm_options[5])\n",
    "        elif(len(ml_algorithm_options)==7):\n",
    "            hls = (int(ml_algorithm_options[0]), ml_algorithm_options[1])\n",
    "            af = ml_algorithm_options[2]\n",
    "            sol = ml_algorithm_options[3]\n",
    "            lr = ml_algorithm_options[4]\n",
    "            mi = int(ml_algorithm_options[5])\n",
    "            t = float(ml_algorithm_options[6])            \n",
    "        elif(len(ml_algorithm_options)==8):\n",
    "            hls = (int(ml_algorithm_options[0]), ml_algorithm_options[1], ml_algorithm_options[2])\n",
    "            af = ml_algorithm_options[3]\n",
    "            sol = ml_algorithm_options[4]\n",
    "            lr = ml_algorithm_options[5]\n",
    "            mi = int(ml_algorithm_options[6])\n",
    "            t = float(ml_algorithm_options[7]) \n",
    "        \n",
    "        clf = MLPClassifier(hidden_layer_sizes=hls, activation=af, solver=sol, learning_rate=lr, \n",
    "                          max_iter=mi, random_state=0, tol=t, early_stopping=True)\n",
    "\n",
    "        return clf\n",
    "\n",
    "\n",
    "class FeatureSelectionTransformer:\n",
    "    def __init__(self, training_df, testing_df, training_label_col, error_log):\n",
    "        self.training_df = training_df\n",
    "        self.testing_df = testing_df\n",
    "        self.training_label_col = training_label_col\n",
    "        self.error_log = error_log\n",
    "        self.model = None   \n",
    "\n",
    "    def select_ml_algorithms(self, ml_algorithm):\n",
    "        ml_alg_selection = MLAlgorithmTransformer()\n",
    "        if ml_algorithm[0] == \"AdaBoostClassifier\":\n",
    "            return ml_alg_selection.AdaBoost(str(ml_algorithm[1]), int(ml_algorithm[2]), float(ml_algorithm[3]))\n",
    "        elif ml_algorithm[0] == \"DecisionTreeClassifier\":\n",
    "            return ml_alg_selection.DecisionTree(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])    \n",
    "        elif ml_algorithm[0] == \"ExtraTreeClassifier\":\n",
    "            return ml_alg_selection.ExtraTree(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"RandomForestClassifier\":\n",
    "            return ml_alg_selection.RandomForest(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"ExtraTreesClassifier\":\n",
    "            return ml_alg_selection.ExtraTrees(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"GradientBoostingClassifier\":\n",
    "            return ml_alg_selection.GradientBoosting(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7]) \n",
    "        elif ml_algorithm[0] == \"XGBClassifier\":\n",
    "            return ml_alg_selection.XGBoost(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4])         \n",
    "        elif ml_algorithm[0] == \"SVM\":\n",
    "            return ml_alg_selection.SVM(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5]) \n",
    "        elif ml_algorithm[0] == \"NuSVM\":\n",
    "            return ml_alg_selection.SVM(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5])             \n",
    "        elif ml_algorithm[0] == \"NeuroNets\":\n",
    "            return ml_alg_selection.NeuroNets(ml_algorithm[1:])             \n",
    "                                              \n",
    "        else:\n",
    "            return None        \n",
    "\n",
    "    def select_fwe(self, alpha_str, score_function_str):\n",
    "    \n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            self.model = SelectFwe(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.training_df, self.training_label_col)\n",
    "            #df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            cols_idxs = self.model.get_support(indices=True)\n",
    "            features_df_new = self.training_df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:\n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature selection - select_fwe\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file         \n",
    "            return None \n",
    "            \n",
    "    \n",
    "    def select_fdr(self, alpha_str, score_function_str):\n",
    "    \n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            self.model = SelectFdr(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.training_df, self.training_label_col)\n",
    "            #df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            cols_idxs = self.model.get_support(indices=True)\n",
    "            features_df_new = self.training_df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:\n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature selection - select_fdr\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file        \n",
    "            return None\n",
    "            \n",
    "    \n",
    "    def select_fpr(self, alpha_str, score_function_str):    \n",
    "        score_function_actual = f_classif    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2      \n",
    "        \n",
    "        try:\n",
    "            self.model = SelectFpr(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.training_df, self.training_label_col)      \n",
    "            cols_idxs = self.model.get_support(indices=True)\n",
    "            features_df_new = self.training_df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:\n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature selection - select_fpr\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file           \n",
    "            return None\n",
    "            \n",
    "    \n",
    "    def select_percentile(self, percentile_str, score_function_str):\n",
    "        score_function_actual = f_classif    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "            \n",
    "        try:\n",
    "            self.model = SelectPercentile(score_func=score_function_actual, percentile = int(percentile_str)).fit(self.training_df, self.training_label_col)       \n",
    "            cols_idxs = self.model.get_support(indices=True)\n",
    "            features_df_new = self.training_df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:\n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature selection - select_percentile\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file           \n",
    "            return None\n",
    "            \n",
    "    \n",
    "    def variance_threshold(self,thrsh):\n",
    "        try:\n",
    "            self.model =VarianceThreshold(threshold=thrsh).fit(self.training_df, self.training_label_col)      \n",
    "            cols_idxs = self.model.get_support(indices=True)\n",
    "            features_df_new = self.training_df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:            \n",
    "            with open(self.error_log, \"a\") as f:\n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature selection - variance_threshold\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file \n",
    "            return None\n",
    "\n",
    "    def select_rfe(self, n_features_to_select_rfe, step_rfe, ml_algorithm):\n",
    "        try:\n",
    "            \n",
    "            estimator = self.select_ml_algorithms(ml_algorithm) \n",
    "            self.model = RFE(estimator, n_features_to_select=float(n_features_to_select_rfe), step=float(step_rfe)).fit(self.training_df, self.training_label_col)      \n",
    "                           \n",
    "            cols_idxs = self.model.get_support(indices=True)\n",
    "            features_df_new = self.training_df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:\n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature selection - variance_threshold\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file \n",
    "            return None    \n",
    "\n",
    "    def apply_model(self):\n",
    "        try:\n",
    "            self.testing_df = self.testing_df[self.training_df.columns]\n",
    "            cols_idxs = self.model.get_support(indices=True)\n",
    "            features_df_new_testing = self.testing_df.iloc[:,cols_idxs]\n",
    "            df_np_testing = pd.DataFrame(self.model.transform(self.testing_df), columns=features_df_new_testing.columns)\n",
    "            return features_df_new_testing\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:\n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature selection - apply model\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file \n",
    "            return None\n",
    "        \n",
    "\n",
    "\n",
    "class ScalingTransformer:\n",
    "    def __init__(self, training_df, testing_df, error_log):\n",
    "        self.training_df = training_df\n",
    "        self.testing_df = testing_df\n",
    "        self.error_log = error_log\n",
    "        self.model = None\n",
    "\n",
    "    def normalizer(self, norm_hp):\n",
    "        try:\n",
    "            self.model = Normalizer(norm=norm_hp).fit(self.training_df)\n",
    "            df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.training_df.columns)\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:\n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature scaling - scaling normalizer\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file           \n",
    "            return None\n",
    "\n",
    "    \n",
    "    def max_abs_scaler(self):\n",
    "        try:\n",
    "            self.model = MaxAbsScaler().fit(self.training_df)\n",
    "            df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.training_df.columns)\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:\n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature scaling - max_abs_scaler\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file            \n",
    "            return None\n",
    "\n",
    "    \n",
    "    def min_max_scaler(self):\n",
    "        try:\n",
    "            self.model = MinMaxScaler().fit(self.training_df)\n",
    "            df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.training_df.columns)\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:\n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature scaling - min_max_scaler\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file         \n",
    "            return None \n",
    "\n",
    "    \n",
    "    def standard_scaler(self, with_mean_str, with_std_str):\n",
    "        with_mean_actual = True\n",
    "        with_std_actual = True\n",
    "    \n",
    "        if with_mean_str == \"False\":\n",
    "            with_mean_actual = False\n",
    "        if with_std_str == \"False\":\n",
    "            with_std_actual = False        \n",
    "        try:\n",
    "            self.model = StandardScaler(with_mean=with_mean_actual, with_std=with_std_actual).fit(self.training_df)\n",
    "            df_np = self.model.transform(self.training_df)    \n",
    "            return pd.DataFrame(df_np, columns = self.training_df.columns)\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:            \n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature scaling - standard_scaler\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def robust_scaler(self, with_centering_str, with_scaling_str):\n",
    "        with_centering_actual = True\n",
    "        with_scaling_actual = True\n",
    "    \n",
    "        if with_centering_str == \"False\":\n",
    "            with_centering_actual = False\n",
    "        if with_scaling_str == \"False\":\n",
    "            with_scaling_actual = False        \n",
    "        try:\n",
    "            self.model = RobustScaler(with_centering=with_centering_actual, with_scaling=with_scaling_actual).fit(self.training_df)\n",
    "            df_np = self.model.transform(self.training_df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.training_df.columns)\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:            \n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature scaling - robust_scaler\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file            \n",
    "            return None\n",
    "\n",
    "    def apply_model(self):\n",
    "        try:\n",
    "            df_np_testing = pd.DataFrame(self.model.transform(self.testing_df), columns = self.testing_df.columns)\n",
    "            return df_np_testing\n",
    "        except Exception as e:\n",
    "            with open(self.error_log, \"a\") as f:            \n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature scaling - apply model\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file \n",
    "            return None\n",
    "        \n",
    "\n",
    "\n",
    "class GrammarBasedGP:\n",
    "    def __init__(self, grammar, training_dir, testing_dir, fitness_cache={}, num_cores=20, time_budget_minutes_alg_eval = 3, \n",
    "                 population_size=20, max_generations=3, max_time=5, mutation_rate=0.15, crossover_rate=0.8, \n",
    "                 crossover_mutation_rate=0.05, elitism_size=1, fitness_metric=\"auc\", \n",
    "                 experiment_name = \"expABC\", stopping_criterion = \"time\", seed=0):\n",
    "        self.grammar = grammar\n",
    "        self.training_dir = training_dir\n",
    "        self.testing_dir = testing_dir\n",
    "        self.fitness_cache = fitness_cache\n",
    "        self.num_cores = num_cores\n",
    "        self.time_budget_minutes_alg_eval = time_budget_minutes_alg_eval\n",
    "        self.population_size = population_size\n",
    "        self.max_generations = max_generations\n",
    "        self.max_time = max_time\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.crossover_mutation_rate = crossover_mutation_rate\n",
    "        self.elitism_size = elitism_size\n",
    "        self.fitness_metric = fitness_metric\n",
    "        self.experiment_name = experiment_name\n",
    "        self.stopping_criterion = stopping_criterion\n",
    "        self.seed = seed        \n",
    "        self.population = []\n",
    "\n",
    "    def select_ml_algorithms(self, ml_algorithm):        \n",
    "        ml_alg_selection = MLAlgorithmTransformer()\n",
    "        if ml_algorithm[0] == \"AdaBoostClassifier\":\n",
    "            return ml_alg_selection.AdaBoost(str(ml_algorithm[1]), int(ml_algorithm[2]), float(ml_algorithm[3]))\n",
    "        elif ml_algorithm[0] == \"DecisionTreeClassifier\":\n",
    "            return ml_alg_selection.DecisionTree(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])    \n",
    "        elif ml_algorithm[0] == \"ExtraTreeClassifier\":\n",
    "            return ml_alg_selection.ExtraTree(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"RandomForestClassifier\":\n",
    "            return ml_alg_selection.RandomForest(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"ExtraTreesClassifier\":\n",
    "            return ml_alg_selection.ExtraTrees(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"GradientBoostingClassifier\":\n",
    "            return ml_alg_selection.GradientBoosting(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7]) \n",
    "        elif ml_algorithm[0] == \"XGBClassifier\":\n",
    "            return ml_alg_selection.XGBoost(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4])  \n",
    "        elif ml_algorithm[0] == \"SVM\":\n",
    "            return ml_alg_selection.SVM(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5])             \n",
    "        elif ml_algorithm[0] == \"NuSVM\":\n",
    "            return ml_alg_selection.NuSVM(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5])             \n",
    "        elif ml_algorithm[0] == \"NeuroNets\":\n",
    "            return ml_alg_selection.NeuroNets(ml_algorithm[1:])             \n",
    "                                                                  \n",
    "        else:\n",
    "            return None    \n",
    "\n",
    "    \n",
    "    def select_features(self, feature_selection, ml_algorithm, training_dataset_df, training_label_col, testing_dataset_df=None, testing=False):\n",
    "\n",
    "        cp_training_dataset_df = training_dataset_df.copy(deep=True)\n",
    "        cp_testing_datset_df = None        \n",
    "        if(testing):\n",
    "            cp_testing_datset_df = testing_dataset_df.copy(deep=True)\n",
    "\n",
    "        cp_training_label_col = training_label_col.copy(deep=True)\n",
    "        error_log = self.experiment_name + \"_error.log\"\n",
    "        feature_selection_transformer = FeatureSelectionTransformer(cp_training_dataset_df, cp_testing_datset_df, cp_training_label_col, error_log)\n",
    "        mod_training_dataset_df = None\n",
    "        mod_testing_dataset_df = None        \n",
    "        if feature_selection[0] == \"NoFeatureSelection\":\n",
    "            if(testing):\n",
    "                return training_dataset_df, testing_dataset_df\n",
    "            else:\n",
    "                return training_dataset_df\n",
    "        elif feature_selection[0] == \"VarianceThreshold\":\n",
    "            mod_training_dataset_df = feature_selection_transformer.variance_threshold(float(feature_selection[1]))\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = feature_selection_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df\n",
    "\n",
    "        elif feature_selection[0] == \"SelectPercentile\":        \n",
    "            mod_training_dataset_df = feature_selection_transformer.select_percentile(feature_selection[1], feature_selection[2])    \n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = feature_selection_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df   \n",
    "        elif feature_selection[0] == \"SelectFpr\":        \n",
    "            mod_training_dataset_df = feature_selection_transformer.select_fpr(feature_selection[1], feature_selection[2])    \n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = feature_selection_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df  \n",
    "        elif feature_selection[0] == \"SelectFdr\":        \n",
    "            mod_training_dataset_df = feature_selection_transformer.select_fdr(feature_selection[1], feature_selection[2])    \n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = feature_selection_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df  \n",
    "        elif feature_selection[0] == \"SelectFwe\":        \n",
    "            mod_training_dataset_df = feature_selection_transformer.select_fwe(feature_selection[1], feature_selection[2])    \n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = feature_selection_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df  \n",
    "        elif feature_selection[0] == \"SelectRFE\":        \n",
    "            mod_training_dataset_df = feature_selection_transformer.select_rfe(feature_selection[1], feature_selection[2], ml_algorithm)    \n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = feature_selection_transformer.apply_model()               \n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df                  \n",
    "        else:\n",
    "            return None       \n",
    "\n",
    "    \n",
    "    def scale_features(self, feature_scaling, training_dataset_df, testing_dataset_df=None, testing=False):\n",
    "\n",
    "        cp_training_dataset_df = training_dataset_df.copy(deep=True)\n",
    "        cp_testing_datset_df = None\n",
    "        if(testing):\n",
    "            cp_testing_datset_df = testing_dataset_df.copy(deep=True)\n",
    "        error_log = self.experiment_name + \"_error.log\"\n",
    "        scaling_transformer = ScalingTransformer(cp_training_dataset_df, cp_testing_datset_df, error_log)\n",
    "        mod_training_dataset_df = None\n",
    "        mod_testing_dataset_df = None\n",
    "        if feature_scaling[0] == \"NoScaling\":\n",
    "            if(testing):\n",
    "                return training_dataset_df, testing_dataset_df\n",
    "            else:\n",
    "                return training_dataset_df\n",
    "            \n",
    "        elif feature_scaling[0] == \"Normalizer\":\n",
    "            mod_training_dataset_df = scaling_transformer.normalizer(str(feature_scaling[1]))\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = scaling_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df\n",
    "        elif feature_scaling[0] == \"MinMaxScaler\":\n",
    "            mod_training_dataset_df = scaling_transformer.min_max_scaler()\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = scaling_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df\n",
    "        elif feature_scaling[0] == \"MaxAbsScaler\":\n",
    "            mod_training_dataset_df = scaling_transformer.max_abs_scaler()\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = scaling_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df  \n",
    "        elif feature_scaling[0] == \"StandardScaler\":\n",
    "            mod_training_dataset_df  = scaling_transformer.standard_scaler(feature_scaling[1], feature_scaling[2])\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = scaling_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df\n",
    "        elif feature_scaling[0] == \"RobustScaler\":\n",
    "            mod_training_dataset_df = scaling_transformer.robust_scaler(feature_scaling[1], feature_scaling[2])\n",
    "            if(testing):\n",
    "                mod_testing_dataset_df = scaling_transformer.apply_model()\n",
    "                return mod_training_dataset_df, mod_testing_dataset_df\n",
    "            else:\n",
    "                return mod_training_dataset_df      \n",
    "        else:            \n",
    "            return None    \n",
    "    \n",
    "\n",
    "\n",
    "    def represent_molecules(self, list_of_feature_types, training_dataset_df, testing_dataset_df=None, testing=False):\n",
    "        \"\"\"\n",
    "        represents a chemical dataset with descriptors.\n",
    "        \"\"\"          \n",
    "    \n",
    "        columns = []\n",
    "        for lft in list_of_feature_types:\n",
    "            if lft == \"General_Descriptors\":\n",
    "                columns += [\"HeavyAtomCount\",\"MolLogP\",\"NumHeteroatoms\",\"NumRotatableBonds\",\"RingCount\",\"TPSA\",\"LabuteASA\",\"MolWt\",\"FCount\",\"FCount2\",\"Acceptor_Count\",\"Aromatic_Count\",\"Donor_Count\",\"Hydrophobe_Count\",\"NegIonizable_Count\",\"PosIonizable_Count\",]\n",
    "            elif lft == \"Advanced_Descriptors\":\n",
    "                columns += [\"BalabanJ\",\"BertzCT\",\"Chi0\",\"Chi0n\",\"Chi0v\",\"Chi1\",\"Chi1n\",\"Chi1v\",\"Chi2n\",\"Chi2v\",\"Chi3n\",\"Chi3v\",\"Chi4n\",\"Chi4v\",\"HallKierAlpha\",\"Kappa1\",\"Kappa2\",\"Kappa3\",\"NHOHCount\",\"NOCount\",\"PEOE_VSA1\",\"PEOE_VSA10\",\"PEOE_VSA11\",\"PEOE_VSA12\",\"PEOE_VSA13\",\"PEOE_VSA14\",\"PEOE_VSA2\",\"PEOE_VSA3\",\"PEOE_VSA4\",\"PEOE_VSA5\",\"PEOE_VSA6\",\"PEOE_VSA7\",\"PEOE_VSA8\",\"PEOE_VSA9\",\"SMR_VSA1\",\"SMR_VSA10\",\"SMR_VSA2\",\"SMR_VSA3\",\"SMR_VSA4\",\"SMR_VSA5\",\"SMR_VSA6\",\"SMR_VSA7\",\"SMR_VSA8\",\"SMR_VSA9\",\"SlogP_VSA1\",\"SlogP_VSA10\",\"SlogP_VSA11\",\"SlogP_VSA12\",\"SlogP_VSA2\",\"SlogP_VSA3\",\"SlogP_VSA4\",\"SlogP_VSA5\",\"SlogP_VSA6\",\"SlogP_VSA7\",\"SlogP_VSA8\",\"SlogP_VSA9\",\"VSA_EState1\",\"VSA_EState10\",\"VSA_EState2\",\"VSA_EState3\",\"VSA_EState4\",\"VSA_EState5\",\"VSA_EState6\",\"VSA_EState7\",\"VSA_EState8\",\"VSA_EState9\"]\n",
    "            elif lft == \"Toxicophores\":\n",
    "                columns += [\"Tox_1\",\"Tox_2\",\"Tox_3\",\"Tox_4\",\"Tox_5\",\"Tox_6\",\"Tox_7\",\"Tox_8\",\"Tox_9\",\"Tox_10\",\"Tox_11\",\"Tox_12\",\"Tox_13\",\"Tox_14\",\"Tox_15\",\"Tox_16\",\"Tox_17\",\"Tox_18\",\"Tox_19\",\"Tox_20\",\"Tox_21\",\"Tox_22\",\"Tox_23\",\"Tox_24\",\"Tox_25\",\"Tox_26\",\"Tox_27\",\"Tox_28\",\"Tox_29\",\"Tox_30\",\"Tox_31\",\"Tox_32\",\"Tox_33\",\"Tox_34\",\"Tox_35\",\"Tox_36\"]\n",
    "            elif lft == \"Fragments\":\n",
    "                columns += [\"fr_Al_COO\",\"fr_Al_OH\",\"fr_Al_OH_noTert\",\"fr_ArN\",\"fr_Ar_COO\",\"fr_Ar_N\",\"fr_Ar_NH\",\"fr_Ar_OH\",\"fr_COO\",\"fr_COO2\",\"fr_C_O\",\"fr_C_O_noCOO\",\"fr_C_S\",\"fr_HOCCN\",\"fr_Imine\",\"fr_NH0\",\"fr_NH1\",\"fr_NH2\",\"fr_N_O\",\"fr_Ndealkylation1\",\"fr_Ndealkylation2\",\"fr_Nhpyrrole\",\"fr_SH\",\"fr_aldehyde\",\"fr_alkyl_carbamate\",\"fr_alkyl_halide\",\"fr_allylic_oxid\",\"fr_amide\",\"fr_amidine\",\"fr_aniline\",\"fr_aryl_methyl\",\"fr_azide\",\"fr_azo\",\"fr_barbitur\",\"fr_benzene\",\"fr_benzodiazepine\",\"fr_bicyclic\",\"fr_diazo\",\"fr_dihydropyridine\",\"fr_epoxide\",\"fr_ester\",\"fr_ether\",\"fr_furan\",\"fr_guanido\",\"fr_halogen\",\"fr_hdrzine\",\"fr_hdrzone\",\"fr_imidazole\",\"fr_imide\",\"fr_isocyan\",\"fr_isothiocyan\",\"fr_ketone\",\"fr_ketone_Topliss\",\"fr_lactam\",\"fr_lactone\",\"fr_methoxy\",\"fr_morpholine\",\"fr_nitrile\",\"fr_nitro\",\"fr_nitro_arom\",\"fr_nitro_arom_nonortho\",\"fr_nitroso\",\"fr_oxazole\",\"fr_oxime\",\"fr_para_hydroxylation\",\"fr_phenol\",\"fr_phenol_noOrthoHbond\",\"fr_phos_acid\",\"fr_phos_ester\",\"fr_piperdine\",\"fr_piperzine\",\"fr_priamide\",\"fr_prisulfonamd\",\"fr_pyridine\",\"fr_quatN\",\"fr_sulfide\",\"fr_sulfonamd\",\"fr_sulfone\",\"fr_term_acetylene\",\"fr_tetrazole\",\"fr_thiazole\",\"fr_thiocyan\",\"fr_thiophene\",\"fr_unbrch_alkane\",\"fr_urea\"]\n",
    "            elif lft == \"Graph_based_Signatures\":\n",
    "                columns += [\"Acceptor:Acceptor-6.00\",\"Acceptor:Aromatic-6.00\",\"Acceptor:Donor-6.00\",\"Acceptor:Hydrophobe-6.00\",\"Acceptor:NegIonizable-6.00\",\"Acceptor:PosIonizable-6.00\",\"Aromatic:Aromatic-6.00\",\"Aromatic:Donor-6.00\",\"Aromatic:Hydrophobe-6.00\",\"Aromatic:NegIonizable-6.00\",\"Aromatic:PosIonizable-6.00\",\"Donor:Donor-6.00\",\"Donor:Hydrophobe-6.00\",\"Donor:NegIonizable-6.00\",\"Donor:PosIonizable-6.00\",\"Hydrophobe:Hydrophobe-6.00\",\"Hydrophobe:NegIonizable-6.00\",\"Hydrophobe:PosIonizable-6.00\",\"NegIonizable:NegIonizable-6.00\",\"NegIonizable:PosIonizable-6.00\",\"PosIonizable:PosIonizable-6.00\",\"Acceptor:Acceptor-4.00\",\"Acceptor:Aromatic-4.00\",\"Acceptor:Donor-4.00\",\"Acceptor:Hydrophobe-4.00\",\"Acceptor:NegIonizable-4.00\",\"Acceptor:PosIonizable-4.00\",\"Aromatic:Aromatic-4.00\",\"Aromatic:Donor-4.00\",\"Aromatic:Hydrophobe-4.00\",\"Aromatic:NegIonizable-4.00\",\"Aromatic:PosIonizable-4.00\",\"Donor:Donor-4.00\",\"Donor:Hydrophobe-4.00\",\"Donor:NegIonizable-4.00\",\"Donor:PosIonizable-4.00\",\"Hydrophobe:Hydrophobe-4.00\",\"Hydrophobe:NegIonizable-4.00\",\"Hydrophobe:PosIonizable-4.00\",\"NegIonizable:NegIonizable-4.00\",\"NegIonizable:PosIonizable-4.00\",\"PosIonizable:PosIonizable-4.00\",\"Acceptor:Acceptor-2.00\",\"Acceptor:Aromatic-2.00\",\"Acceptor:Donor-2.00\",\"Acceptor:Hydrophobe-2.00\",\"Acceptor:NegIonizable-2.00\",\"Acceptor:PosIonizable-2.00\",\"Aromatic:Aromatic-2.00\",\"Aromatic:Donor-2.00\",\"Aromatic:Hydrophobe-2.00\",\"Aromatic:NegIonizable-2.00\",\"Aromatic:PosIonizable-2.00\",\"Donor:Donor-2.00\",\"Donor:Hydrophobe-2.00\",\"Donor:NegIonizable-2.00\",\"Donor:PosIonizable-2.00\",\"Hydrophobe:Hydrophobe-2.00\",\"Hydrophobe:NegIonizable-2.00\",\"Hydrophobe:PosIonizable-2.00\",\"NegIonizable:NegIonizable-2.00\",\"NegIonizable:PosIonizable-2.00\",\"PosIonizable:PosIonizable-2.00\"]\n",
    "            \n",
    "        mod_training_dataset_df = None\n",
    "        mod_testing_dataset_df = None\n",
    "        try:\n",
    "            cp_training_dataset_df = training_dataset_df.copy(deep=True)\n",
    "            mod_training_dataset_df = cp_training_dataset_df[columns]\n",
    "\n",
    "            if(testing):\n",
    "                cp_testing_dataset_df = testing_dataset_df.copy(deep=True)\n",
    "                mod_testing_dataset_df = cp_testing_dataset_df[columns]                \n",
    "                \n",
    "        except:\n",
    "            error_log = self.experiment_name + \"_error.log\"\n",
    "            with open(error_log, \"a\") as f:            \n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on feature representation\" + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file \n",
    "    \n",
    "        if(testing):\n",
    "            return mod_training_dataset_df, mod_testing_dataset_df\n",
    "        else:\n",
    "            return mod_training_dataset_df   \n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_train_test(self, pipeline):\n",
    "        \"\"\"\n",
    "        Evaluates the pipeline on training and testing, performing each step of the ML pipeline.\n",
    "        \"\"\"  \n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        #all the steps in Auto-ADMET pipeline:\n",
    "        pipeline_string = self.grammar.parse_tree_to_string(pipeline)\n",
    "        pipeline_list = pipeline_string.split(\" # \")\n",
    "        representation = pipeline_list[0].split(\" \")\n",
    "        feature_scaling = pipeline_list[1].split(\" \")\n",
    "        feature_selection = pipeline_list[2].split(\" \")\n",
    "        ml_algorithm = pipeline_list[3].split(\" \")\n",
    "\n",
    "        #applying the steps to an actual dataset:\n",
    "        training_dataset_df = pd.read_csv(self.training_dir, header=0, sep=\",\")\n",
    "        training_label_col = training_dataset_df[\"CLASS\"]\n",
    "        training_dataset_df = training_dataset_df.drop(\"CLASS\", axis=1)\n",
    "        training_dataset_df = training_dataset_df.drop(\"ID\", axis=1)\n",
    "        training_dataset_df_cols = training_dataset_df.columns\n",
    "        \n",
    "        testing_dataset_df = pd.read_csv(self.testing_dir, header=0, sep=\",\")\n",
    "        testing_label_col = testing_dataset_df[\"CLASS\"]\n",
    "        testing_dataset_df = testing_dataset_df.drop(\"CLASS\", axis=1)\n",
    "        testing_dataset_df = testing_dataset_df.drop(\"ID\", axis=1)\n",
    "        testing_dataset_df = testing_dataset_df[training_dataset_df_cols]\n",
    "\n",
    "        rep_training_dataset_df, rep_testing_dataset_df = self.represent_molecules(representation, training_dataset_df, testing_dataset_df, True)\n",
    "        prep_training_dataset_df, prep_testing_dataset_df = self.scale_features(feature_scaling, rep_training_dataset_df, rep_testing_dataset_df, True)\n",
    "        sel_training_dataset_df, sel_testing_dataset_df = self.select_features(feature_selection, ml_algorithm, prep_training_dataset_df, training_label_col, testing_dataset_df, True)\n",
    "        sel_testing_dataset_df = sel_testing_dataset_df[sel_training_dataset_df.columns]\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            ml_algorithm  = self.select_ml_algorithms(ml_algorithm)\n",
    "            ml_model = ml_algorithm.fit(sel_training_dataset_df, training_label_col)\n",
    "            predictions = ml_model.predict(sel_testing_dataset_df)\n",
    "            probabilities = ml_model.predict_proba(sel_testing_dataset_df)[:, 1]\n",
    "            actuals = np.array(testing_label_col)\n",
    "            \n",
    "            mcc_test = round(matthews_corrcoef(actuals, predictions), 4)\n",
    "            auc_test = round(roc_auc_score(actuals, probabilities), 4)\n",
    "            rec_test = round(recall_score(actuals, predictions), 4)\n",
    "            apr_test = round(average_precision_score(actuals, predictions), 4)\n",
    "            prec_test = round(precision_score(actuals, predictions), 4)\n",
    "            acc_test = round(accuracy_score(actuals, predictions), 4)\n",
    "            return mcc_test, auc_test, rec_test, apr_test, prec_test, acc_test        \n",
    "        except Exception as e:\n",
    "            error_log = self.experiment_name + \"_error.log\"\n",
    "            with open(error_log, \"a\") as f:            \n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on pipeline - fitting\" + \"\\n\")\n",
    "                f.write(pipeline_string + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file             \n",
    "            return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    def evaluate_fitness(self, pipeline, dataset_path, time_budget_minutes_alg_eval):\n",
    "        \"\"\"\n",
    "        evaluates pipeline with the fitness, performing each step of the ML pipeline.\n",
    "        \"\"\"  \n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        #all the steps in Auto-ADMET pipeline:\n",
    "        pipeline_string = self.grammar.parse_tree_to_string(pipeline)\n",
    "        pipeline_list = pipeline_string.split(\" # \")\n",
    "        representation = pipeline_list[0].split(\" \")\n",
    "        feature_scaling = pipeline_list[1].split(\" \")\n",
    "        feature_selection = pipeline_list[2].split(\" \")\n",
    "        ml_algorithm = pipeline_list[3].split(\" \")\n",
    "\n",
    "        #applying the steps to an actual dataset:\n",
    "        dataset_df = pd.read_csv(self.training_dir, header=0, sep=\",\")\n",
    "        label_col = dataset_df[\"CLASS\"]\n",
    "        dataset_df = dataset_df.drop(\"CLASS\", axis=1)\n",
    "        dataset_df = dataset_df.drop(\"ID\", axis=1)\n",
    "\n",
    "        rep_dataset_df = self.represent_molecules(representation, dataset_df)\n",
    "        if(rep_dataset_df is None):\n",
    "            return 0.0\n",
    "     \n",
    "        prep_dataset_df = self.scale_features(feature_scaling, rep_dataset_df)\n",
    "        if(prep_dataset_df is None):\n",
    "            return 0.0\n",
    "            \n",
    "        sel_dataset_df = self.select_features(feature_selection, ml_algorithm, prep_dataset_df, label_col)        \n",
    "        if(sel_dataset_df is None):\n",
    "            return 0.0\n",
    "\n",
    "        ml_algorithm  = self.select_ml_algorithms(ml_algorithm)\n",
    "        sel_dataset_df[\"CLASS\"] = pd.Series(label_col)\n",
    "        \n",
    "        final_scores = []\n",
    "        trials = range(3)\n",
    "        for t in trials: \n",
    "            current_seed = self.seed + t\n",
    "            outer_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=current_seed)\n",
    "            try:\n",
    "                y = sel_dataset_df.iloc[:,-1:]\n",
    "                X = sel_dataset_df[sel_dataset_df.columns[:-1]]\n",
    "                scores = None\n",
    "                \n",
    "                if(self.fitness_metric == \"auc\"):                \n",
    "                    scores = cross_val_score(ml_algorithm, X, y, cv=outer_cv, scoring=make_scorer(roc_auc_score))\n",
    "                elif(self.fitness_metric == \"mcc\"):            \n",
    "                    scores = cross_val_score(ml_algorithm, X, y, cv=outer_cv, scoring=make_scorer(matthews_corrcoef))\n",
    "                elif(self.fitness_metric == \"recall\"):\n",
    "                    scores = cross_val_score(ml_algorithm, X, y, cv=outer_cv, scoring=make_scorer(recall_score))\n",
    "                elif(self.fitness_metric == \"precision\"):\n",
    "                    scores = cross_val_score(ml_algorithm, X, y, cv=outer_cv, scoring=make_scorer(precision_score))\n",
    "                elif(self.fitness_metric == \"auprc\"):\n",
    "                    scores = cross_val_score(ml_algorithm, X, y, cv=outer_cv, scoring=make_scorer(average_precision_score))\n",
    "                elif(self.fitness_metric == \"accuracy\"):\n",
    "                    scores = cross_val_score(ml_algorithm, X, y, cv=outer_cv, scoring=make_scorer(accuracy_score))                \n",
    "    \n",
    "                final_scores += list(scores)               \n",
    "            except Exception as e:\n",
    "                error_log = self.experiment_name + \"_error.log\"\n",
    "                with open(error_log, \"a\") as f:            \n",
    "                    fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                    f.write(\"Error on calculation scores - fitting\" + \"\\n\")\n",
    "                    f.write(pipeline_string + \"\\n\")\n",
    "                    f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                    fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file \n",
    "                final_scores += [0.0, 0.0, 0.0]\n",
    "                           \n",
    "        \n",
    "        # This function should evaluate the fitness of the individual within the time budget        \n",
    "        elapsed_time = time.time() - start_time    \n",
    "        fitness_value = np.array(final_scores).mean()\n",
    "        if elapsed_time > (time_budget_minutes_alg_eval * 60):  # Check if elapsed time exceeds time budget\n",
    "            error_log = self.experiment_name + \"_error.log\"\n",
    "            with open(error_log, \"a\") as f:            \n",
    "                fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file\n",
    "                f.write(\"Error on pipeline - exceeded time budget\" + \"\\n\")\n",
    "                f.write(pipeline_string + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\"  + \"\\n\")\n",
    "                fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file              \n",
    "            fitness_value = fitness_value * 0.7  # Set fitness value to zero if time budget exceeded\n",
    "       \n",
    "            \n",
    "        return fitness_value\n",
    "\n",
    "    \n",
    "        \n",
    "    def fitness(self):\n",
    "        \"\"\"\n",
    "        Calculates the fitness function in parallel using multiprocessing,\n",
    "        while caching results to avoid redundant evaluations.\n",
    "        \"\"\"\n",
    "        with multiprocessing.Pool(processes=self.num_cores) as pool:\n",
    "            results = []\n",
    "            async_results = []\n",
    "            \n",
    "            # Submit all tasks asynchronously, checking cache first\n",
    "            for pipeline in self.population:\n",
    "                pipeline_str =self.grammar.parse_tree_to_string(pipeline)  # Convert individual to a string representation\n",
    "                \n",
    "                if pipeline_str in self.fitness_cache:\n",
    "                    # Use cached value if available\n",
    "                    results.append((pipeline, self.fitness_cache[pipeline_str]))\n",
    "                else:\n",
    "                    # Otherwise, evaluate it asynchronously\n",
    "                    async_result = pool.apply_async(\n",
    "                        self.evaluate_fitness, \n",
    "                        (pipeline, self.training_dir, self.time_budget_minutes_alg_eval)\n",
    "                    )\n",
    "                    async_results.append((pipeline, async_result))\n",
    "    \n",
    "            # Collect results in a non-blocking way\n",
    "            for pipeline, async_result in async_results:\n",
    "                try:\n",
    "                    fitness_value = async_result.get(timeout=self.time_budget_minutes_alg_eval * 60)\n",
    "                except multiprocessing.TimeoutError:\n",
    "                    fitness_value = 0.0  # Timeout case\n",
    "                \n",
    "                # Cache the computed fitness value\n",
    "                pipeline_str =self.grammar.parse_tree_to_string(pipeline)\n",
    "               \n",
    "                self.fitness_cache[pipeline_str] = fitness_value  # Store in dictionary\n",
    "                results.append((pipeline, fitness_value))\n",
    "        \n",
    "        # Separate pipelines and fitness values\n",
    "        pipelines, fitness_results = zip(*results) if results else ([], [])\n",
    "    \n",
    "        return list(pipelines), list(fitness_results)\n",
    "\n",
    "\n",
    "    \n",
    "    def crossover(self, parent1, parent2):\n",
    "        \"\"\"\n",
    "        Performs crossover by swapping compatible components between parents.\n",
    "        \"\"\"\n",
    "        if isinstance(parent1, str) or isinstance(parent2, str):  # No crossover if terminal\n",
    "            return parent1, parent2\n",
    "    \n",
    "        root1, children1 = list(parent1.items())[0]\n",
    "        root2, children2 = list(parent2.items())[0]\n",
    "    \n",
    "        if root1 == \"<start>\" and root2 == \"<start>\":\n",
    "            # Swap one of the four main components of `<start>`\n",
    "            idx = random.choice([0, 2, 4, 6])  # Indices of the main components\n",
    "            children1[idx], children2[idx] = children2[idx], children1[idx]\n",
    "        elif root1 == root2:\n",
    "            # Swap subtrees for other non-terminals\n",
    "            idx1 = random.randint(0, len(children1) - 1)\n",
    "            idx2 = random.randint(0, len(children2) - 1)\n",
    "            children1[idx1], children2[idx2] = children2[idx2], children1[idx1]\n",
    "    \n",
    "        return parent1, parent2\n",
    "\n",
    "    \n",
    "    def mutate(self, individual, max_mutation_depth=4):\n",
    "        \"\"\"\n",
    "        Mutates an individual by replacing a specific component with a new valid subtree.\n",
    "        \"\"\"\n",
    "        if isinstance(individual, str):  # Terminal, no mutation possible\n",
    "            return individual\n",
    "    \n",
    "        root, children = list(individual.items())[0]\n",
    "    \n",
    "        if root == \"<start>\":\n",
    "            # Mutate one of the four main components of `<start>`\n",
    "            idx = random.choice([0, 2, 4, 6])  # Indices of the main components\n",
    "            components = [\"<feature_definition>\", \"<feature_scaling>\", \"<feature_selection>\", \"<ml_algorithms>\"]\n",
    "            replacement = self.grammar.generate_parse_tree(components[idx // 2], max_depth=max_mutation_depth)\n",
    "            children[idx] = replacement\n",
    "        else:\n",
    "            # Mutate other non-terminals\n",
    "            idx = random.randint(0, len(children) - 1)\n",
    "            children[idx] = self.grammar.generate_parse_tree(root, max_depth=max_mutation_depth)\n",
    "    \n",
    "        return individual\n",
    "    \n",
    "    \n",
    "    def evolve(self):\n",
    "        \"\"\"\n",
    "        Runs the genetic programming algorithm.\n",
    "        \"\"\"\n",
    "        # Initialize population\n",
    "        self.population = [self.grammar.generate_parse_tree() for _ in range(self.population_size)]\n",
    "        pop_indices = []  \n",
    "        \n",
    "        generation = 0\n",
    "        start = datetime.now()\n",
    "        end = start\n",
    "        time_diff_minutes = (end - start).total_seconds() / 60\n",
    "        condition = \"\"\n",
    "        if(self.stopping_criterion == \"generations\"):\n",
    "            condition = generation < self.max_generations\n",
    "        elif(self.stopping_criterion == \"time\"):\n",
    "            condition = time_diff_minutes < (self.max_time - 0.5)\n",
    "\n",
    "        #seed_index = 1\n",
    "        #random.seed(seed_index)\n",
    "        while condition:   \n",
    "            print(\"GENERATION: \" + str(generation))\n",
    "            if(self.stopping_criterion == \"generations\"):\n",
    "                condition = generation < self.max_generations\n",
    "            elif(self.stopping_criterion == \"time\"):\n",
    "                condition = time_diff_minutes < (self.max_time - 0.5)            \n",
    "            \n",
    "            #condition = generation < self.max_generations\n",
    "            # Evaluate fitness\n",
    "            pop_fitness_scores = self.fitness()\n",
    "            evaluated_population = pop_fitness_scores[0]\n",
    "            self.population = deepcopy(evaluated_population)\n",
    "            fitness_scores = pop_fitness_scores[1]\n",
    "\n",
    "            pop_indices = sorted(range(len(self.population)), key=lambda i: fitness_scores[i], reverse=True)\n",
    "            elites = [self.population[i] for i in pop_indices[:self.elitism_size]] \n",
    "            # Elitism: retain the best individuals\n",
    "            new_population = []\n",
    "            new_population.extend(elites)\n",
    "\n",
    "            #Adding new individuals if the population has >80% of the individuals are the same\n",
    "            ind_count_pop = {}            \n",
    "            for i in pop_indices:\n",
    "                ind = self.grammar.parse_tree_to_string(self.population[i])\n",
    "                print(ind + \"--->\" + str(fitness_scores[i]))\n",
    "                if(ind not in ind_count_pop):\n",
    "                    ind_count_pop[ind] = 1\n",
    "                else:\n",
    "                    count = ind_count_pop[ind] + 1\n",
    "                    ind_count_pop[ind] = count\n",
    "\n",
    "            max_count = -1\n",
    "            for ind in ind_count_pop:\n",
    "                ind_count = ind_count_pop[ind]\n",
    "                if  ind_count > max_count:\n",
    "                    max_count = ind_count\n",
    "\n",
    "            population_stabilisation_rate = float(max_count)/float(self.population_size)\n",
    "            \n",
    "            if(population_stabilisation_rate > 0.7):\n",
    "                new_ind1 = self.grammar.generate_parse_tree()\n",
    "                new_ind2 = self.grammar.generate_parse_tree()\n",
    "                new_population.append(new_ind1)\n",
    "                new_population.append(new_ind2)\n",
    "\n",
    "            # Selection probabilities\n",
    "            fitness_values = [1.0 / (f + 1e-6) for f in fitness_scores]\n",
    "            total_fitness = sum(fitness_values)\n",
    "            probabilities = [f / total_fitness for f in fitness_values]\n",
    "\n",
    "            while len(new_population) < self.population_size:\n",
    "                \n",
    "                idx1 = random.randint(0, len(self.population) - 1)\n",
    "                idx2 = random.randint(0, len(self.population) - 1)\n",
    "                while idx1 == idx2:\n",
    "                    idx2 = random.randint(0, len(self.population) - 1)\n",
    "                   \n",
    "                parent1 = self.population[idx1]\n",
    "                parent2 = self.population[idx2]\n",
    "\n",
    "                random_num = random.random()\n",
    "                if (random_num < self.crossover_mutation_rate):                    \n",
    "                    #perform crossover                    \n",
    "                    child1, child2 = self.crossover(deepcopy(parent1), deepcopy(parent2))                    \n",
    "                    # and mutation                    \n",
    "                    child1_1 = self.mutate(deepcopy(child1))\n",
    "                    child2_1 = self.mutate(deepcopy(child2))\n",
    "                    new_population.extend([child1_1, child2_1]) \n",
    "                elif (random_num < (self.crossover_mutation_rate + self.mutation_rate)):\n",
    "                    #only mutation\n",
    "                    child = self.mutate(deepcopy(parent1))\n",
    "                    new_population.append(child)\n",
    "                elif (random_num < (self.crossover_mutation_rate + self.mutation_rate + self.crossover_rate)):\n",
    "                    #only crossover\n",
    "                    child1, child2 = self.crossover(deepcopy(parent1), deepcopy(parent2))\n",
    "                    new_population.extend([child1, child2])     \n",
    "                else:\n",
    "                    #no operation\n",
    "                    new_population.extend([deepcopy(parent1), deepcopy(parent2)])\n",
    "                    \n",
    "            # Trim excess individuals\n",
    "            self.population = new_population[:self.population_size]\n",
    "            end =  datetime.now()\n",
    "            time_diff_minutes = (end - start).total_seconds() / 60\n",
    "            generation += 1\n",
    "            print(\"-----------------------------------------------\")            \n",
    "\n",
    "        best_indices = sorted(range(len(self.population)), key=lambda i: fitness_scores[i], reverse=True)[:1]\n",
    "        best_fitness = [fitness_scores[i] for i in pop_indices][0]  \n",
    "        best_individual = self.population[best_indices[0]]\n",
    "        mcc, auc, rec, apr, prec, acc = self.evaluate_train_test(best_individual)\n",
    "        \n",
    "        final_file_name = self.experiment_name + \".txt\"\n",
    "        final_result = \"\"\n",
    "        with open(final_file_name, \"a\") as file:\n",
    "            final_result += self.experiment_name + \";\"\n",
    "            final_result += str(self.seed) + \";\"\n",
    "            final_result += str(generation) + \";\"\n",
    "            final_result += str(round(time_diff_minutes, 4)) + \";\"\n",
    "            final_result += str(mcc) + \";\"\n",
    "            final_result += str(auc) + \";\"\n",
    "            final_result += str(rec) + \";\"\n",
    "            final_result += str(apr) + \";\"\n",
    "            final_result += str(prec) + \";\"\n",
    "            final_result += str(acc) + \";\" + \";\"\n",
    "            final_result += self.grammar.parse_tree_to_string(best_individual) + \"\\n\"\n",
    "            print(final_result)\n",
    "            file.write(final_result)\n",
    "            file.close()\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":    \n",
    "    \n",
    "    random.seed(15)  # For reproducibility\n",
    "\n",
    "    # Define grammar\n",
    "    with open(\"/home/alexgcsa/auto-admet/autoadmet.bnf\", \"r\") as file:\n",
    "        grammar_text = file.read()\n",
    "\n",
    "    training_dir = \"/home/alexgcsa/auto-admet/datasets/01_caco2_train.csv\"\n",
    "    testing_dir = \"/home/alexgcsa/auto-admet/datasets/01_caco2_blindtest.csv\"\n",
    "    # Load grammar\n",
    "    grammar = BNFGrammar()\n",
    "    grammar.load_grammar(grammar_text)\n",
    "\n",
    "    # Run GGP\n",
    "    ggp = GrammarBasedGP(grammar, training_dir, testing_dir, fitness_metric=\"mcc\")\n",
    "    best_program = ggp.evolve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2932db1-9dbb-4d11-a0d2-1e96efb9e062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf6126-fcee-4270-885d-4b8adb4a227f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
