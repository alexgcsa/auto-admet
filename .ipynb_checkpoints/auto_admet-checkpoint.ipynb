{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d2cfc3c-a8f6-446a-aa89-b7073651d567",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3249551180.py, line 414)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 414\u001b[0;36m\u001b[0m\n\u001b[0;31m    ml_algorithm  = self.\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SelectFpr, SelectFwe, SelectFdr, chi2, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class BNFGrammar:\n",
    "    def __init__(self):\n",
    "        self.grammar = defaultdict(list)\n",
    "        self.non_terminals = set()\n",
    "        self.terminals = set()\n",
    "\n",
    "    def load_grammar(self, bnf_text: str):\n",
    "        \"\"\"\n",
    "        Parses the BNF grammar from a string.\n",
    "        \"\"\"\n",
    "        for line in bnf_text.strip().splitlines():\n",
    "            if \"::=\" in line:\n",
    "                lhs, rhs = line.split(\"::=\", 1)\n",
    "                lhs = lhs.strip()\n",
    "                self.non_terminals.add(lhs)\n",
    "                rhs_options = [option.strip() for option in rhs.split(\"|\")]\n",
    "                for option in rhs_options:\n",
    "                    self.grammar[lhs].append(option.split())\n",
    "                    for token in option.split():\n",
    "                        if token not in self.non_terminals:\n",
    "                            self.terminals.add(token)\n",
    "\n",
    "    def generate_parse_tree(self, symbol: str = \"<start>\", max_depth: int = 10) -> dict:\n",
    "        \"\"\"\n",
    "        Generates a parse tree starting from the given symbol, ensuring mandatory grammar components are included.\n",
    "        \"\"\"\n",
    "        if max_depth <= 0 or symbol not in self.grammar:\n",
    "            return symbol  # Return the symbol as a terminal if max depth is reached\n",
    "    \n",
    "        # Strictly enforce the `<start>` rule\n",
    "        if symbol == \"<start>\":\n",
    "            # Generate each mandatory component\n",
    "            feature_def = self.generate_parse_tree(\"<feature_definition>\", max_depth - 1)\n",
    "            scaling = self.generate_parse_tree(\"<feature_scaling>\", max_depth - 1)\n",
    "            selection = self.generate_parse_tree(\"<feature_selection>\", max_depth - 1)\n",
    "            ml_algo = self.generate_parse_tree(\"<ml_algorithms>\", max_depth - 1)\n",
    "    \n",
    "            return {symbol: [feature_def, \"#\", scaling, \"#\", selection, \"#\", ml_algo]}\n",
    "    \n",
    "        # Select a random production for other non-terminals\n",
    "        production = random.choice(self.grammar[symbol])\n",
    "        return {symbol: [self.generate_parse_tree(token, max_depth - 1) for token in production]}\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    def parse_tree_to_string(self, tree) -> str:\n",
    "        \"\"\"\n",
    "        Reconstructs a string from the parse tree.\n",
    "        \"\"\"\n",
    "        if isinstance(tree, str):\n",
    "            # Leaf node (terminal)\n",
    "            return tree\n",
    "        # Non-terminal with its production rules as children\n",
    "        root, children = list(tree.items())[0]\n",
    "        return \" \".join(self.parse_tree_to_string(child) for child in children)\n",
    "\n",
    "    def validate_parse_tree(self, tree, symbol=\"<start>\") -> bool:\n",
    "        \"\"\"\n",
    "        Validates if the parse tree conforms to the grammar and respects the `<start>` structure.\n",
    "        \"\"\"\n",
    "        if isinstance(tree, str):\n",
    "            return tree in self.terminals  # Check terminal validity\n",
    "    \n",
    "        if not isinstance(tree, dict) or len(tree) != 1:\n",
    "            return False\n",
    "    \n",
    "        root, children = list(tree.items())[0]\n",
    "        if root != symbol:\n",
    "            return False\n",
    "    \n",
    "        if symbol == \"<start>\":\n",
    "            # Check `<start>` structure\n",
    "            if len(children) != 7:\n",
    "                return False\n",
    "            expected_symbols = [\"<feature_definition>\", \"#\", \"<feature_scaling>\", \"#\", \"<feature_selection>\", \"#\", \"<ml_algorithms>\"]\n",
    "            for i, child_symbol in enumerate(expected_symbols):\n",
    "                if i % 2 == 0 and not self.validate_parse_tree(children[i], child_symbol):  # Validate non-terminals\n",
    "                    return False\n",
    "                if i % 2 == 1 and children[i] != \"#\":  # Ensure separator\n",
    "                    return False\n",
    "    \n",
    "        # Validate other non-terminals\n",
    "        for production in self.grammar[symbol]:\n",
    "            if len(production) == len(children) and all(\n",
    "                self.validate_parse_tree(child, production[i])\n",
    "                for i, child in enumerate(children)\n",
    "            ):\n",
    "                return True\n",
    "    \n",
    "        return False\n",
    "        \n",
    "\n",
    "class MLAlgorithmTransformer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def XGBEvaluation(dataset, n_estimators_str, max_depth_str, max_leaves_str, learning_rate_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "    \n",
    "            \n",
    "        try:\n",
    "            clf = XGBClassifier(n_estimators=int(n_estimators_str), max_depth=max_depth_actual, random_state=42, \n",
    "                                max_leaves=int(max_leaves_str), learning_rate=float(learning_rate_str), n_jobs=1)        \n",
    "    \n",
    "    \n",
    "            y =dataset.iloc[:,-1:]\n",
    "            X = dataset[dataset.columns[:-1]]\n",
    "            \n",
    "            scores = cross_val_score(clf, X, y, cv=5, scoring=make_scorer(matthews_corrcoef))\n",
    "            mean_scores = scores.mean()\n",
    "            return mean_scores   \n",
    "            \n",
    "        except:\n",
    "            return 0.0  \n",
    "    \n",
    "    def GradientBoosting(n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, loss_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, random_state=42, \n",
    "                                         min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str), \n",
    "                                         max_features=max_features_actual, loss=loss_str)        \n",
    "    \n",
    "        return clf      \n",
    " \n",
    "    \n",
    "    def ExtraTrees(n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "     \n",
    "            \n",
    "        clf = ExtraTreesClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, n_jobs=1, random_state=42, \n",
    "                                   class_weight=class_weight_actual,  min_samples_split=int(min_samples_split_str), \n",
    "                                   min_samples_leaf=int(min_samples_split_str), max_features=max_features_actual)        \n",
    "    \n",
    "        return clf  \n",
    "    \n",
    "    \n",
    "    def RandomForest(n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "     \n",
    "        clf = RandomForestClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, n_jobs=1, random_state=42, \n",
    "                                     class_weight=class_weight_actual,  min_samples_split=int(min_samples_split_str), \n",
    "                                     min_samples_leaf=int(min_samples_split_str), max_features=max_features_actual)        \n",
    " \n",
    "        return clf        \n",
    "    \n",
    "    def ExtraTree(criterion_str, splitter_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "            \n",
    "        clf = ExtraTreeClassifier(criterion=criterion_str, splitter='best', max_depth=max_depth_actual, \n",
    "                                  min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str),                                      \n",
    "                                  max_features=max_features_actual, random_state=0)      \n",
    "    \n",
    "        return clf  \n",
    "        \n",
    "    \n",
    "    \n",
    "    def DecisionTree(criterion_str, splitter_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "\n",
    "\n",
    "        clf = DecisionTreeClassifier(criterion=criterion_str, splitter=splitter_str, max_depth=max_depth_actual, \n",
    "                                     min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str), \n",
    "                                     max_features=max_features_actual, random_state=0,\n",
    "                                     class_weight=class_weight_actual)      \n",
    "    \n",
    "        return clf\n",
    "    \n",
    "    \n",
    "    def AdaBoost(alg, n_est, lr):\n",
    "        clf = AdaBoostClassifier(n_estimators=n_est, learning_rate=lr, algorithm=alg, random_state=0)\n",
    "        return clf\n",
    "\n",
    "\n",
    "\n",
    "class FeatureSelectionTransformer:\n",
    "    def __init__(self, df, label_col):\n",
    "        self.df = df\n",
    "        self.label_col = label_col\n",
    "\n",
    "    def select_fwe(self, alpha_str, score_function_str):\n",
    "    \n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            model = SelectFwe(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.df, self.label_col)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return self.df    \n",
    "    \n",
    "    def select_fdr(self, alpha_str, score_function_str):\n",
    "    \n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            model = SelectFdr(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.df, self.label_col)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return self.df    \n",
    "    \n",
    "    def select_fpr(self, alpha_str, score_function_str):\n",
    "    \n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            model = SelectFpr(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.df, self.label_col)\n",
    "            df_np = model.transform(self.df)\n",
    "        \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return self.df    \n",
    "    \n",
    "    def select_percentile(self, percentile_str, score_function_str):\n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            model = SelectPercentile(score_func=score_function_actual, percentile = int(percentile_str)).fit(self.df, self.label_col)\n",
    "            df_np = model.transform(self.df)\n",
    "        \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return self.df\n",
    "    \n",
    "    def variance_threshold(self,thrsh):\n",
    "        try:\n",
    "            model =VarianceThreshold(threshold=thrsh).fit(self.df, self.label_col)\n",
    "            df_np = model.transform(self.df)\n",
    "        \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return self.df\n",
    "\n",
    "class ScalingTransformer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def normalizer(self, norm_hp):\n",
    "        try:\n",
    "            model = Normalizer(norm=norm_hp).fit(self.df)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.df.columns)\n",
    "        except:\n",
    "            return self.df \n",
    "    \n",
    "    def max_abs_scaler(self):\n",
    "        try:\n",
    "            model = MaxAbsScaler().fit(self.df)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.df.columns)\n",
    "        except:\n",
    "            return self.df  \n",
    "    \n",
    "    def min_max_scaler(self):\n",
    "        try:\n",
    "            model = MinMaxScaler().fit(self.df)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.df.columns)\n",
    "        except:\n",
    "            return self.df  \n",
    "    \n",
    "    def standard_scaler(self, with_mean_str, with_std_str):\n",
    "        with_mean_actual = True\n",
    "        with_std_actual = True\n",
    "    \n",
    "        if with_mean_str == \"False\":\n",
    "            with_mean_actual = False\n",
    "        if with_std_str == \"False\":\n",
    "            with_std_actual = False        \n",
    "        try:\n",
    "            model = StandardScaler(with_mean=with_mean_actual, with_std=with_std_actual).fit(self.df)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.df.columns)\n",
    "        except:\n",
    "            return self.df \n",
    "    \n",
    "    def robust_scaler(self, with_centering_str, with_scaling_str):\n",
    "        with_centering_actual = True\n",
    "        with_scaling_actual = True\n",
    "    \n",
    "        if with_centering_str == \"False\":\n",
    "            with_centering_actual = False\n",
    "        if with_scaling_str == \"False\":\n",
    "            with_scaling_actual = False        \n",
    "        try:\n",
    "            model = RobustScaler(with_centering=with_centering_actual, with_scaling=with_scaling_actual).fit(self.df)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.df.columns)\n",
    "        except:\n",
    "            return self.df\n",
    "\n",
    "\n",
    "class GrammarBasedGP:\n",
    "    def __init__(self, grammar, training_dir, testing_dir, num_cores=20, time_budget_minutes_alg_eval = 3, population_size=20, max_generations=10, mutation_rate=0.1, crossover_rate=0.7, crossover_mutation_rate=0.05, elitism_size=2):\n",
    "        self.grammar = grammar\n",
    "        self.training_dir = training_dir\n",
    "        self.testing_dir = testing_dir\n",
    "        self.num_cores = num_cores\n",
    "        self.time_budget_minutes_alg_eval = time_budget_minutes_alg_eval\n",
    "        self.population_size = population_size\n",
    "        self.max_generations = max_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.crossover_mutation_rate = crossover_mutation_rate\n",
    "        self.elitism_size = elitism_size        \n",
    "        self.population = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def select_ml_algorithms(self, ml_algorithm):\n",
    "        ml_alg_selection = MLAlgorithmTransformer()\n",
    "        if ml_algorithm[0] == \"AdaBoostClassifier\":\n",
    "            return ml_alg_selection.AdaBoost(str(ml_algorithm[1]), int(ml_algorithm[2]), float(ml_algorithm[3]))\n",
    "        elif ml_algorithm[0] == \"DecisionTreeClassifier\":\n",
    "            return ml_alg_selection.DecisionTree(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])    \n",
    "        elif ml_algorithm[0] == \"ExtraTreeClassifier\":\n",
    "            return ml_alg_selection.ExtraTree(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"RandomForestClassifier\":\n",
    "            return ml_alg_selection.RandomForest(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"ExtraTreesClassifier\":\n",
    "            return ml_alg_selection.ExtraTrees(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"GradientBoostingClassifier\":\n",
    "            return ml_alg_selection.GradientBoosting(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7]) \n",
    "        elif ml_algorithm[0] == \"XGBClassifier\":\n",
    "            return ml_alg_selection.XGBoost(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4])         \n",
    "                            \n",
    "        else:\n",
    "            return None    \n",
    "\n",
    "    \n",
    "    def select_features(self, feature_selection, dataset_df, label_col):\n",
    "\n",
    "        cp_dataset_df = dataset_df.copy(deep=True)\n",
    "        feature_selection_transformer = FeatureSelectionTransformer(cp_dataset_df, label_col)\n",
    "    \n",
    "        if feature_selection[0] == \"NoFeatureSelection\":\n",
    "            return dataset_df\n",
    "        elif feature_selection[0] == \"VarianceThreshold\":\n",
    "            mod_dataset_df = feature_selection_transformer.variance_threshold(float(feature_selection[1]))\n",
    "            return mod_dataset_df\n",
    "        elif feature_selection[0] == \"SelectPercentile\":        \n",
    "            mod_dataset_df = feature_selection_transformer.select_percentile(feature_selection[1], feature_selection[2])    \n",
    "            return mod_dataset_df     \n",
    "        elif feature_selection[0] == \"SelectFpr\":        \n",
    "            mod_dataset_df = feature_selection_transformer.select_fpr(feature_selection[1], feature_selection[2])    \n",
    "            return mod_dataset_df\n",
    "        elif feature_selection[0] == \"SelectFdr\":        \n",
    "            mod_dataset_df = feature_selection_transformer.select_fdr(feature_selection[1], feature_selection[2])    \n",
    "            return mod_dataset_df\n",
    "        elif feature_selection[0] == \"SelectFwe\":        \n",
    "            mod_dataset_df = feature_selection_transformer.select_fwe(feature_selection[1], feature_selection[2])    \n",
    "            return mod_dataset_df          \n",
    "        else:\n",
    "            return cp_dataset_df        \n",
    "\n",
    "    def scale_features(self, feature_scaling, dataset_df):\n",
    "\n",
    "        cp_dataset_df = dataset_df.copy(deep=True)\n",
    "        scaling_transformer = ScalingTransformer(cp_dataset_df)\n",
    "        if feature_scaling[0] == \"NoScaling\":\n",
    "            return dataset_df\n",
    "        elif feature_scaling[0] == \"Normalizer\":\n",
    "            mod_dataset_df = scaling_transformer.normalizer(str(feature_scaling[1]))\n",
    "            return mod_dataset_df\n",
    "        elif feature_scaling[0] == \"MinMaxScaler\":\n",
    "            mod_dataset_df = scaling_transformer.min_max_scaler()\n",
    "            return mod_dataset_df\n",
    "        elif feature_scaling[0] == \"MaxAbsScaler\":\n",
    "            mod_dataset_df = scaling_transformer.max_abs_scaler()\n",
    "            return mod_dataset_df    \n",
    "        elif feature_scaling[0] == \"StandardScaler\":\n",
    "            mod_dataset_df = scaling_transformer.standard_scaler(feature_scaling[1], feature_scaling[2])\n",
    "            return mod_dataset_df\n",
    "        elif feature_scaling[0] == \"RobustScaler\":\n",
    "            mod_dataset_df = scaling_transformer.robust_scaler(feature_scaling[1], feature_scaling[2])\n",
    "            return mod_dataset_df           \n",
    "        else:\n",
    "            return cp_dataset_df    \n",
    "\n",
    "\n",
    "    def represent_molecules(self, list_of_feature_types, dataset_df):\n",
    "        \"\"\"\n",
    "        represents a chemical dataset with descriptors.\n",
    "        \"\"\"          \n",
    "    \n",
    "        columns = []\n",
    "        for lft in list_of_feature_types:\n",
    "            if lft == \"General_Descriptors\":\n",
    "                columns += [\"HeavyAtomCount\",\"MolLogP\",\"NumHeteroatoms\",\"NumRotatableBonds\",\"RingCount\",\"TPSA\",\"LabuteASA\",\"MolWt\",\"FCount\",\"FCount2\",\"Acceptor_Count\",\"Aromatic_Count\",\"Donor_Count\",\"Hydrophobe_Count\",\"NegIonizable_Count\",\"PosIonizable_Count\",]\n",
    "            elif lft == \"Advanced_Descriptors\":\n",
    "                columns += [\"BalabanJ\",\"BertzCT\",\"Chi0\",\"Chi0n\",\"Chi0v\",\"Chi1\",\"Chi1n\",\"Chi1v\",\"Chi2n\",\"Chi2v\",\"Chi3n\",\"Chi3v\",\"Chi4n\",\"Chi4v\",\"HallKierAlpha\",\"Kappa1\",\"Kappa2\",\"Kappa3\",\"NHOHCount\",\"NOCount\",\"PEOE_VSA1\",\"PEOE_VSA10\",\"PEOE_VSA11\",\"PEOE_VSA12\",\"PEOE_VSA13\",\"PEOE_VSA14\",\"PEOE_VSA2\",\"PEOE_VSA3\",\"PEOE_VSA4\",\"PEOE_VSA5\",\"PEOE_VSA6\",\"PEOE_VSA7\",\"PEOE_VSA8\",\"PEOE_VSA9\",\"SMR_VSA1\",\"SMR_VSA10\",\"SMR_VSA2\",\"SMR_VSA3\",\"SMR_VSA4\",\"SMR_VSA5\",\"SMR_VSA6\",\"SMR_VSA7\",\"SMR_VSA8\",\"SMR_VSA9\",\"SlogP_VSA1\",\"SlogP_VSA10\",\"SlogP_VSA11\",\"SlogP_VSA12\",\"SlogP_VSA2\",\"SlogP_VSA3\",\"SlogP_VSA4\",\"SlogP_VSA5\",\"SlogP_VSA6\",\"SlogP_VSA7\",\"SlogP_VSA8\",\"SlogP_VSA9\",\"VSA_EState1\",\"VSA_EState10\",\"VSA_EState2\",\"VSA_EState3\",\"VSA_EState4\",\"VSA_EState5\",\"VSA_EState6\",\"VSA_EState7\",\"VSA_EState8\",\"VSA_EState9\"]\n",
    "            elif lft == \"Toxicophores\":\n",
    "                columns += [\"Tox_1\",\"Tox_2\",\"Tox_3\",\"Tox_4\",\"Tox_5\",\"Tox_6\",\"Tox_7\",\"Tox_8\",\"Tox_9\",\"Tox_10\",\"Tox_11\",\"Tox_12\",\"Tox_13\",\"Tox_14\",\"Tox_15\",\"Tox_16\",\"Tox_17\",\"Tox_18\",\"Tox_19\",\"Tox_20\",\"Tox_21\",\"Tox_22\",\"Tox_23\",\"Tox_24\",\"Tox_25\",\"Tox_26\",\"Tox_27\",\"Tox_28\",\"Tox_29\",\"Tox_30\",\"Tox_31\",\"Tox_32\",\"Tox_33\",\"Tox_34\",\"Tox_35\",\"Tox_36\"]\n",
    "            elif lft == \"Fragments\":\n",
    "                columns += [\"fr_Al_COO\",\"fr_Al_OH\",\"fr_Al_OH_noTert\",\"fr_ArN\",\"fr_Ar_COO\",\"fr_Ar_N\",\"fr_Ar_NH\",\"fr_Ar_OH\",\"fr_COO\",\"fr_COO2\",\"fr_C_O\",\"fr_C_O_noCOO\",\"fr_C_S\",\"fr_HOCCN\",\"fr_Imine\",\"fr_NH0\",\"fr_NH1\",\"fr_NH2\",\"fr_N_O\",\"fr_Ndealkylation1\",\"fr_Ndealkylation2\",\"fr_Nhpyrrole\",\"fr_SH\",\"fr_aldehyde\",\"fr_alkyl_carbamate\",\"fr_alkyl_halide\",\"fr_allylic_oxid\",\"fr_amide\",\"fr_amidine\",\"fr_aniline\",\"fr_aryl_methyl\",\"fr_azide\",\"fr_azo\",\"fr_barbitur\",\"fr_benzene\",\"fr_benzodiazepine\",\"fr_bicyclic\",\"fr_diazo\",\"fr_dihydropyridine\",\"fr_epoxide\",\"fr_ester\",\"fr_ether\",\"fr_furan\",\"fr_guanido\",\"fr_halogen\",\"fr_hdrzine\",\"fr_hdrzone\",\"fr_imidazole\",\"fr_imide\",\"fr_isocyan\",\"fr_isothiocyan\",\"fr_ketone\",\"fr_ketone_Topliss\",\"fr_lactam\",\"fr_lactone\",\"fr_methoxy\",\"fr_morpholine\",\"fr_nitrile\",\"fr_nitro\",\"fr_nitro_arom\",\"fr_nitro_arom_nonortho\",\"fr_nitroso\",\"fr_oxazole\",\"fr_oxime\",\"fr_para_hydroxylation\",\"fr_phenol\",\"fr_phenol_noOrthoHbond\",\"fr_phos_acid\",\"fr_phos_ester\",\"fr_piperdine\",\"fr_piperzine\",\"fr_priamide\",\"fr_prisulfonamd\",\"fr_pyridine\",\"fr_quatN\",\"fr_sulfide\",\"fr_sulfonamd\",\"fr_sulfone\",\"fr_term_acetylene\",\"fr_tetrazole\",\"fr_thiazole\",\"fr_thiocyan\",\"fr_thiophene\",\"fr_unbrch_alkane\",\"fr_urea\"]\n",
    "            elif lft == \"Graph_based_Signatures\":\n",
    "                columns += [\"Acceptor:Acceptor-6.00\",\"Acceptor:Aromatic-6.00\",\"Acceptor:Donor-6.00\",\"Acceptor:Hydrophobe-6.00\",\"Acceptor:NegIonizable-6.00\",\"Acceptor:PosIonizable-6.00\",\"Aromatic:Aromatic-6.00\",\"Aromatic:Donor-6.00\",\"Aromatic:Hydrophobe-6.00\",\"Aromatic:NegIonizable-6.00\",\"Aromatic:PosIonizable-6.00\",\"Donor:Donor-6.00\",\"Donor:Hydrophobe-6.00\",\"Donor:NegIonizable-6.00\",\"Donor:PosIonizable-6.00\",\"Hydrophobe:Hydrophobe-6.00\",\"Hydrophobe:NegIonizable-6.00\",\"Hydrophobe:PosIonizable-6.00\",\"NegIonizable:NegIonizable-6.00\",\"NegIonizable:PosIonizable-6.00\",\"PosIonizable:PosIonizable-6.00\",\"Acceptor:Acceptor-4.00\",\"Acceptor:Aromatic-4.00\",\"Acceptor:Donor-4.00\",\"Acceptor:Hydrophobe-4.00\",\"Acceptor:NegIonizable-4.00\",\"Acceptor:PosIonizable-4.00\",\"Aromatic:Aromatic-4.00\",\"Aromatic:Donor-4.00\",\"Aromatic:Hydrophobe-4.00\",\"Aromatic:NegIonizable-4.00\",\"Aromatic:PosIonizable-4.00\",\"Donor:Donor-4.00\",\"Donor:Hydrophobe-4.00\",\"Donor:NegIonizable-4.00\",\"Donor:PosIonizable-4.00\",\"Hydrophobe:Hydrophobe-4.00\",\"Hydrophobe:NegIonizable-4.00\",\"Hydrophobe:PosIonizable-4.00\",\"NegIonizable:NegIonizable-4.00\",\"NegIonizable:PosIonizable-4.00\",\"PosIonizable:PosIonizable-4.00\",\"Acceptor:Acceptor-2.00\",\"Acceptor:Aromatic-2.00\",\"Acceptor:Donor-2.00\",\"Acceptor:Hydrophobe-2.00\",\"Acceptor:NegIonizable-2.00\",\"Acceptor:PosIonizable-2.00\",\"Aromatic:Aromatic-2.00\",\"Aromatic:Donor-2.00\",\"Aromatic:Hydrophobe-2.00\",\"Aromatic:NegIonizable-2.00\",\"Aromatic:PosIonizable-2.00\",\"Donor:Donor-2.00\",\"Donor:Hydrophobe-2.00\",\"Donor:NegIonizable-2.00\",\"Donor:PosIonizable-2.00\",\"Hydrophobe:Hydrophobe-2.00\",\"Hydrophobe:NegIonizable-2.00\",\"Hydrophobe:PosIonizable-2.00\",\"NegIonizable:NegIonizable-2.00\",\"NegIonizable:PosIonizable-2.00\",\"PosIonizable:PosIonizable-2.00\"]\n",
    "            \n",
    "        mod_dataset_df = None\n",
    "        try:\n",
    "            cp_dataset_df = dataset_df.copy(deep=True)\n",
    "            mod_dataset_df = cp_dataset_df[columns]\n",
    "        except:\n",
    "            print(\"Error representation. \")\n",
    "    \n",
    "    \n",
    "        return mod_dataset_df    \n",
    "\n",
    "    def evaluate_fitness(self, pipeline, dataset_path, time_budget_minutes_alg_eval):\n",
    "        \"\"\"\n",
    "        evaluates pipeline with the fitness, performing each step of the ML pipeline.\n",
    "        \"\"\"  \n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        #all the steps in Auto-ADMET pipeline:\n",
    "        pipeline_string = self.grammar.parse_tree_to_string(pipeline)\n",
    "        pipeline_list = pipeline_string.split(\" # \")\n",
    "        representation = pipeline_list[0].split(\" \")\n",
    "        feature_scaling = pipeline_list[1].split(\" \")\n",
    "        feature_selection = pipeline_list[2].split(\" \")\n",
    "        ml_algorithm = pipeline_list[3].split(\" \")\n",
    "\n",
    "\n",
    "        #applying the steps to an actual dataset:\n",
    "\n",
    "        dataset_df = pd.read_csv(self.training_dir, header=0, sep=\",\")\n",
    "        #dataset_df = dataset_df.sample(random_state=generation, frac = 1.0)\n",
    "\n",
    "        label_col = dataset_df[\"CLASS\"]\n",
    "        dataset_df = dataset_df.drop(\"CLASS\", axis=1)\n",
    "\n",
    "        rep_dataset_df = self.represent_molecules(representation, dataset_df) \n",
    "        prep_dataset_df = self.scale_features(feature_scaling, rep_dataset_df)\n",
    "        sel_dataset_df = self.select_features(feature_selection, rep_dataset_df, label_col)\n",
    "        ml_algorithm  = self.select_ml_algorithms(ml_algorithm)\n",
    "\n",
    "        \n",
    "        \n",
    "        print(rep_dataset_df.shape)\n",
    "        print(sel_dataset_df.shape)\n",
    "        \n",
    "        #sel_dataset_df = select(feature_selection, prep_dataset_df, label_col)\n",
    "        #sel_dataset_df[\"CLASS\"] = pd.Series(label_col)\n",
    "        #score = evaluate_pipeline(ml_algorithm, sel_dataset_df)        \n",
    "\n",
    "        print(pipeline_string)\n",
    "        #print(pipeline_list) \n",
    "        #print(representation)\n",
    "        #print(dataset_df.columns.values)\n",
    "        #print(feature_scaling)\n",
    "        #print(feature_selection)\n",
    "        #print(ml_algorithm)\n",
    "        print()\n",
    "          \n",
    "        \n",
    "        #score = fitness_function(pipeline, dataset_path)\n",
    "        # This function should evaluate the fitness of the individual within the time budget\n",
    "        fitness_value = random.random()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time    \n",
    "        if elapsed_time > (time_budget_minutes_alg_eval * 60):  # Check if elapsed time exceeds time budget\n",
    "            fitness_value = 0.0  # Set fitness value to zero if time budget exceeded\n",
    "            \n",
    "            \n",
    "        return fitness_value\n",
    "        \n",
    "    def fitness(self):\n",
    "        \"\"\"\n",
    "        Calculates the fitness function in parallel, using multiprocessing.\n",
    "        \"\"\"        \n",
    "        with multiprocessing.Pool(processes=self.num_cores) as pool:\n",
    "            results = []\n",
    "            i=0\n",
    "            for pipeline in self.population:\n",
    "                result = pool.apply_async(self.evaluate_fitness, (pipeline, self.training_dir, self.time_budget_minutes_alg_eval))\n",
    "                try:\n",
    "                    fitness_value = result.get(timeout=self.time_budget_minutes_alg_eval * 60)\n",
    "                    results.append((pipeline, fitness_value))\n",
    "                except multiprocessing.TimeoutError:\n",
    "                    results.append((pipeline, 0.0))  # Set fitness value to zero and elapsed time to the time budget\n",
    "    \n",
    "        fitness_results = []\n",
    "        pipelines = []\n",
    "        for result in results:\n",
    "            individual, fitness_value = result[0], result[1]\n",
    "            pipelines.append(individual)\n",
    "            fitness_results.append(fitness_value)\n",
    "\n",
    "        \n",
    "        return pipelines, fitness_results\n",
    "    \n",
    "    def crossover(self, parent1, parent2):\n",
    "        \"\"\"\n",
    "        Performs crossover by swapping compatible components between parents.\n",
    "        \"\"\"\n",
    "        if isinstance(parent1, str) or isinstance(parent2, str):  # No crossover if terminal\n",
    "            return parent1, parent2\n",
    "    \n",
    "        root1, children1 = list(parent1.items())[0]\n",
    "        root2, children2 = list(parent2.items())[0]\n",
    "    \n",
    "        if root1 == \"<start>\" and root2 == \"<start>\":\n",
    "            # Swap one of the four main components of `<start>`\n",
    "            idx = random.choice([0, 2, 4, 6])  # Indices of the main components\n",
    "            children1[idx], children2[idx] = children2[idx], children1[idx]\n",
    "        elif root1 == root2:\n",
    "            # Swap subtrees for other non-terminals\n",
    "            idx1 = random.randint(0, len(children1) - 1)\n",
    "            idx2 = random.randint(0, len(children2) - 1)\n",
    "            children1[idx1], children2[idx2] = children2[idx2], children1[idx1]\n",
    "    \n",
    "        return parent1, parent2\n",
    "\n",
    "    def mutate(self, individual, max_mutation_depth=4):\n",
    "        \"\"\"\n",
    "        Mutates an individual by replacing a specific component with a new valid subtree.\n",
    "        \"\"\"\n",
    "        if isinstance(individual, str):  # Terminal, no mutation possible\n",
    "            return individual\n",
    "    \n",
    "        root, children = list(individual.items())[0]\n",
    "    \n",
    "        if root == \"<start>\":\n",
    "            # Mutate one of the four main components of `<start>`\n",
    "            idx = random.choice([0, 2, 4, 6])  # Indices of the main components\n",
    "            components = [\"<feature_definition>\", \"<feature_scaling>\", \"<feature_selection>\", \"<ml_algorithms>\"]\n",
    "            replacement = self.grammar.generate_parse_tree(components[idx // 2], max_depth=max_mutation_depth)\n",
    "            children[idx] = replacement\n",
    "        else:\n",
    "            # Mutate other non-terminals\n",
    "            idx = random.randint(0, len(children) - 1)\n",
    "            children[idx] = self.grammar.generate_parse_tree(root, max_depth=max_mutation_depth)\n",
    "    \n",
    "        return individual\n",
    "    \n",
    "    \n",
    "    def evolve(self):\n",
    "        \"\"\"\n",
    "        Runs the genetic programming algorithm.\n",
    "        \"\"\"\n",
    "        # Initialize population\n",
    "        self.population = [self.grammar.generate_parse_tree() for _ in range(self.population_size)]\n",
    "        #for s in self.population:\n",
    "            #print(grammar.parse_tree_to_string(s))\n",
    "            \n",
    "        print(\"-----------------------------------------------\")\n",
    "        for generation in range(self.max_generations):\n",
    "            # Evaluate fitness\n",
    "            pop_fitness_scores = self.fitness()\n",
    "            evaluated_population = pop_fitness_scores[0]\n",
    "            self.population = deepcopy(evaluated_population)\n",
    "            fitness_scores = pop_fitness_scores[1]\n",
    "\n",
    "\n",
    "            #for i in range(len(self.population)):\n",
    "            #    print(\"%d, %s, %f\"%(i, self.population[i], fitness_scores[i]))            \n",
    "            \n",
    "            elites_indices = sorted(range(len(self.population)), key=lambda i: fitness_scores[i], reverse=True)[:self.elitism_size]\n",
    "            elites = [self.population[i] for i in elites_indices]    \n",
    "            #print(elites_indices)       \n",
    "            \n",
    "            #fitness_scores.sort(key=lambda x: x[0])\n",
    "\n",
    "            # Elitism: retain the best individuals\n",
    "            new_population = []\n",
    "            new_population.extend(elites)\n",
    "\n",
    "            # Selection probabilities\n",
    "            fitness_values = [1.0 / (f + 1e-6) for f in fitness_scores]\n",
    "            total_fitness = sum(fitness_values)\n",
    "            probabilities = [f / total_fitness for f in fitness_values]\n",
    "\n",
    "            while len(new_population) < self.population_size:\n",
    "                random_num = random.random() \n",
    "                \n",
    "                parent1, parent2 = random.choices(self.population, probabilities, k=2)\n",
    "                \n",
    "                if (random_num < self.crossover_mutation_rate):                    \n",
    "                    #perform crossover                    \n",
    "                    child1, child2 = self.crossover(deepcopy(parent1), deepcopy(parent2))                    \n",
    "                    # and mutation                    \n",
    "                    child1_1 = self.mutate(deepcopy(child1))\n",
    "                    child2_1 = self.mutate(deepcopy(child2))\n",
    "                    new_population.extend([child1_1, child2_1]) \n",
    "                elif (random_num < (self.crossover_mutation_rate + self.mutation_rate)):\n",
    "                    #only mutation\n",
    "                    child = self.mutate(deepcopy(parent1))\n",
    "                    new_population.append(child)\n",
    "                elif (random_num < (self.crossover_mutation_rate + self.mutation_rate + self.crossover_rate)):\n",
    "                    #only crossover\n",
    "                    child1, child2 = self.crossover(deepcopy(parent1), deepcopy(parent2))\n",
    "                    new_population.extend([child1, child2])     \n",
    "                else:\n",
    "                    #no operation\n",
    "                    new_population.extend([deepcopy(parent1), deepcopy(parent2)])\n",
    "                    \n",
    "            # Trim excess individuals\n",
    "            self.population = new_population[:self.population_size]\n",
    "\n",
    "            #for s in self.population:\n",
    "            #    print()\n",
    "            #    print(grammar.parse_tree_to_string(s))\n",
    "\n",
    "            print(\"-----------------------------------------------\")            \n",
    "\n",
    "            # Print best individual of the generation\n",
    "            #best_fitness, best_individual = fitness_scores[0]\n",
    "        print(\"################################################################\")\n",
    "        best_indices = sorted(range(len(self.population)), key=lambda i: fitness_scores[i], reverse=True)[:1]\n",
    "        best_fitness = [fitness_scores[i] for i in elites_indices][0]  \n",
    "        best_individual = self.population[best_indices[0]]\n",
    "        print(f\"Generation {generation}: Best Fitness = {best_fitness}\")\n",
    "        print(f\"Best Individual: {self.grammar.parse_tree_to_string(best_individual)}\")\n",
    "\n",
    "        return None  # Return the best individual\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(7)  # For reproducibility\n",
    "\n",
    "    # Define grammar\n",
    "    grammar_text = \"\"\"\n",
    "    <start> ::= <feature_definition> # <feature_scaling> # <feature_selection> # <ml_algorithms>\n",
    "    <feature_definition> ::=  General_Descriptors | Advanced_Descriptors | Graph_based_Signatures | Toxicophores | Fragments | General_Descriptors Advanced_Descriptors | General_Descriptors Graph_based_Signatures | General_Descriptors Toxicophores | General_Descriptors Fragments | Advanced_Descriptors Graph_based_Signatures | Advanced_Descriptors Toxicophores | Advanced_Descriptors Fragments | Graph_based_Signatures Toxicophores | Graph_based_Signatures Fragments | Toxicophores Fragments | General_Descriptors Advanced_Descriptors Graph_based_Signatures | General_Descriptors Advanced_Descriptors Toxicophores | General_Descriptors Advanced_Descriptors Fragments | General_Descriptors Graph_based_Signatures Toxicophores | General_Descriptors Graph_based_Signatures Fragments | General_Descriptors Toxicophores Fragments | Advanced_Descriptors Graph_based_Signatures Toxicophores | Advanced_Descriptors Graph_based_Signatures Fragments | Advanced_Descriptors Toxicophores Fragments | Graph_based_Signatures Toxicophores Fragments | General_Descriptors Advanced_Descriptors Graph_based_Signatures Toxicophores | General_Descriptors Advanced_Descriptors Graph_based_Signatures Fragments | General_Descriptors Advanced_Descriptors Toxicophores Fragments | General_Descriptors Graph_based_Signatures Toxicophores Fragments | Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments | General_Descriptors Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments\n",
    "    <feature_scaling> ::= <no_scaling> | <normalizer> | MinMaxScaler | MaxAbsScaler | <robust_scaler> | <standard_scaler>\n",
    "    <normalizer> ::= Normalizer <norm>\n",
    "    <robust_scaler> ::= RobustScaler <boolean> <boolean>\n",
    "    <standard_scaler> ::= StandardScaler <boolean> <boolean>\n",
    "    <feature_selection> ::= <no_feature_selection> | <variance_threshold> | <select_percentile> | <selectfpr> | <selectfwe> | <selectfdr>\n",
    "    <variance_threshold> ::= VarianceThreshold <threshold>\n",
    "    <select_percentile> ::= SelectPercentile <percentile> <score_function>\n",
    "    <selectfpr> ::= SelectFpr <value_rand_1> <score_function>\n",
    "    <selectfwe> ::= SelectFwe <value_rand_1> <score_function>\n",
    "    <selectfdr> ::= SelectFdr <value_rand_1> <score_function>\n",
    "    <ml_algorithms> ::= <adaboost> | <decision_tree> | <extra_tree> | <random_rorest> | <extra_trees> | <gradient_boosting> |  <xgboost>\n",
    "    <adaboost> ::= AdaBoostClassifier <algorithm_ada> <n_estimators> <learning_rate_ada>\n",
    "    <decision_tree> ::= DecisionTreeClassifier <criterion> <splitter> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight>\n",
    "    <extra_tree> ::= ExtraTreeClassifier <criterion> <splitter> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight> \n",
    "    <random_rorest> ::= RandomForestClassifier <n_estimators> <criterion> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight_rf>\n",
    "    <extra_trees> ::= ExtraTreesClassifier <n_estimators> <criterion> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight_rf>\n",
    "    <gradient_boosting> ::= GradientBoostingClassifier <n_estimators> <criterion_gb> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <loss>\n",
    "    <xgboost> ::= XGBClassifier <n_estimators> <max_depth> <max_leaves> <learning_rate_ada>\n",
    "    <no_scaling> ::= NoScaling\n",
    "    <no_feature_selection> ::= NoFeatureSelection\n",
    "    <norm> ::= l1 | l2 | max\n",
    "    <threshold> ::= 0.0 | 0.05 | 0.10 | 0.15 | 0.20 | 0.25 | 0.30 | 0.35 | 0.40 | 0.45 | 0.50 | 0.55 | 0.60 | 0.65 | 0.70 | 0.75 | 0.80 | 0.85 | 0.90 | 0.95 | 1.0\n",
    "    <algorithm_ada> ::= SAMME.R | SAMME\n",
    "    <n_estimators> ::= 2 | 5 | 10 | 15 | 20 | 25 | 30 | 35 | 45 | 50 | 55 | 60 | 65 | 70 | 75 | 80 | 90 | 95 | 100 | 150 | 200 | 250 | 300 | 350 | 400 | 450 | 500 | 600 | 700 | 900 | 1000 | 1500 | 2000 | 2500 | 3000 | 4000 | 5000 | 7500 | 10000\n",
    "    <learning_rate_ada> ::= 0.01 | 0.05 | 0.10 | 0.15 | 0.20 | 0.25 | 0.30 | 0.35 | 0.40 | 0.45 | 0.50 | 0.55 | 0.60 | 0.65 | 0.70 | 0.75 | 0.80 | 0.85 | 0.90 | 0.95 | 1.0 | 1.05 | 1.10 | 1.15 | 1.20 | 1.25 | 1.30 | 1.35 | 1.40 | 1.45 | 1.50 | 1.55 | 1.60 | 1.65 | 1.70 | 1.75 | 1.80 | 1.85 | 1.90 | 1.95 | 2.0\n",
    "    <boolean> ::= True | False\n",
    "    <percentile> ::= 5 | 10 | 15 | 20 | 25 | 30 | 35 | 45 | 50 | 55 | 60 | 65 | 70 | 75 | 80 | 90 | 95\n",
    "    <score_function> ::= f_classif | chi2\n",
    "    <value_rand_1> ::= 0.0 | 0.05 | 0.10 | 0.15 | 0.20 | 0.25 | 0.30 | 0.35 | 0.40 | 0.45 | 0.50 | 0.55 | 0.60 | 0.65 | 0.70 | 0.75 | 0.80 | 0.85 | 0.90 | 0.95 | 1.0\n",
    "    <criterion> ::= gini | entropy | log_loss\n",
    "    <splitter> ::= best | random\n",
    "    <max_depth> ::= 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | None\n",
    "    <min_samples_split> ::= 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20\n",
    "    <min_samples_leaf> ::= 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20\n",
    "    <max_features> ::= None | log2 | sqrt\n",
    "    <class_weight> ::= balanced | None\n",
    "    <class_weight_rf> ::= balanced | balanced_subsample | None\n",
    "    <criterion_gb> ::= friedman_mse | squared_error\n",
    "    <loss> ::= log_loss | exponential\n",
    "    <max_leaves> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10\n",
    "    \"\"\"\n",
    "    training_dir = \"/home/alexgcsa/auto-admet/datasets/01_caco2_train.csv\"\n",
    "    testing_dir = \"/home/alexgcsa/auto-admet/datasets/01_caco2_blindtest.csv\"\n",
    "    # Load grammar\n",
    "    grammar = BNFGrammar()\n",
    "    grammar.load_grammar(grammar_text)\n",
    "\n",
    "    # Run GGP\n",
    "    ggp = GrammarBasedGP(grammar, training_dir, testing_dir)\n",
    "    best_program = ggp.evolve()\n",
    "\n",
    "    # Print the best program\n",
    "    #print(\"Best Program Found:\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d0df0-b942-4b60-80cb-113dcc0ddc18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bcb96-5f97-4d08-9acb-216b01c8a890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
