{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2cfc3c-a8f6-446a-aa89-b7073651d567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Advanced_Descriptors Graph_based_Signatures Toxicophores # RobustScaler False False # SelectFwe 0.55 chi2 # DecisionTreeClassifier log_loss best 3 20 10 sqrt None 0.4241639067042495\n",
      "General_Descriptors Advanced_Descriptors Graph_based_Signatures Fragments # MaxAbsScaler # SelectFdr 0.30 chi2 # ExtraTreeClassifier gini random 16 10 7 sqrt None 0.42470023206802104\n",
      "Fragments # StandardScaler False True # SelectFdr 0.80 chi2 # DecisionTreeClassifier gini random None 9 20 None balanced 0.4041569273108099\n",
      "General_Descriptors Advanced_Descriptors Fragments # MinMaxScaler # VarianceThreshold 0.85 # ExtraTreeClassifier entropy random 8 6 3 None balanced 0.5277602611117665\n",
      "Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments # MaxAbsScaler # SelectFpr 0.10 chi2 # ExtraTreeClassifier log_loss random 19 16 3 None None 0.47877305391810665\n",
      "General_Descriptors Advanced_Descriptors Fragments # NoScaling # SelectFwe 0.15 f_classif # ExtraTreeClassifier log_loss best 19 3 20 None None 0.5100281241107358\n",
      "General_Descriptors Advanced_Descriptors Graph_based_Signatures Toxicophores # StandardScaler True False # SelectFdr 0.75 f_classif # RandomForestClassifier 25 log_loss 13 16 13 sqrt balanced 0.5493132567050872\n",
      "General_Descriptors Advanced_Descriptors Graph_based_Signatures # StandardScaler True False # SelectFdr 0.45 chi2 # GradientBoostingClassifier 5 squared_error 12 7 20 None exponential 0.5304387802413778\n",
      "Advanced_Descriptors Toxicophores Fragments # Normalizer l1 # NoFeatureSelection # ExtraTreesClassifier 50 log_loss 20 17 12 None None 0.5405181364559903\n",
      "Advanced_Descriptors # Normalizer l1 # SelectFdr 0.60 chi2 # XGBClassifier 25 6 7 1.25 0.48241220096709253\n",
      "Advanced_Descriptors Toxicophores # Normalizer max # NoFeatureSelection # XGBClassifier 350 19 0 1.60 0.5033853803166309\n",
      "Toxicophores Fragments # StandardScaler False True # VarianceThreshold 0.35 # RandomForestClassifier 250 gini 16 2 16 sqrt balanced_subsample 0.5381189319191297\n",
      "General_Descriptors Graph_based_Signatures Toxicophores Fragments # MinMaxScaler # SelectFdr 0.45 f_classif # AdaBoostClassifier SAMME.R 250 0.45 0.5921418513673495\n",
      "General_Descriptors Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments # Normalizer max # SelectFwe 0.05 f_classif # AdaBoostClassifier SAMME 500 0.45 0.4971824019572658\n",
      "Graph_based_Signatures Toxicophores # Normalizer l2 # SelectFwe 0.75 f_classif # AdaBoostClassifier SAMME 1000 1.50 0.36084578517777466\n",
      "Advanced_Descriptors Graph_based_Signatures # NoScaling # NoFeatureSelection # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.6053847257432378\n",
      "Toxicophores Fragments # Normalizer l2 # SelectFwe 0.15 f_classif # ExtraTreesClassifier 3000 gini 12 2 3 None None 0.40417095578580164\n",
      "General_Descriptors Toxicophores # StandardScaler True False # SelectFwe 0.40 chi2 # AdaBoostClassifier SAMME 3000 1.15 0.5169191491626131\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced 0.635057104706487\n",
      "General_Descriptors Graph_based_Signatures # NoScaling # SelectFpr 0.10 f_classif # AdaBoostClassifier SAMME.R 5000 0.35 0.49781641709286734\n",
      "---------------------------------\n",
      "Best in the population\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "Advanced_Descriptors Graph_based_Signatures # NoScaling # NoFeatureSelection # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential\n",
      "---------------------------------\n",
      "-----------------------------------------------\n",
      "Fragments # Normalizer l2 # SelectFdr 0.80 chi2 # DecisionTreeClassifier gini random None 9 20 None balanced 0.24989249201893982\n",
      "Fragments # StandardScaler False True # SelectFdr 0.80 chi2 # ExtraTreeClassifier log_loss random 19 16 3 None NoneAdvanced_Descriptors Toxicophores # StandardScaler False True # SelectFdr 0.80 chi2 # DecisionTreeClassifier gini random None 9 20 None balanced  0.371540675501954850.49203370808282826\n",
      "\n",
      "Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments # MaxAbsScaler # SelectFpr 0.10 chi2 # DecisionTreeClassifier gini random None 9 20 None balanced 0.5065569592929292\n",
      "General_Descriptors Advanced_Descriptors Graph_based_Signatures Fragments # StandardScaler True False # SelectFdr 0.30 chi2 # ExtraTreeClassifier gini random 16 10 7 sqrt None 0.42470023206802104Advanced_Descriptors Graph_based_Signatures # MinMaxScaler # VarianceThreshold 0.85 # ExtraTreeClassifier entropy random 8 6 3 None balanced\n",
      " 0.4584898473681133\n",
      "General_Descriptors Advanced_Descriptors Graph_based_Signatures Toxicophores # Normalizer l1 # SelectFdr 0.75 f_classif # RandomForestClassifier 25 log_loss 13 16 13 sqrt balanced 0.5849971463345423\n",
      "Advanced_Descriptors # StandardScaler True False # SelectFdr 0.60 chi2 # XGBClassifier 25 6 7 1.25 0.5176950989153555\n",
      "General_Descriptors Toxicophores # StandardScaler True False # SelectFwe 0.40 chi2 # XGBClassifier 350 19 0 1.60 0.5766705478012223\n",
      "Fragments # Normalizer max # NoFeatureSelection # XGBClassifier 350 19 0 1.60 0.42568943560049044\n",
      "General_Descriptors Graph_based_Signatures # NoScaling # SelectFpr 0.10 f_classif # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.6105710443345151\n",
      "General_Descriptors Advanced_Descriptors Fragments # NoScaling # NoFeatureSelection # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.613225451784504\n",
      "Toxicophores Fragments # StandardScaler False True # SelectFwe 0.15 f_classif # ExtraTreesClassifier 3000 gini 12 2 3 None None 0.5624372689152672\n",
      "General_Descriptors Toxicophores # MaxAbsScaler # SelectFwe 0.40 chi2 # AdaBoostClassifier SAMME 3000 1.15 0.5169191491626131\n",
      "Advanced_Descriptors Toxicophores # Normalizer max # NoFeatureSelection # AdaBoostClassifier SAMME 3000 1.15 0.4451594335513853\n",
      "Advanced_Descriptors Graph_based_Signatures # NoScaling # NoFeatureSelection # AdaBoostClassifier SAMME.R 5000 0.35 0.5321234798413663\n",
      "---------------------------------\n",
      "Best in the population\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "General_Descriptors Advanced_Descriptors Fragments # NoScaling # NoFeatureSelection # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential\n",
      "---------------------------------\n",
      "-----------------------------------------------\n",
      "Toxicophores Fragments # RobustScaler False False # SelectFwe 0.55 chi2 # DecisionTreeClassifier log_loss best 3 20 10 sqrt None 0.29176683246914414\n",
      "Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments # RobustScaler False False # SelectFwe 0.55 chi2 # DecisionTreeClassifier log_loss best 3 20 10 sqrt None Advanced_Descriptors Graph_based_Signatures Toxicophores # MaxAbsScaler # SelectFwe 0.40 chi2 # DecisionTreeClassifier gini random None 9 20 None balanced0.34362765359068004 \n",
      "0.5391386663112254\n",
      "Fragments # Normalizer l2 # VarianceThreshold 0.85 # DecisionTreeClassifier gini random None 9 20 None balanced 0.33058704564809194\n",
      "Advanced_Descriptors Graph_based_Signatures # MinMaxScaler # SelectFdr 0.80 chi2 # ExtraTreeClassifier entropy random 8 6 3 None balanced 0.42649190641813\n",
      "General_Descriptors Graph_based_Signatures # StandardScaler False True # SelectFpr 0.10 f_classif # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.6139695292845426\n",
      "Advanced_Descriptors Graph_based_Signatures # NoScaling # SelectFpr 0.10 f_classif # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.6227827856813566\n",
      "Toxicophores Fragments # NoScaling # SelectFwe 0.15 f_classif # ExtraTreesClassifier 3000 gini 12 2 3 None None 0.5624372689152672\n",
      "General_Descriptors Graph_based_Signatures # NoScaling # NoFeatureSelection # AdaBoostClassifier SAMME.R 5000 0.35 0.48962281370095584\n",
      "---------------------------------\n",
      "Best in the population\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "Advanced_Descriptors Graph_based_Signatures # NoScaling # SelectFpr 0.10 f_classif # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential\n",
      "---------------------------------\n",
      "-----------------------------------------------\n",
      "Fragments # NoScaling # SelectFpr 0.10 f_classif # ExtraTreeClassifier log_loss best 4 9 4 None NoneAdvanced_Descriptors Graph_based_Signatures # Normalizer l2 # VarianceThreshold 0.85 # DecisionTreeClassifier gini random None 9 20 None balanced Fragments # StandardScaler False True # NoFeatureSelection # ExtraTreeClassifier log_loss random 19 16 3 None None0.41384986540259944\n",
      " 0.46316641586284585 \n",
      "0.37154067550195485\n",
      "Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments # MaxAbsScaler # SelectFwe 0.55 chi2 # DecisionTreeClassifier log_loss best 3 20 10 sqrt NoneAdvanced_Descriptors Graph_based_Signatures # RobustScaler False False # SelectFwe 0.55 chi2 # DecisionTreeClassifier log_loss best 3 20 10 sqrt None 0.34362765359068004 0.5105828496171098\n",
      "\n",
      "Advanced_Descriptors # StandardScaler True False # NoFeatureSelection # XGBClassifier 25 6 7 1.25 0.5176950989153555\n",
      "Fragments # Normalizer max # SelectFdr 0.60 chi2 # XGBClassifier 350 19 0 1.60 0.3463798250517378\n",
      "Advanced_Descriptors Graph_based_Signatures # Normalizer max # NoFeatureSelection # XGBClassifier 350 19 0 1.60 0.544855528879164\n",
      "Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments # NoScaling # NoFeatureSelection # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.6195427448393123\n",
      "Advanced_Descriptors Graph_based_Signatures # NoScaling # SelectFdr 0.80 chi2 # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.6053847257432378\n",
      "Fragments # NoScaling # NoFeatureSelection # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.49585975897678125\n",
      "General_Descriptors Toxicophores # RobustScaler False False # SelectFwe 0.40 chi2 # AdaBoostClassifier SAMME 3000 1.15 0.5169191491626131\n",
      "---------------------------------\n",
      "Best in the population\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "Advanced_Descriptors Graph_based_Signatures # NoScaling # SelectFpr 0.10 f_classif # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential\n",
      "---------------------------------\n",
      "-----------------------------------------------\n",
      "Fragments # Normalizer max # SelectFpr 0.10 f_classif # DecisionTreeClassifier gini random 16 19 13 sqrt None 0.15472603106921662\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/core.py\", line 730, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1500, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/sklearn.py\", line 521, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/sklearn.py\", line 958, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/core.py\", line 730, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/core.py\", line 1529, in __init__\n",
      "    self._init(\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/core.py\", line 1590, in _init\n",
      "    _check_call(ret)\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/core.py\", line 282, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [14:35:12] /workspace/src/data/iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column.\n",
      "Stack trace:\n",
      "  [bt] (0) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3effba) [0x7c29c71effba]\n",
      "  [bt] (1) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3f59b7) [0x7c29c71f59b7]\n",
      "  [bt] (2) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3f8858) [0x7c29c71f8858]\n",
      "  [bt] (3) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3a2a07) [0x7c29c71a2a07]\n",
      "  [bt] (4) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x2b0) [0x7c29c6f65c40]\n",
      "  [bt] (5) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/lib-dynload/../../libffi.so.7(+0x69dd) [0x7c2aceeeb9dd]\n",
      "  [bt] (6) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/lib-dynload/../../libffi.so.7(+0x6067) [0x7c2aceeeb067]\n",
      "  [bt] (7) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x9319) [0x7c2acee65319]\n",
      "  [bt] (8) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x865a) [0x7c2acee6465a]\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/core.py\", line 730, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1500, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/sklearn.py\", line 521, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/sklearn.py\", line 958, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/core.py\", line 730, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/core.py\", line 1529, in __init__\n",
      "    self._init(\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/core.py\", line 1590, in _init\n",
      "    _check_call(ret)\n",
      "  File \"/home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/core.py\", line 282, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [14:35:13] /workspace/src/data/iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column.\n",
      "Stack trace:\n",
      "  [bt] (0) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3effba) [0x7c29c71effba]\n",
      "  [bt] (1) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3f59b7) [0x7c29c71f59b7]\n",
      "  [bt] (2) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3f8858) [0x7c29c71f8858]\n",
      "  [bt] (3) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3a2a07) [0x7c29c71a2a07]\n",
      "  [bt] (4) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x2b0) [0x7c29c6f65c40]\n",
      "  [bt] (5) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/lib-dynload/../../libffi.so.7(+0x69dd) [0x7c2aceeeb9dd]\n",
      "  [bt] (6) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/lib-dynload/../../libffi.so.7(+0x6067) [0x7c2aceeeb067]\n",
      "  [bt] (7) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x9319) [0x7c2acee65319]\n",
      "  [bt] (8) /home/alexgcsa/anaconda3/envs/autoadmet/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x865a) [0x7c2acee6465a]\n",
      "\n",
      "\n",
      "\n",
      "Fragments # Normalizer l1 # SelectFdr 0.60 chi2 # XGBClassifier 350 19 0 1.60 -1.0\n",
      "Advanced_Descriptors Graph_based_Signatures # NoScaling # SelectFdr 0.80 chi2 # ExtraTreeClassifier log_loss random 19 16 3 None None 0.4783812368757733\n",
      "Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments # StandardScaler True False # SelectFwe 0.55 chi2 # DecisionTreeClassifier log_loss best 3 20 10 sqrt None 0.34362765359068004\n",
      "Advanced_Descriptors # MaxAbsScaler # SelectFdr 0.60 chi2 # XGBClassifier 25 6 7 1.25 0.5176950989153555\n",
      "Advanced_Descriptors # Normalizer max # SelectFdr 0.60 chi2 # XGBClassifier 25 6 7 1.25 0.5132575827371744\n",
      "Fragments # StandardScaler True False # NoFeatureSelection # XGBClassifier 350 19 0 1.60 0.49411579901004715\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # XGBClassifier 350 19 0 1.60 0.5744451926660197\n",
      "Fragments # Normalizer max # NoFeatureSelection # RandomForestClassifier 1000 log_loss 13 3 7 None balanced 0.5013471351499315\n",
      "Fragments # StandardScaler False True # NoFeatureSelection # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.49585975897678125\n",
      "---------------------------------\n",
      "Best in the population\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "Advanced_Descriptors Graph_based_Signatures # NoScaling # SelectFpr 0.10 f_classif # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential\n",
      "---------------------------------\n",
      "-----------------------------------------------\n",
      "Fragments # NoScaling # NoFeatureSelection # DecisionTreeClassifier gini random 16 19 13 sqrt None 0.2238286596128561\n",
      "Graph_based_Signatures Toxicophores # Normalizer max # SelectFpr 0.10 f_classif # DecisionTreeClassifier gini random 16 19 13 sqrt NoneAdvanced_Descriptors Graph_based_Signatures # Normalizer l2 # SelectFdr 0.60 chi2 # DecisionTreeClassifier gini random None 9 20 None balanced  0.34557998113179060.46316641586284585\n",
      "\n",
      "Advanced_Descriptors Graph_based_Signatures # Normalizer l2 # NoFeatureSelection # DecisionTreeClassifier gini random None 9 20 None balanced 0.46316641586284585\n",
      "Advanced_Descriptors # StandardScaler True False # VarianceThreshold 0.85 # XGBClassifier 25 6 7 1.25 0.5249592892378123\n",
      "Fragments # NoScaling # NoFeatureSelection # XGBClassifier 350 19 0 1.60 0.49411579901004715\n",
      "Advanced_Descriptors Graph_based_Signatures # Normalizer max # NoFeatureSelection # XGBClassifier 1500 5 4 1.95 0.4357007540486667\n",
      "Fragments # Normalizer max # SelectFpr 0.10 f_classif # XGBClassifier 10000 8 4 0.10 0.4437915487828813\n",
      "Fragments # StandardScaler False True # VarianceThreshold 0.85 # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.5137535909661534\n",
      "---------------------------------\n",
      "Best in the population\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "Advanced_Descriptors Graph_based_Signatures # NoScaling # SelectFpr 0.10 f_classif # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential\n",
      "---------------------------------\n",
      "-----------------------------------------------\n",
      "General_Descriptors Graph_based_Signatures Fragments # Normalizer l2 # SelectFdr 0.60 chi2 # DecisionTreeClassifier gini random None 9 20 None balanced 0.4210857755091529\n",
      "Advanced_Descriptors Graph_based_Signatures # Normalizer max # NoFeatureSelection # RandomForestClassifier 35 log_loss 7 11 3 log2 balanced 0.5594090743161949\n",
      "Advanced_Descriptors # Normalizer max # VarianceThreshold 0.85 # XGBClassifier 25 6 7 1.25 0.5132575827371744\n",
      "Fragments # Normalizer max # SelectFpr 0.10 f_classif # XGBClassifier 350 19 0 1.60 0.4558095958430005\n",
      "Fragments # Normalizer max # VarianceThreshold 0.85 # XGBClassifier 350 19 0 1.60 0.42568943560049044\n",
      "General_Descriptors Graph_based_Signatures Fragments # Normalizer max # NoFeatureSelection # XGBClassifier 1500 5 4 1.95 0.5102497661239574\n",
      "Fragments # StandardScaler False True # NoFeatureSelection # RandomForestClassifier 1000 log_loss 13 3 7 None balanced 0.5417502081210532\n",
      "Fragments # Normalizer max # NoFeatureSelection # XGBClassifier 10000 8 4 0.10 0.47548399495683064\n",
      "Graph_based_Signatures Toxicophores # NoScaling # SelectFpr 0.10 f_classif # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.5280304710382864\n",
      "Fragments # Normalizer max # VarianceThreshold 0.85 # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.47718154320411943\n",
      "Fragments # StandardScaler False False # SelectFpr 0.10 f_classif # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.46270586538670616\n",
      "Advanced_Descriptors Graph_based_Signatures # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced 0.6564864708422519\n",
      "---------------------------------\n",
      "Best in the population\n",
      "Advanced_Descriptors Graph_based_Signatures # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "---------------------------------\n",
      "-----------------------------------------------\n",
      "Fragments # NoScaling # SelectFpr 0.15 chi2 # DecisionTreeClassifier gini random 16 19 13 sqrt NoneGraph_based_Signatures Toxicophores # NoScaling # SelectFpr 0.10 f_classif # DecisionTreeClassifier gini random 16 19 13 sqrt None  0.149685240006633250.4058963675900282\n",
      "\n",
      "Fragments # Normalizer max # VarianceThreshold 0.85 # DecisionTreeClassifier gini random 16 19 13 sqrt None 0.15917685970357595\n",
      "Fragments # StandardScaler False False # NoFeatureSelection # XGBClassifier 10000 8 4 0.10 0.5027209406156647\n",
      "Fragments # Normalizer max # NoFeatureSelection # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.47718154320411943\n",
      "Graph_based_Signatures Toxicophores # MaxAbsScaler # NoFeatureSelection # GradientBoostingClassifier 1000 friedman_mse 17 2 7 sqrt exponential 0.4958770251418022\n",
      "---------------------------------\n",
      "Best in the population\n",
      "Advanced_Descriptors Graph_based_Signatures # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "---------------------------------\n",
      "-----------------------------------------------\n",
      "Fragments # Normalizer max # NoFeatureSelection # DecisionTreeClassifier gini random 16 19 13 sqrt None Fragments # NoScaling # VarianceThreshold 0.85 # DecisionTreeClassifier gini random 16 19 13 sqrt None0.15917685970357595 \n",
      "Fragments # Normalizer max # VarianceThreshold 0.85 # ExtraTreeClassifier entropy random None 10 13 sqrt balanced0.3061637374654696\n",
      "0.24845167993635023 Fragments # NoScaling # VarianceThreshold 0.85 # DecisionTreeClassifier gini random 16 19 13 sqrt None\n",
      "Fragments # Normalizer max # NoFeatureSelection # DecisionTreeClassifier gini random 16 19 13 sqrt None  0.3061637374654696\n",
      "0.15917685970357595\n",
      "Fragments # NoScaling # SelectFpr 0.15 f_classif # DecisionTreeClassifier gini random 16 19 13 sqrt None 0.24019271102909698\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # DecisionTreeClassifier gini random 16 19 13 sqrt None 0.3966968337789965\n",
      "Fragments # NoScaling # SelectFpr 0.15 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced 0.5546436879299994\n",
      "---------------------------------\n",
      "Best in the population\n",
      "Advanced_Descriptors Graph_based_Signatures # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "---------------------------------\n",
      "-----------------------------------------------\n",
      "Fragments # Normalizer max # SelectFpr 0.15 chi2 # DecisionTreeClassifier gini random 16 19 13 sqrt None 0.19037130033365388\n",
      "Fragments # NoScaling # SelectFpr 0.60 chi2 # DecisionTreeClassifier gini random 16 19 13 sqrt None 0.28542419546758263\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # NoFeatureSelection # DecisionTreeClassifier gini random 16 19 13 sqrt None 0.3966968337789965\n",
      "Advanced_Descriptors Graph_based_Signatures # RobustScaler True True # SelectFpr 0.60 chi2 # DecisionTreeClassifier gini random 16 19 13 sqrt None 0.43714365296779106\n",
      "Fragments # NoScaling # NoFeatureSelection # ExtraTreesClassifier 2 gini 13 18 15 log2 balanced 0.1742446131767509\n",
      "Fragments # StandardScaler False False # SelectFpr 0.15 chi2 # XGBClassifier 10000 8 4 0.10 0.45774307960901767\n",
      "Fragments # NoScaling # NoFeatureSelection # RandomForestClassifier 1000 log_loss 13 3 7 None balanced 0.5417502081210532\n",
      "---------------------------------\n",
      "Best in the population\n",
      "Advanced_Descriptors Graph_based_Signatures # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "General_Descriptors Graph_based_Signatures Fragments # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n",
      "---------------------------------\n",
      "-----------------------------------------------\n",
      "################################################################\n",
      "Generation 9: Best Fitness = 0.6564864708422519\n",
      "Best Individual: Advanced_Descriptors Graph_based_Signatures # RobustScaler True True # SelectFpr 0.60 chi2 # RandomForestClassifier 1000 log_loss 13 3 7 None balanced\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SelectFpr, SelectFwe, SelectFdr, chi2, f_classif\n",
    "import warnings\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class BNFGrammar:\n",
    "    def __init__(self):\n",
    "        self.grammar = defaultdict(list)\n",
    "        self.non_terminals = set()\n",
    "        self.terminals = set()\n",
    "        \n",
    "\n",
    "    def load_grammar(self, bnf_text: str):\n",
    "        \"\"\"\n",
    "        Parses the BNF grammar from a string.\n",
    "        \"\"\"\n",
    "        for line in bnf_text.strip().splitlines():\n",
    "            if \"::=\" in line:\n",
    "                lhs, rhs = line.split(\"::=\", 1)\n",
    "                lhs = lhs.strip()\n",
    "                self.non_terminals.add(lhs)\n",
    "                rhs_options = [option.strip() for option in rhs.split(\"|\")]\n",
    "                for option in rhs_options:\n",
    "                    self.grammar[lhs].append(option.split())\n",
    "                    for token in option.split():\n",
    "                        if token not in self.non_terminals:\n",
    "                            self.terminals.add(token)\n",
    "                            \n",
    "\n",
    "    def generate_parse_tree(self, symbol: str = \"<start>\", max_depth: int = 10) -> dict:\n",
    "        \"\"\"\n",
    "        Generates a parse tree starting from the given symbol, ensuring mandatory grammar components are included.\n",
    "        \"\"\"\n",
    "        if max_depth <= 0 or symbol not in self.grammar:\n",
    "            return symbol  # Return the symbol as a terminal if max depth is reached\n",
    "    \n",
    "        # Strictly enforce the `<start>` rule\n",
    "        if symbol == \"<start>\":\n",
    "            # Generate each mandatory component\n",
    "            feature_def = self.generate_parse_tree(\"<feature_definition>\", max_depth - 1)\n",
    "            scaling = self.generate_parse_tree(\"<feature_scaling>\", max_depth - 1)\n",
    "            selection = self.generate_parse_tree(\"<feature_selection>\", max_depth - 1)\n",
    "            ml_algo = self.generate_parse_tree(\"<ml_algorithms>\", max_depth - 1)\n",
    "    \n",
    "            return {symbol: [feature_def, \"#\", scaling, \"#\", selection, \"#\", ml_algo]}\n",
    "    \n",
    "        # Select a random production for other non-terminals\n",
    "        production = random.choice(self.grammar[symbol])\n",
    "        return {symbol: [self.generate_parse_tree(token, max_depth - 1) for token in production]}\n",
    "\n",
    "    \n",
    "    def parse_tree_to_string(self, tree) -> str:\n",
    "        \"\"\"\n",
    "        Reconstructs a string from the parse tree.\n",
    "        \"\"\"\n",
    "        if isinstance(tree, str):\n",
    "            # Leaf node (terminal)\n",
    "            return tree\n",
    "        # Non-terminal with its production rules as children\n",
    "        root, children = list(tree.items())[0]\n",
    "        return \" \".join(self.parse_tree_to_string(child) for child in children)\n",
    "\n",
    "    \n",
    "    def validate_parse_tree(self, tree, symbol=\"<start>\") -> bool:\n",
    "        \"\"\"\n",
    "        Validates if the parse tree conforms to the grammar and respects the `<start>` structure.\n",
    "        \"\"\"\n",
    "        if isinstance(tree, str):\n",
    "            return tree in self.terminals  # Check terminal validity\n",
    "    \n",
    "        if not isinstance(tree, dict) or len(tree) != 1:\n",
    "            return False\n",
    "    \n",
    "        root, children = list(tree.items())[0]\n",
    "        if root != symbol:\n",
    "            return False\n",
    "    \n",
    "        if symbol == \"<start>\":\n",
    "            # Check `<start>` structure\n",
    "            if len(children) != 7:\n",
    "                return False\n",
    "            expected_symbols = [\"<feature_definition>\", \"#\", \"<feature_scaling>\", \"#\", \"<feature_selection>\", \"#\", \"<ml_algorithms>\"]\n",
    "            for i, child_symbol in enumerate(expected_symbols):\n",
    "                if i % 2 == 0 and not self.validate_parse_tree(children[i], child_symbol):  # Validate non-terminals\n",
    "                    return False\n",
    "                if i % 2 == 1 and children[i] != \"#\":  # Ensure separator\n",
    "                    return False\n",
    "    \n",
    "        # Validate other non-terminals\n",
    "        for production in self.grammar[symbol]:\n",
    "            if len(production) == len(children) and all(\n",
    "                self.validate_parse_tree(child, production[i])\n",
    "                for i, child in enumerate(children)\n",
    "            ):\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "        \n",
    "\n",
    "class MLAlgorithmTransformer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def XGBoost(self, n_estimators_str, max_depth_str, max_leaves_str, learning_rate_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        clf = XGBClassifier(n_estimators=int(n_estimators_str), max_depth=max_depth_actual, random_state=42, \n",
    "                            max_leaves=int(max_leaves_str), learning_rate=float(learning_rate_str), n_jobs=1)        \n",
    "    \n",
    "        return clf \n",
    "    \n",
    "    \n",
    "    def GradientBoosting(self, n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, loss_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, random_state=42, \n",
    "                                         min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str), \n",
    "                                         max_features=max_features_actual, loss=loss_str)        \n",
    "    \n",
    "        return clf      \n",
    " \n",
    "    \n",
    "    def ExtraTrees(self, n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "     \n",
    "            \n",
    "        clf = ExtraTreesClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, n_jobs=1, random_state=42, \n",
    "                                   class_weight=class_weight_actual,  min_samples_split=int(min_samples_split_str), \n",
    "                                   min_samples_leaf=int(min_samples_split_str), max_features=max_features_actual)        \n",
    "    \n",
    "        return clf  \n",
    "    \n",
    "    \n",
    "    def RandomForest(self, n_estimators_str, criterion_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "     \n",
    "        clf = RandomForestClassifier(n_estimators=int(n_estimators_str), criterion=criterion_str, max_depth=max_depth_actual, n_jobs=1, random_state=42, \n",
    "                                     class_weight=class_weight_actual,  min_samples_split=int(min_samples_split_str), \n",
    "                                     min_samples_leaf=int(min_samples_split_str), max_features=max_features_actual)        \n",
    " \n",
    "        return clf   \n",
    "        \n",
    "    \n",
    "    def ExtraTree(self, criterion_str, splitter_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "            \n",
    "        clf = ExtraTreeClassifier(criterion=criterion_str, splitter='best', max_depth=max_depth_actual, \n",
    "                                  min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str),                                      \n",
    "                                  max_features=max_features_actual, random_state=0)      \n",
    "    \n",
    "        return clf  \n",
    "            \n",
    "    \n",
    "    def DecisionTree(self, criterion_str, splitter_str, max_depth_str, min_samples_split_str, min_samples_leaf_str, max_features_str, class_weight_str):\n",
    "        max_depth_actual = None\n",
    "        if max_depth_str != \"None\":\n",
    "            max_depth_actual = int(max_depth_str)\n",
    "    \n",
    "        max_features_actual = None\n",
    "        if max_features_str != \"None\":\n",
    "            max_features_actual = max_features_str  \n",
    "    \n",
    "        class_weight_actual = None\n",
    "        if class_weight_str != \"None\":\n",
    "            class_weight_actual = class_weight_str   \n",
    "\n",
    "\n",
    "        clf = DecisionTreeClassifier(criterion=criterion_str, splitter=splitter_str, max_depth=max_depth_actual, \n",
    "                                     min_samples_split=int(min_samples_split_str), min_samples_leaf=int(min_samples_split_str), \n",
    "                                     max_features=max_features_actual, random_state=0,\n",
    "                                     class_weight=class_weight_actual)      \n",
    "    \n",
    "        return clf\n",
    "    \n",
    "    \n",
    "    def AdaBoost(self, alg, n_est, lr):\n",
    "        clf = AdaBoostClassifier(n_estimators=n_est, learning_rate=lr, algorithm=alg, random_state=0)\n",
    "        return clf\n",
    "\n",
    "\n",
    "\n",
    "class FeatureSelectionTransformer:\n",
    "    def __init__(self, df, label_col):\n",
    "        self.df = df\n",
    "        self.label_col = label_col\n",
    "\n",
    "    def select_fwe(self, alpha_str, score_function_str):\n",
    "    \n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            model = SelectFwe(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.df, self.label_col)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return self.df  \n",
    "            \n",
    "    \n",
    "    def select_fdr(self, alpha_str, score_function_str):\n",
    "    \n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            model = SelectFdr(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.df, self.label_col)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return self.df \n",
    "            \n",
    "    \n",
    "    def select_fpr(self, alpha_str, score_function_str):\n",
    "    \n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            model = SelectFpr(score_func=score_function_actual, alpha = float(alpha_str)).fit(self.df, self.label_col)\n",
    "            df_np = model.transform(self.df)\n",
    "        \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return self.df  \n",
    "            \n",
    "    \n",
    "    def select_percentile(self, percentile_str, score_function_str):\n",
    "        score_function_actual = f_classif\n",
    "    \n",
    "        if(score_function_str == \"chi2\"):\n",
    "            score_function_actual = chi2       \n",
    "        \n",
    "        try:\n",
    "            model = SelectPercentile(score_func=score_function_actual, percentile = int(percentile_str)).fit(self.df, self.label_col)\n",
    "            df_np = model.transform(self.df)\n",
    "        \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return self.df\n",
    "            \n",
    "    \n",
    "    def variance_threshold(self,thrsh):\n",
    "        try:\n",
    "            model =VarianceThreshold(threshold=thrsh).fit(self.df, self.label_col)\n",
    "            df_np = model.transform(self.df)\n",
    "        \n",
    "            cols_idxs = model.get_support(indices=True)\n",
    "            features_df_new = self.df.iloc[:,cols_idxs]\n",
    "        \n",
    "            return features_df_new\n",
    "        except Exception as e:\n",
    "            return self.df\n",
    "\n",
    "\n",
    "class ScalingTransformer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def normalizer(self, norm_hp):\n",
    "        try:\n",
    "            model = Normalizer(norm=norm_hp).fit(self.df)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.df.columns)\n",
    "        except:\n",
    "            return self.df \n",
    "\n",
    "    \n",
    "    def max_abs_scaler(self):\n",
    "        try:\n",
    "            model = MaxAbsScaler().fit(self.df)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.df.columns)\n",
    "        except:\n",
    "            return self.df  \n",
    "\n",
    "    \n",
    "    def min_max_scaler(self):\n",
    "        try:\n",
    "            model = MinMaxScaler().fit(self.df)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.df.columns)\n",
    "        except:\n",
    "            return self.df  \n",
    "\n",
    "    \n",
    "    def standard_scaler(self, with_mean_str, with_std_str):\n",
    "        with_mean_actual = True\n",
    "        with_std_actual = True\n",
    "    \n",
    "        if with_mean_str == \"False\":\n",
    "            with_mean_actual = False\n",
    "        if with_std_str == \"False\":\n",
    "            with_std_actual = False        \n",
    "        try:\n",
    "            model = StandardScaler(with_mean=with_mean_actual, with_std=with_std_actual).fit(self.df)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.df.columns)\n",
    "        except:\n",
    "            return self.df \n",
    "\n",
    "    \n",
    "    def robust_scaler(self, with_centering_str, with_scaling_str):\n",
    "        with_centering_actual = True\n",
    "        with_scaling_actual = True\n",
    "    \n",
    "        if with_centering_str == \"False\":\n",
    "            with_centering_actual = False\n",
    "        if with_scaling_str == \"False\":\n",
    "            with_scaling_actual = False        \n",
    "        try:\n",
    "            model = RobustScaler(with_centering=with_centering_actual, with_scaling=with_scaling_actual).fit(self.df)\n",
    "            df_np = model.transform(self.df)\n",
    "    \n",
    "            return pd.DataFrame(df_np, columns = self.df.columns)\n",
    "        except:\n",
    "            return self.df\n",
    "\n",
    "\n",
    "class GrammarBasedGP:\n",
    "    def __init__(self, grammar, training_dir, testing_dir, fitness_cache={}, num_cores=20, time_budget_minutes_alg_eval = 3, population_size=20, max_generations=10, mutation_rate=0.1, crossover_rate=0.7, crossover_mutation_rate=0.05, elitism_size=2):\n",
    "        self.grammar = grammar\n",
    "        self.training_dir = training_dir\n",
    "        self.testing_dir = testing_dir\n",
    "        self.fitness_cache = fitness_cache\n",
    "        self.num_cores = num_cores\n",
    "        self.time_budget_minutes_alg_eval = time_budget_minutes_alg_eval\n",
    "        self.population_size = population_size\n",
    "        self.max_generations = max_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.crossover_mutation_rate = crossover_mutation_rate\n",
    "        self.elitism_size = elitism_size        \n",
    "        self.population = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def select_ml_algorithms(self, ml_algorithm):\n",
    "        ml_alg_selection = MLAlgorithmTransformer()\n",
    "        if ml_algorithm[0] == \"AdaBoostClassifier\":\n",
    "            return ml_alg_selection.AdaBoost(str(ml_algorithm[1]), int(ml_algorithm[2]), float(ml_algorithm[3]))\n",
    "        elif ml_algorithm[0] == \"DecisionTreeClassifier\":\n",
    "            return ml_alg_selection.DecisionTree(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])    \n",
    "        elif ml_algorithm[0] == \"ExtraTreeClassifier\":\n",
    "            return ml_alg_selection.ExtraTree(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"RandomForestClassifier\":\n",
    "            return ml_alg_selection.RandomForest(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"ExtraTreesClassifier\":\n",
    "            return ml_alg_selection.ExtraTrees(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7])         \n",
    "        elif ml_algorithm[0] == \"GradientBoostingClassifier\":\n",
    "            return ml_alg_selection.GradientBoosting(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4], ml_algorithm[5], ml_algorithm[6], ml_algorithm[7]) \n",
    "        elif ml_algorithm[0] == \"XGBClassifier\":\n",
    "            return ml_alg_selection.XGBoost(ml_algorithm[1], ml_algorithm[2], ml_algorithm[3], ml_algorithm[4])         \n",
    "                            \n",
    "        else:\n",
    "            return None    \n",
    "\n",
    "    \n",
    "    def select_features(self, feature_selection, dataset_df, label_col):\n",
    "\n",
    "        cp_dataset_df = dataset_df.copy(deep=True)\n",
    "        feature_selection_transformer = FeatureSelectionTransformer(cp_dataset_df, label_col)\n",
    "    \n",
    "        if feature_selection[0] == \"NoFeatureSelection\":\n",
    "            return dataset_df\n",
    "        elif feature_selection[0] == \"VarianceThreshold\":\n",
    "            mod_dataset_df = feature_selection_transformer.variance_threshold(float(feature_selection[1]))\n",
    "            return mod_dataset_df\n",
    "        elif feature_selection[0] == \"SelectPercentile\":        \n",
    "            mod_dataset_df = feature_selection_transformer.select_percentile(feature_selection[1], feature_selection[2])    \n",
    "            return mod_dataset_df     \n",
    "        elif feature_selection[0] == \"SelectFpr\":        \n",
    "            mod_dataset_df = feature_selection_transformer.select_fpr(feature_selection[1], feature_selection[2])    \n",
    "            return mod_dataset_df\n",
    "        elif feature_selection[0] == \"SelectFdr\":        \n",
    "            mod_dataset_df = feature_selection_transformer.select_fdr(feature_selection[1], feature_selection[2])    \n",
    "            return mod_dataset_df\n",
    "        elif feature_selection[0] == \"SelectFwe\":        \n",
    "            mod_dataset_df = feature_selection_transformer.select_fwe(feature_selection[1], feature_selection[2])    \n",
    "            return mod_dataset_df          \n",
    "        else:\n",
    "            return cp_dataset_df        \n",
    "\n",
    "    \n",
    "    def scale_features(self, feature_scaling, dataset_df):\n",
    "\n",
    "        cp_dataset_df = dataset_df.copy(deep=True)\n",
    "        scaling_transformer = ScalingTransformer(cp_dataset_df)\n",
    "        if feature_scaling[0] == \"NoScaling\":\n",
    "            return dataset_df\n",
    "        elif feature_scaling[0] == \"Normalizer\":\n",
    "            mod_dataset_df = scaling_transformer.normalizer(str(feature_scaling[1]))\n",
    "            return mod_dataset_df\n",
    "        elif feature_scaling[0] == \"MinMaxScaler\":\n",
    "            mod_dataset_df = scaling_transformer.min_max_scaler()\n",
    "            return mod_dataset_df\n",
    "        elif feature_scaling[0] == \"MaxAbsScaler\":\n",
    "            mod_dataset_df = scaling_transformer.max_abs_scaler()\n",
    "            return mod_dataset_df    \n",
    "        elif feature_scaling[0] == \"StandardScaler\":\n",
    "            mod_dataset_df = scaling_transformer.standard_scaler(feature_scaling[1], feature_scaling[2])\n",
    "            return mod_dataset_df\n",
    "        elif feature_scaling[0] == \"RobustScaler\":\n",
    "            mod_dataset_df = scaling_transformer.robust_scaler(feature_scaling[1], feature_scaling[2])\n",
    "            return mod_dataset_df           \n",
    "        else:\n",
    "            return cp_dataset_df    \n",
    "\n",
    "\n",
    "    def represent_molecules(self, list_of_feature_types, dataset_df):\n",
    "        \"\"\"\n",
    "        represents a chemical dataset with descriptors.\n",
    "        \"\"\"          \n",
    "    \n",
    "        columns = []\n",
    "        for lft in list_of_feature_types:\n",
    "            if lft == \"General_Descriptors\":\n",
    "                columns += [\"HeavyAtomCount\",\"MolLogP\",\"NumHeteroatoms\",\"NumRotatableBonds\",\"RingCount\",\"TPSA\",\"LabuteASA\",\"MolWt\",\"FCount\",\"FCount2\",\"Acceptor_Count\",\"Aromatic_Count\",\"Donor_Count\",\"Hydrophobe_Count\",\"NegIonizable_Count\",\"PosIonizable_Count\",]\n",
    "            elif lft == \"Advanced_Descriptors\":\n",
    "                columns += [\"BalabanJ\",\"BertzCT\",\"Chi0\",\"Chi0n\",\"Chi0v\",\"Chi1\",\"Chi1n\",\"Chi1v\",\"Chi2n\",\"Chi2v\",\"Chi3n\",\"Chi3v\",\"Chi4n\",\"Chi4v\",\"HallKierAlpha\",\"Kappa1\",\"Kappa2\",\"Kappa3\",\"NHOHCount\",\"NOCount\",\"PEOE_VSA1\",\"PEOE_VSA10\",\"PEOE_VSA11\",\"PEOE_VSA12\",\"PEOE_VSA13\",\"PEOE_VSA14\",\"PEOE_VSA2\",\"PEOE_VSA3\",\"PEOE_VSA4\",\"PEOE_VSA5\",\"PEOE_VSA6\",\"PEOE_VSA7\",\"PEOE_VSA8\",\"PEOE_VSA9\",\"SMR_VSA1\",\"SMR_VSA10\",\"SMR_VSA2\",\"SMR_VSA3\",\"SMR_VSA4\",\"SMR_VSA5\",\"SMR_VSA6\",\"SMR_VSA7\",\"SMR_VSA8\",\"SMR_VSA9\",\"SlogP_VSA1\",\"SlogP_VSA10\",\"SlogP_VSA11\",\"SlogP_VSA12\",\"SlogP_VSA2\",\"SlogP_VSA3\",\"SlogP_VSA4\",\"SlogP_VSA5\",\"SlogP_VSA6\",\"SlogP_VSA7\",\"SlogP_VSA8\",\"SlogP_VSA9\",\"VSA_EState1\",\"VSA_EState10\",\"VSA_EState2\",\"VSA_EState3\",\"VSA_EState4\",\"VSA_EState5\",\"VSA_EState6\",\"VSA_EState7\",\"VSA_EState8\",\"VSA_EState9\"]\n",
    "            elif lft == \"Toxicophores\":\n",
    "                columns += [\"Tox_1\",\"Tox_2\",\"Tox_3\",\"Tox_4\",\"Tox_5\",\"Tox_6\",\"Tox_7\",\"Tox_8\",\"Tox_9\",\"Tox_10\",\"Tox_11\",\"Tox_12\",\"Tox_13\",\"Tox_14\",\"Tox_15\",\"Tox_16\",\"Tox_17\",\"Tox_18\",\"Tox_19\",\"Tox_20\",\"Tox_21\",\"Tox_22\",\"Tox_23\",\"Tox_24\",\"Tox_25\",\"Tox_26\",\"Tox_27\",\"Tox_28\",\"Tox_29\",\"Tox_30\",\"Tox_31\",\"Tox_32\",\"Tox_33\",\"Tox_34\",\"Tox_35\",\"Tox_36\"]\n",
    "            elif lft == \"Fragments\":\n",
    "                columns += [\"fr_Al_COO\",\"fr_Al_OH\",\"fr_Al_OH_noTert\",\"fr_ArN\",\"fr_Ar_COO\",\"fr_Ar_N\",\"fr_Ar_NH\",\"fr_Ar_OH\",\"fr_COO\",\"fr_COO2\",\"fr_C_O\",\"fr_C_O_noCOO\",\"fr_C_S\",\"fr_HOCCN\",\"fr_Imine\",\"fr_NH0\",\"fr_NH1\",\"fr_NH2\",\"fr_N_O\",\"fr_Ndealkylation1\",\"fr_Ndealkylation2\",\"fr_Nhpyrrole\",\"fr_SH\",\"fr_aldehyde\",\"fr_alkyl_carbamate\",\"fr_alkyl_halide\",\"fr_allylic_oxid\",\"fr_amide\",\"fr_amidine\",\"fr_aniline\",\"fr_aryl_methyl\",\"fr_azide\",\"fr_azo\",\"fr_barbitur\",\"fr_benzene\",\"fr_benzodiazepine\",\"fr_bicyclic\",\"fr_diazo\",\"fr_dihydropyridine\",\"fr_epoxide\",\"fr_ester\",\"fr_ether\",\"fr_furan\",\"fr_guanido\",\"fr_halogen\",\"fr_hdrzine\",\"fr_hdrzone\",\"fr_imidazole\",\"fr_imide\",\"fr_isocyan\",\"fr_isothiocyan\",\"fr_ketone\",\"fr_ketone_Topliss\",\"fr_lactam\",\"fr_lactone\",\"fr_methoxy\",\"fr_morpholine\",\"fr_nitrile\",\"fr_nitro\",\"fr_nitro_arom\",\"fr_nitro_arom_nonortho\",\"fr_nitroso\",\"fr_oxazole\",\"fr_oxime\",\"fr_para_hydroxylation\",\"fr_phenol\",\"fr_phenol_noOrthoHbond\",\"fr_phos_acid\",\"fr_phos_ester\",\"fr_piperdine\",\"fr_piperzine\",\"fr_priamide\",\"fr_prisulfonamd\",\"fr_pyridine\",\"fr_quatN\",\"fr_sulfide\",\"fr_sulfonamd\",\"fr_sulfone\",\"fr_term_acetylene\",\"fr_tetrazole\",\"fr_thiazole\",\"fr_thiocyan\",\"fr_thiophene\",\"fr_unbrch_alkane\",\"fr_urea\"]\n",
    "            elif lft == \"Graph_based_Signatures\":\n",
    "                columns += [\"Acceptor:Acceptor-6.00\",\"Acceptor:Aromatic-6.00\",\"Acceptor:Donor-6.00\",\"Acceptor:Hydrophobe-6.00\",\"Acceptor:NegIonizable-6.00\",\"Acceptor:PosIonizable-6.00\",\"Aromatic:Aromatic-6.00\",\"Aromatic:Donor-6.00\",\"Aromatic:Hydrophobe-6.00\",\"Aromatic:NegIonizable-6.00\",\"Aromatic:PosIonizable-6.00\",\"Donor:Donor-6.00\",\"Donor:Hydrophobe-6.00\",\"Donor:NegIonizable-6.00\",\"Donor:PosIonizable-6.00\",\"Hydrophobe:Hydrophobe-6.00\",\"Hydrophobe:NegIonizable-6.00\",\"Hydrophobe:PosIonizable-6.00\",\"NegIonizable:NegIonizable-6.00\",\"NegIonizable:PosIonizable-6.00\",\"PosIonizable:PosIonizable-6.00\",\"Acceptor:Acceptor-4.00\",\"Acceptor:Aromatic-4.00\",\"Acceptor:Donor-4.00\",\"Acceptor:Hydrophobe-4.00\",\"Acceptor:NegIonizable-4.00\",\"Acceptor:PosIonizable-4.00\",\"Aromatic:Aromatic-4.00\",\"Aromatic:Donor-4.00\",\"Aromatic:Hydrophobe-4.00\",\"Aromatic:NegIonizable-4.00\",\"Aromatic:PosIonizable-4.00\",\"Donor:Donor-4.00\",\"Donor:Hydrophobe-4.00\",\"Donor:NegIonizable-4.00\",\"Donor:PosIonizable-4.00\",\"Hydrophobe:Hydrophobe-4.00\",\"Hydrophobe:NegIonizable-4.00\",\"Hydrophobe:PosIonizable-4.00\",\"NegIonizable:NegIonizable-4.00\",\"NegIonizable:PosIonizable-4.00\",\"PosIonizable:PosIonizable-4.00\",\"Acceptor:Acceptor-2.00\",\"Acceptor:Aromatic-2.00\",\"Acceptor:Donor-2.00\",\"Acceptor:Hydrophobe-2.00\",\"Acceptor:NegIonizable-2.00\",\"Acceptor:PosIonizable-2.00\",\"Aromatic:Aromatic-2.00\",\"Aromatic:Donor-2.00\",\"Aromatic:Hydrophobe-2.00\",\"Aromatic:NegIonizable-2.00\",\"Aromatic:PosIonizable-2.00\",\"Donor:Donor-2.00\",\"Donor:Hydrophobe-2.00\",\"Donor:NegIonizable-2.00\",\"Donor:PosIonizable-2.00\",\"Hydrophobe:Hydrophobe-2.00\",\"Hydrophobe:NegIonizable-2.00\",\"Hydrophobe:PosIonizable-2.00\",\"NegIonizable:NegIonizable-2.00\",\"NegIonizable:PosIonizable-2.00\",\"PosIonizable:PosIonizable-2.00\"]\n",
    "            \n",
    "        mod_dataset_df = None\n",
    "        try:\n",
    "            cp_dataset_df = dataset_df.copy(deep=True)\n",
    "            mod_dataset_df = cp_dataset_df[columns]\n",
    "        except:\n",
    "            print(\"Error representation. \")\n",
    "    \n",
    "    \n",
    "        return mod_dataset_df    \n",
    "\n",
    "    \n",
    "    def evaluate_fitness(self, pipeline, dataset_path, time_budget_minutes_alg_eval):\n",
    "        \"\"\"\n",
    "        evaluates pipeline with the fitness, performing each step of the ML pipeline.\n",
    "        \"\"\"  \n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        #all the steps in Auto-ADMET pipeline:\n",
    "        pipeline_string = self.grammar.parse_tree_to_string(pipeline)\n",
    "        pipeline_list = pipeline_string.split(\" # \")\n",
    "        representation = pipeline_list[0].split(\" \")\n",
    "        feature_scaling = pipeline_list[1].split(\" \")\n",
    "        feature_selection = pipeline_list[2].split(\" \")\n",
    "        ml_algorithm = pipeline_list[3].split(\" \")\n",
    "\n",
    "        #applying the steps to an actual dataset:\n",
    "        dataset_df = pd.read_csv(self.training_dir, header=0, sep=\",\")\n",
    "        #dataset_df = dataset_df.sample(random_state=generation, frac = 1.0)\n",
    "\n",
    "        label_col = dataset_df[\"CLASS\"]\n",
    "        dataset_df = dataset_df.drop(\"CLASS\", axis=1)\n",
    "\n",
    "        rep_dataset_df = self.represent_molecules(representation, dataset_df) \n",
    "        prep_dataset_df = self.scale_features(feature_scaling, rep_dataset_df)\n",
    "        sel_dataset_df = self.select_features(feature_selection, prep_dataset_df, label_col)\n",
    "        ml_algorithm  = self.select_ml_algorithms(ml_algorithm)\n",
    "        sel_dataset_df[\"CLASS\"] = pd.Series(label_col)        \n",
    "        \n",
    "        try:\n",
    "            y = sel_dataset_df.iloc[:,-1:]\n",
    "            X = sel_dataset_df[sel_dataset_df.columns[:-1]]\n",
    "            scores = cross_val_score(ml_algorithm, X, y, cv=5, scoring=make_scorer(matthews_corrcoef))\n",
    "            fitness_value = scores.mean()\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            fitness_value = -1.00            \n",
    "\n",
    "        # This function should evaluate the fitness of the individual within the time budget        \n",
    "        elapsed_time = time.time() - start_time    \n",
    "        if elapsed_time > (time_budget_minutes_alg_eval * 60):  # Check if elapsed time exceeds time budget\n",
    "            fitness_value = 0.0  # Set fitness value to zero if time budget exceeded\n",
    "\n",
    "        print(pipeline_string, fitness_value)\n",
    "            \n",
    "        return fitness_value\n",
    "        \n",
    "    def fitness(self):\n",
    "        \"\"\"\n",
    "        Calculates the fitness function in parallel using multiprocessing,\n",
    "        while caching results to avoid redundant evaluations.\n",
    "        \"\"\"\n",
    "        with multiprocessing.Pool(processes=self.num_cores) as pool:\n",
    "            results = []\n",
    "            async_results = []\n",
    "            \n",
    "            # Submit all tasks asynchronously, checking cache first\n",
    "            for pipeline in self.population:\n",
    "                pipeline_str =self.grammar.parse_tree_to_string(pipeline)  # Convert individual to a string representation\n",
    "                \n",
    "                if pipeline_str in self.fitness_cache:\n",
    "                    # Use cached value if available\n",
    "                    results.append((pipeline, self.fitness_cache[pipeline_str]))\n",
    "                else:\n",
    "                    # Otherwise, evaluate it asynchronously\n",
    "                    async_result = pool.apply_async(\n",
    "                        self.evaluate_fitness, \n",
    "                        (pipeline, self.training_dir, self.time_budget_minutes_alg_eval)\n",
    "                    )\n",
    "                    async_results.append((pipeline, async_result))\n",
    "    \n",
    "            # Collect results in a non-blocking way\n",
    "            for pipeline, async_result in async_results:\n",
    "                try:\n",
    "                    fitness_value = async_result.get(timeout=self.time_budget_minutes_alg_eval * 60)\n",
    "                except multiprocessing.TimeoutError:\n",
    "                    fitness_value = 0.0  # Timeout case\n",
    "                \n",
    "                # Cache the computed fitness value\n",
    "                pipeline_str =self.grammar.parse_tree_to_string(pipeline)\n",
    "                self.fitness_cache[pipeline_str] = fitness_value  # Store in dictionary\n",
    "                results.append((pipeline, fitness_value))\n",
    "        \n",
    "        # Separate pipelines and fitness values\n",
    "        pipelines, fitness_results = zip(*results) if results else ([], [])\n",
    "    \n",
    "        return list(pipelines), list(fitness_results)\n",
    "\n",
    "\n",
    "    \n",
    "    def crossover(self, parent1, parent2):\n",
    "        \"\"\"\n",
    "        Performs crossover by swapping compatible components between parents.\n",
    "        \"\"\"\n",
    "        if isinstance(parent1, str) or isinstance(parent2, str):  # No crossover if terminal\n",
    "            return parent1, parent2\n",
    "    \n",
    "        root1, children1 = list(parent1.items())[0]\n",
    "        root2, children2 = list(parent2.items())[0]\n",
    "    \n",
    "        if root1 == \"<start>\" and root2 == \"<start>\":\n",
    "            # Swap one of the four main components of `<start>`\n",
    "            idx = random.choice([0, 2, 4, 6])  # Indices of the main components\n",
    "            children1[idx], children2[idx] = children2[idx], children1[idx]\n",
    "        elif root1 == root2:\n",
    "            # Swap subtrees for other non-terminals\n",
    "            idx1 = random.randint(0, len(children1) - 1)\n",
    "            idx2 = random.randint(0, len(children2) - 1)\n",
    "            children1[idx1], children2[idx2] = children2[idx2], children1[idx1]\n",
    "    \n",
    "        return parent1, parent2\n",
    "\n",
    "    \n",
    "    def mutate(self, individual, max_mutation_depth=4):\n",
    "        \"\"\"\n",
    "        Mutates an individual by replacing a specific component with a new valid subtree.\n",
    "        \"\"\"\n",
    "        if isinstance(individual, str):  # Terminal, no mutation possible\n",
    "            return individual\n",
    "    \n",
    "        root, children = list(individual.items())[0]\n",
    "    \n",
    "        if root == \"<start>\":\n",
    "            # Mutate one of the four main components of `<start>`\n",
    "            idx = random.choice([0, 2, 4, 6])  # Indices of the main components\n",
    "            components = [\"<feature_definition>\", \"<feature_scaling>\", \"<feature_selection>\", \"<ml_algorithms>\"]\n",
    "            replacement = self.grammar.generate_parse_tree(components[idx // 2], max_depth=max_mutation_depth)\n",
    "            children[idx] = replacement\n",
    "        else:\n",
    "            # Mutate other non-terminals\n",
    "            idx = random.randint(0, len(children) - 1)\n",
    "            children[idx] = self.grammar.generate_parse_tree(root, max_depth=max_mutation_depth)\n",
    "    \n",
    "        return individual\n",
    "    \n",
    "    \n",
    "    def evolve(self):\n",
    "        \"\"\"\n",
    "        Runs the genetic programming algorithm.\n",
    "        \"\"\"\n",
    "        # Initialize population\n",
    "        self.population = [self.grammar.generate_parse_tree() for _ in range(self.population_size)]\n",
    "           \n",
    "        print(\"-----------------------------------------------\")\n",
    "        for generation in range(self.max_generations):\n",
    "            # Evaluate fitness\n",
    "            pop_fitness_scores = self.fitness()\n",
    "            evaluated_population = pop_fitness_scores[0]\n",
    "            self.population = deepcopy(evaluated_population)\n",
    "            fitness_scores = pop_fitness_scores[1]\n",
    "\n",
    "            elites_indices = sorted(range(len(self.population)), key=lambda i: fitness_scores[i], reverse=True)[:self.elitism_size]\n",
    "            elites = [self.population[i] for i in elites_indices]    \n",
    "\n",
    "            # Elitism: retain the best individuals\n",
    "            new_population = []\n",
    "            new_population.extend(elites)\n",
    "\n",
    "            # Selection probabilities\n",
    "            fitness_values = [1.0 / (f + 1e-6) for f in fitness_scores]\n",
    "            total_fitness = sum(fitness_values)\n",
    "            probabilities = [f / total_fitness for f in fitness_values]\n",
    "\n",
    "            while len(new_population) < self.population_size:\n",
    "                random_num = random.random()                \n",
    "                parent1, parent2 = random.choices(self.population, probabilities, k=2)\n",
    "                \n",
    "                if (random_num < self.crossover_mutation_rate):                    \n",
    "                    #perform crossover                    \n",
    "                    child1, child2 = self.crossover(deepcopy(parent1), deepcopy(parent2))                    \n",
    "                    # and mutation                    \n",
    "                    child1_1 = self.mutate(deepcopy(child1))\n",
    "                    child2_1 = self.mutate(deepcopy(child2))\n",
    "                    new_population.extend([child1_1, child2_1]) \n",
    "                elif (random_num < (self.crossover_mutation_rate + self.mutation_rate)):\n",
    "                    #only mutation\n",
    "                    child = self.mutate(deepcopy(parent1))\n",
    "                    new_population.append(child)\n",
    "                elif (random_num < (self.crossover_mutation_rate + self.mutation_rate + self.crossover_rate)):\n",
    "                    #only crossover\n",
    "                    child1, child2 = self.crossover(deepcopy(parent1), deepcopy(parent2))\n",
    "                    new_population.extend([child1, child2])     \n",
    "                else:\n",
    "                    #no operation\n",
    "                    new_population.extend([deepcopy(parent1), deepcopy(parent2)])\n",
    "                    \n",
    "            # Trim excess individuals\n",
    "            self.population = new_population[:self.population_size]\n",
    "\n",
    "            print(\"-----------------------------------------------\")            \n",
    "\n",
    "        print(\"################################################################\")\n",
    "        best_indices = sorted(range(len(self.population)), key=lambda i: fitness_scores[i], reverse=True)[:1]\n",
    "        best_fitness = [fitness_scores[i] for i in elites_indices][0]  \n",
    "        best_individual = self.population[best_indices[0]]\n",
    "        print(f\"Generation {generation}: Best Fitness = {best_fitness}\")\n",
    "        print(f\"Best Individual: {self.grammar.parse_tree_to_string(best_individual)}\")\n",
    "\n",
    "        return None  # Return the best individual\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(7)  # For reproducibility\n",
    "\n",
    "    # Define grammar\n",
    "    grammar_text = \"\"\"\n",
    "    <start> ::= <feature_definition> # <feature_scaling> # <feature_selection> # <ml_algorithms>\n",
    "    <feature_definition> ::=  General_Descriptors | Advanced_Descriptors | Graph_based_Signatures | Toxicophores | Fragments | General_Descriptors Advanced_Descriptors | General_Descriptors Graph_based_Signatures | General_Descriptors Toxicophores | General_Descriptors Fragments | Advanced_Descriptors Graph_based_Signatures | Advanced_Descriptors Toxicophores | Advanced_Descriptors Fragments | Graph_based_Signatures Toxicophores | Graph_based_Signatures Fragments | Toxicophores Fragments | General_Descriptors Advanced_Descriptors Graph_based_Signatures | General_Descriptors Advanced_Descriptors Toxicophores | General_Descriptors Advanced_Descriptors Fragments | General_Descriptors Graph_based_Signatures Toxicophores | General_Descriptors Graph_based_Signatures Fragments | General_Descriptors Toxicophores Fragments | Advanced_Descriptors Graph_based_Signatures Toxicophores | Advanced_Descriptors Graph_based_Signatures Fragments | Advanced_Descriptors Toxicophores Fragments | Graph_based_Signatures Toxicophores Fragments | General_Descriptors Advanced_Descriptors Graph_based_Signatures Toxicophores | General_Descriptors Advanced_Descriptors Graph_based_Signatures Fragments | General_Descriptors Advanced_Descriptors Toxicophores Fragments | General_Descriptors Graph_based_Signatures Toxicophores Fragments | Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments | General_Descriptors Advanced_Descriptors Graph_based_Signatures Toxicophores Fragments\n",
    "    <feature_scaling> ::= <no_scaling> | <normalizer> | MinMaxScaler | MaxAbsScaler | <robust_scaler> | <standard_scaler>\n",
    "    <normalizer> ::= Normalizer <norm>\n",
    "    <robust_scaler> ::= RobustScaler <boolean> <boolean>\n",
    "    <standard_scaler> ::= StandardScaler <boolean> <boolean>\n",
    "    <feature_selection> ::= <no_feature_selection> | <variance_threshold> | <select_percentile> | <selectfpr> | <selectfwe> | <selectfdr>\n",
    "    <variance_threshold> ::= VarianceThreshold <threshold>\n",
    "    <select_percentile> ::= SelectPercentile <percentile> <score_function>\n",
    "    <selectfpr> ::= SelectFpr <value_rand_1> <score_function>\n",
    "    <selectfwe> ::= SelectFwe <value_rand_1> <score_function>\n",
    "    <selectfdr> ::= SelectFdr <value_rand_1> <score_function>\n",
    "    <ml_algorithms> ::= <adaboost> | <decision_tree> | <extra_tree> | <random_rorest> | <extra_trees> | <gradient_boosting> |  <xgboost>\n",
    "    <adaboost> ::= AdaBoostClassifier <algorithm_ada> <n_estimators> <learning_rate_ada>\n",
    "    <decision_tree> ::= DecisionTreeClassifier <criterion> <splitter> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight>\n",
    "    <extra_tree> ::= ExtraTreeClassifier <criterion> <splitter> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight> \n",
    "    <random_rorest> ::= RandomForestClassifier <n_estimators> <criterion> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight_rf>\n",
    "    <extra_trees> ::= ExtraTreesClassifier <n_estimators> <criterion> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <class_weight_rf>\n",
    "    <gradient_boosting> ::= GradientBoostingClassifier <n_estimators> <criterion_gb> <max_depth> <min_samples_split> <min_samples_leaf> <max_features> <loss>\n",
    "    <xgboost> ::= XGBClassifier <n_estimators> <max_depth> <max_leaves> <learning_rate_ada>\n",
    "    <no_scaling> ::= NoScaling\n",
    "    <no_feature_selection> ::= NoFeatureSelection\n",
    "    <norm> ::= l1 | l2 | max\n",
    "    <threshold> ::= 0.0 | 0.05 | 0.10 | 0.15 | 0.20 | 0.25 | 0.30 | 0.35 | 0.40 | 0.45 | 0.50 | 0.55 | 0.60 | 0.65 | 0.70 | 0.75 | 0.80 | 0.85 | 0.90 | 0.95 | 1.0\n",
    "    <algorithm_ada> ::= SAMME.R | SAMME\n",
    "    <n_estimators> ::= 2 | 5 | 10 | 15 | 20 | 25 | 30 | 35 | 45 | 50 | 55 | 60 | 65 | 70 | 75 | 80 | 90 | 95 | 100 | 150 | 200 | 250 | 300 | 350 | 400 | 450 | 500 | 600 | 700 | 900 | 1000 | 1500 | 2000 | 2500 | 3000 | 4000 | 5000 | 7500 | 10000\n",
    "    <learning_rate_ada> ::= 0.01 | 0.05 | 0.10 | 0.15 | 0.20 | 0.25 | 0.30 | 0.35 | 0.40 | 0.45 | 0.50 | 0.55 | 0.60 | 0.65 | 0.70 | 0.75 | 0.80 | 0.85 | 0.90 | 0.95 | 1.0 | 1.05 | 1.10 | 1.15 | 1.20 | 1.25 | 1.30 | 1.35 | 1.40 | 1.45 | 1.50 | 1.55 | 1.60 | 1.65 | 1.70 | 1.75 | 1.80 | 1.85 | 1.90 | 1.95 | 2.0\n",
    "    <boolean> ::= True | False\n",
    "    <percentile> ::= 5 | 10 | 15 | 20 | 25 | 30 | 35 | 45 | 50 | 55 | 60 | 65 | 70 | 75 | 80 | 90 | 95\n",
    "    <score_function> ::= f_classif | chi2\n",
    "    <value_rand_1> ::= 0.0 | 0.05 | 0.10 | 0.15 | 0.20 | 0.25 | 0.30 | 0.35 | 0.40 | 0.45 | 0.50 | 0.55 | 0.60 | 0.65 | 0.70 | 0.75 | 0.80 | 0.85 | 0.90 | 0.95 | 1.0\n",
    "    <criterion> ::= gini | entropy | log_loss\n",
    "    <splitter> ::= best | random\n",
    "    <max_depth> ::= 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | None\n",
    "    <min_samples_split> ::= 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20\n",
    "    <min_samples_leaf> ::= 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20\n",
    "    <max_features> ::= None | log2 | sqrt\n",
    "    <class_weight> ::= balanced | None\n",
    "    <class_weight_rf> ::= balanced | balanced_subsample | None\n",
    "    <criterion_gb> ::= friedman_mse | squared_error\n",
    "    <loss> ::= log_loss | exponential\n",
    "    <max_leaves> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10\n",
    "    \"\"\"\n",
    "    training_dir = \"/home/alexgcsa/auto-admet/datasets/01_caco2_train.csv\"\n",
    "    testing_dir = \"/home/alexgcsa/auto-admet/datasets/01_caco2_blindtest.csv\"\n",
    "    # Load grammar\n",
    "    grammar = BNFGrammar()\n",
    "    grammar.load_grammar(grammar_text)\n",
    "\n",
    "    # Run GGP\n",
    "    ggp = GrammarBasedGP(grammar, training_dir, testing_dir)\n",
    "    best_program = ggp.evolve()\n",
    "\n",
    "    # Print the best program\n",
    "    #print(\"Best Program Found:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a3a2d-aa7a-42d7-a354-9f9ec7636710",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
